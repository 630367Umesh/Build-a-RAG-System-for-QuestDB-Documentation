{"source_url": "https://questdb.com/docs", "title": "Introduction", "section": "Introduction", "text": "Introduction On this page Introduction QuestDB is a top performance database that specializes in time-series. It offers category-leading ingestion throughput and fast SQL queries with operational simplicity. Given its effiency, QuestDB reduces operational costs , all while overcoming ingestion bottlenecks. As a result, QuestDB amplifies intensive time-series , capital markets , and heavy industry use cases. Quick start Explore clients Why QuestDB? Try live demo Ingest your data ​ Once running, the next step is to get your data into QuestDB. We've got a range of first-party clients, protocols and methods for ingestion. Whether you're using first-party clients or interfacing with a third-party tool or library, we've got you covered. Read the ingestion overview QuestDB Enterprise ​ QuestDB Enterprise offers everything from open source, plus additional features for running QuestDB at greater scale or significance. Learn more Guides ​ Capacity planning Select a storage medium, plan, size and compress your QuestDB deployment. Read more Design for performance Design and tweak your data model to set yourself up for reliable, optimal performance. Read more Working with time It's about time."}
{"source_url": "https://questdb.com/docs", "title": "Introduction", "section": "Introduction", "text": "d compress your QuestDB deployment. Read more Design for performance Design and tweak your data model to set yourself up for reliable, optimal performance. Read more Working with time It's about time. Learn how to work with timestamps and timezones in QuestDB. Read more Backup and restore Safety is key! See the methods to backup and restore your QuestDB deployment. Read more Resources ​ SQL overview Learn about our powerful extended SQL and how to use it to query QuestDB. Language clients Explore our language clients and how to use them to ingest data into QuestDB. Configuration See all of our available configuration options and fine-tune to match your use case. Third-Party Tools Our recommended third-party tools can aid you in analyzing and visualizing your data. Next Quick start Ingest your data QuestDB Enterprise Guides Resources\n\nIntroduction On this page Introduction QuestDB is a top performance database that specializes in time-series. It offers category-leading ingestion throughput and fast SQL queries with operational simplicity. Given its effiency, QuestDB reduces operational costs , all while overcoming ingestion bottlenecks. As a result, QuestDB amplifies intensive time-"}
{"source_url": "https://questdb.com/docs", "title": "Introduction", "section": "Introduction", "text": "put and fast SQL queries with operational simplicity. Given its effiency, QuestDB reduces operational costs , all while overcoming ingestion bottlenecks. As a result, QuestDB amplifies intensive time-series , capital markets , and heavy industry use cases. Quick start Explore clients Why QuestDB? Try live demo Ingest your data ​ Once running, the next step is to get your data into QuestDB. We've got a range of first-party clients, protocols and methods for ingestion. Whether you're using first-party clients or interfacing with a third-party tool or library, we've got you covered. Read the ingestion overview QuestDB Enterprise ​ QuestDB Enterprise offers everything from open source, plus additional features for running QuestDB at greater scale or significance. Learn more Guides ​ Capacity planning Select a storage medium, plan, size and compress your QuestDB deployment. Read more Design for performance Design and tweak your data model to set yourself up for reliable, optimal performance. Read more Working with time It's about time. Learn how to work with timestamps and timezones in QuestDB. Read more Backup and restore Safety is key! See the methods to backup and restore your QuestD"}
{"source_url": "https://questdb.com/docs", "title": "Introduction", "section": "Introduction", "text": ". Read more Working with time It's about time. Learn how to work with timestamps and timezones in QuestDB. Read more Backup and restore Safety is key! See the methods to backup and restore your QuestDB deployment. Read more Resources ​ SQL overview Learn about our powerful extended SQL and how to use it to query QuestDB. Language clients Explore our language clients and how to use them to ingest data into QuestDB. Configuration See all of our available configuration options and fine-tune to match your use case. Third-Party Tools Our recommended third-party tools can aid you in analyzing and visualizing your data. Next Quick start Ingest your data QuestDB Enterprise Guides Resources\n\nIntroduction On this page Introduction QuestDB is a top performance database that specializes in time-series. It offers category-leading ingestion throughput and fast SQL queries with operational simplicity. Given its effiency, QuestDB reduces operational costs , all while overcoming ingestion bottlenecks. As a result, QuestDB amplifies intensive time-series , capital markets , and heavy industry use cases. Quick start Explore clients Why QuestDB? Try live demo Ingest your data ​ Once running, the next"}
{"source_url": "https://questdb.com/docs", "title": "Introduction", "section": "Introduction", "text": "As a result, QuestDB amplifies intensive time-series , capital markets , and heavy industry use cases. Quick start Explore clients Why QuestDB? Try live demo Ingest your data ​ Once running, the next step is to get your data into QuestDB. We've got a range of first-party clients, protocols and methods for ingestion. Whether you're using first-party clients or interfacing with a third-party tool or library, we've got you covered. Read the ingestion overview QuestDB Enterprise ​ QuestDB Enterprise offers everything from open source, plus additional features for running QuestDB at greater scale or significance. Learn more Guides ​ Capacity planning Select a storage medium, plan, size and compress your QuestDB deployment. Read more Design for performance Design and tweak your data model to set yourself up for reliable, optimal performance. Read more Working with time It's about time. Learn how to work with timestamps and timezones in QuestDB. Read more Backup and restore Safety is key! See the methods to backup and restore your QuestDB deployment. Read more Resources ​ SQL overview Learn about our powerful extended SQL and how to use it to query QuestDB. Language clients Explore our la"}
{"source_url": "https://questdb.com/docs", "title": "Introduction", "section": "Introduction", "text": "the methods to backup and restore your QuestDB deployment. Read more Resources ​ SQL overview Learn about our powerful extended SQL and how to use it to query QuestDB. Language clients Explore our language clients and how to use them to ingest data into QuestDB. Configuration See all of our available configuration options and fine-tune to match your use case. Third-Party Tools Our recommended third-party tools can aid you in analyzing and visualizing your data. Next Quick start\n\nIntroduction On this page Introduction QuestDB is a top performance database that specializes in time-series. It offers category-leading ingestion throughput and fast SQL queries with operational simplicity. Given its effiency, QuestDB reduces operational costs , all while overcoming ingestion bottlenecks. As a result, QuestDB amplifies intensive time-series , capital markets , and heavy industry use cases. Quick start Explore clients Why QuestDB? Try live demo Ingest your data ​ Once running, the next step is to get your data into QuestDB. We've got a range of first-party clients, protocols and methods for ingestion. Whether you're using first-party clients or interfacing with a third-party tool or librar"}
{"source_url": "https://questdb.com/docs", "title": "Introduction", "section": "Introduction", "text": "to get your data into QuestDB. We've got a range of first-party clients, protocols and methods for ingestion. Whether you're using first-party clients or interfacing with a third-party tool or library, we've got you covered. Read the ingestion overview QuestDB Enterprise ​ QuestDB Enterprise offers everything from open source, plus additional features for running QuestDB at greater scale or significance. Learn more Guides ​ Capacity planning Select a storage medium, plan, size and compress your QuestDB deployment. Read more Design for performance Design and tweak your data model to set yourself up for reliable, optimal performance. Read more Working with time It's about time. Learn how to work with timestamps and timezones in QuestDB. Read more Backup and restore Safety is key! See the methods to backup and restore your QuestDB deployment. Read more Resources ​ SQL overview Learn about our powerful extended SQL and how to use it to query QuestDB. Language clients Explore our language clients and how to use them to ingest data into QuestDB. Configuration See all of our available configuration options and fine-tune to match your use case. Third-Party Tools Our recommended third-part"}
{"source_url": "https://questdb.com/docs", "title": "Introduction", "section": "Introduction", "text": "clients and how to use them to ingest data into QuestDB. Configuration See all of our available configuration options and fine-tune to match your use case. Third-Party Tools Our recommended third-party tools can aid you in analyzing and visualizing your data. Next Quick start\n\nIntroduction\n\nOn this page\n\nIntroduction QuestDB is a top performance database that specializes in time-series. It offers category-leading ingestion throughput and fast SQL queries with operational simplicity. Given its effiency, QuestDB reduces operational costs , all while overcoming ingestion bottlenecks. As a result, QuestDB amplifies intensive time-series , capital markets , and heavy industry use cases. Quick start Explore clients Why QuestDB? Try live demo Ingest your data ​ Once running, the next step is to get your data into QuestDB. We've got a range of first-party clients, protocols and methods for ingestion. Whether you're using first-party clients or interfacing with a third-party tool or library, we've got you covered. Read the ingestion overview QuestDB Enterprise ​ QuestDB Enterprise offers everything from open source, plus additional features for running QuestDB at greater scale or significan"}
{"source_url": "https://questdb.com/docs", "title": "Introduction", "section": "Introduction", "text": "'ve got you covered. Read the ingestion overview QuestDB Enterprise ​ QuestDB Enterprise offers everything from open source, plus additional features for running QuestDB at greater scale or significance. Learn more Guides ​ Capacity planning Select a storage medium, plan, size and compress your QuestDB deployment. Read more Design for performance Design and tweak your data model to set yourself up for reliable, optimal performance. Read more Working with time It's about time. Learn how to work with timestamps and timezones in QuestDB. Read more Backup and restore Safety is key! See the methods to backup and restore your QuestDB deployment. Read more Resources ​ SQL overview Learn about our powerful extended SQL and how to use it to query QuestDB. Language clients Explore our language clients and how to use them to ingest data into QuestDB. Configuration See all of our available configuration options and fine-tune to match your use case. Third-Party Tools Our recommended third-party tools can aid you in analyzing and visualizing your data."}
{"source_url": "https://questdb.com/docs", "title": "Introduction", "section": "Introduction", "text": "QuestDB is a top performance database that specializes in time-series.\n\nIt offers category-leading ingestion throughput and fast SQL queries with operational simplicity.\n\nGiven its effiency, QuestDB reduces operational costs , all while overcoming ingestion bottlenecks.\n\nAs a result, QuestDB amplifies intensive time-series , capital markets , and heavy industry use cases.\n\nQuick start Explore clients Why QuestDB? Try live demo"}
{"source_url": "https://questdb.com/docs", "title": "Introduction", "section": "Ingest your data ​", "text": "Once running, the next step is to get your data into QuestDB.\n\nWe've got a range of first-party clients, protocols and methods for ingestion.\n\nWhether you're using first-party clients or interfacing with a third-party tool or library, we've got you covered.\n\nRead the ingestion overview"}
{"source_url": "https://questdb.com/docs", "title": "Introduction", "section": "QuestDB Enterprise ​", "text": "QuestDB Enterprise offers everything from open source, plus additional features for running QuestDB at greater scale or significance.\n\nLearn more"}
{"source_url": "https://questdb.com/docs", "title": "Introduction", "section": "Guides ​", "text": "Capacity planning Select a storage medium, plan, size and compress your QuestDB deployment. Read more Design for performance Design and tweak your data model to set yourself up for reliable, optimal performance. Read more Working with time It's about time. Learn how to work with timestamps and timezones in QuestDB. Read more Backup and restore Safety is key! See the methods to backup and restore your QuestDB deployment. Read more\n\nCapacity planning Select a storage medium, plan, size and compress your QuestDB deployment. Read more Design for performance Design and tweak your data model to set yourself up for reliable, optimal performance. Read more Working with time It's about time. Learn how to work with timestamps and timezones in QuestDB. Read more Backup and restore Safety is key! See the methods to backup and restore your QuestDB deployment. Read more\n\nCapacity planning Select a storage medium, plan, size and compress your QuestDB deployment. Read more"}
{"source_url": "https://questdb.com/docs", "title": "Introduction", "section": "Capacity planning", "text": "Select a storage medium, plan, size and compress your QuestDB deployment.\n\nRead more\n\nDesign for performance Design and tweak your data model to set yourself up for reliable, optimal performance. Read more"}
{"source_url": "https://questdb.com/docs", "title": "Introduction", "section": "Design for performance", "text": "Design and tweak your data model to set yourself up for reliable, optimal performance.\n\nRead more\n\nWorking with time It's about time. Learn how to work with timestamps and timezones in QuestDB. Read more"}
{"source_url": "https://questdb.com/docs", "title": "Introduction", "section": "Working with time", "text": "It's about time. Learn how to work with timestamps and timezones in QuestDB.\n\nRead more\n\nBackup and restore Safety is key! See the methods to backup and restore your QuestDB deployment. Read more"}
{"source_url": "https://questdb.com/docs", "title": "Introduction", "section": "Backup and restore", "text": "Safety is key! See the methods to backup and restore your QuestDB deployment.\n\nRead more"}
{"source_url": "https://questdb.com/docs", "title": "Introduction", "section": "Resources ​", "text": "SQL overview Learn about our powerful extended SQL and how to use it to query QuestDB. Language clients Explore our language clients and how to use them to ingest data into QuestDB. Configuration See all of our available configuration options and fine-tune to match your use case. Third-Party Tools Our recommended third-party tools can aid you in analyzing and visualizing your data.\n\nSQL overview Learn about our powerful extended SQL and how to use it to query QuestDB. Language clients Explore our language clients and how to use them to ingest data into QuestDB. Configuration See all of our available configuration options and fine-tune to match your use case. Third-Party Tools Our recommended third-party tools can aid you in analyzing and visualizing your data.\n\nSQL overview Learn about our powerful extended SQL and how to use it to query QuestDB.\n\nSQL overview Learn about our powerful extended SQL and how to use it to query QuestDB."}
{"source_url": "https://questdb.com/docs", "title": "Introduction", "section": "SQL overview", "text": "Learn about our powerful extended SQL and how to use it to query QuestDB.\n\nLanguage clients Explore our language clients and how to use them to ingest data into QuestDB.\n\nLanguage clients Explore our language clients and how to use them to ingest data into QuestDB."}
{"source_url": "https://questdb.com/docs", "title": "Introduction", "section": "Language clients", "text": "Explore our language clients and how to use them to ingest data into QuestDB.\n\nConfiguration See all of our available configuration options and fine-tune to match your use case.\n\nConfiguration See all of our available configuration options and fine-tune to match your use case."}
{"source_url": "https://questdb.com/docs", "title": "Introduction", "section": "Configuration", "text": "See all of our available configuration options and fine-tune to match your use case.\n\nThird-Party Tools Our recommended third-party tools can aid you in analyzing and visualizing your data.\n\nThird-Party Tools Our recommended third-party tools can aid you in analyzing and visualizing your data."}
{"source_url": "https://questdb.com/docs", "title": "Introduction", "section": "Third-Party Tools", "text": "Our recommended third-party tools can aid you in analyzing and visualizing your data.\n\nNext\n\nQuick start\n\nIngest your data QuestDB Enterprise Guides Resources\n\nIngest your data QuestDB Enterprise Guides Resources\n\nIngest your data\n\nQuestDB Enterprise\n\nGuides\n\nResources"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "Quick start On this page Quick start This guide will get your first QuestDB instance running. As the goal is to, well, start quickly , we'll presume defaults. Once running, we'll provide guides for inserting data, configuration and production hosting. QuestDB already running? Jump ahead! Select a first-party client or ingest method. Install QuestDB ​ Choose from the following options: Docker Homebrew Binaries Docker ​ To use Docker, one must have Docker. You can find installation guides for your platform on the official documentation . Once Docker is installed, you will need to pull QuestDB's image from Docker Hub and create a container: docker run \\ -p 9000:9000 -p 9009:9009 -p 8812:8812 -p 9003:9003 \\ questdb/questdb:9.1.0 For deeper instructions, see the Docker deployment guide . Homebrew ​ To install QuestDB via Homebrew , run the following command: brew install questdb On macOS, the location of the root directory of QuestDB and server configuration files depending on the chip: Apple Silicon (M1/M2/M*) chip: /opt/homebrew/var/questdb Intel chip: /usr/local/var/questdb Binaries ​ Download and run QuestDB via binaries. Select your platform of choice: Linux Windows Any (no JVM) Do"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "le Silicon (M1/M2/M*) chip: /opt/homebrew/var/questdb Intel chip: /usr/local/var/questdb Binaries ​ Download and run QuestDB via binaries. Select your platform of choice: Linux Windows Any (no JVM) Download the binary: questdb-9.1.0-rt-linux-x86-64.tar.gz Next, unpack it: tar -xvf questdb-9.1.0-rt-linux-x86-64.tar.gz The default directory becomes: $HOME/.questdb Download the executable: questdb-9.1.0-rt-windows-x86-64.tar.gz The default root directory becomes: C:\\Windows\\System32\\qdbroot Download the binary: questdb-9.1.0-no-jre-bin.tar.gz This package does not embed Java. Use this if there is no package for your platform, such as ARM Linux. Requires local Java 17. To check your installed version: java -version If you do not have Java, install one of the following: AdoptOpenJDK Amazon Corretto OpenJDK Oracle Java Other Java distributions might work but are not tested regularly. For example, it is known that Azul Zing 17 is incompatible. For environment variable, point JAVA_HOME to your Java 17 installation folder. Run QuestDB ​ Linux/No JVM macOS (Homebrew) Windows ./questdb.sh [start|stop|status] [-d dir] [-f] [-n] [-t tag] Option Description -d Expects a dir directory value which"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "Java 17 installation folder. Run QuestDB ​ Linux/No JVM macOS (Homebrew) Windows ./questdb.sh [start|stop|status] [-d dir] [-f] [-n] [-t tag] Option Description -d Expects a dir directory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see the default root section below. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console . Without this option, the Web Console is cached and deployed only when missing. -n Do not respond to the HUP signal. This keeps QuestDB alive after you close the terminal window where you started it. questdb [start|stop|status] [-d dir] [-f] [-n] [-t tag] Option Description -d Expects a dir directory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see the default root section below. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separat"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "fault values, see the default root section below. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console . Without this option, the Web Console is cached and deployed only when missing. -n Do not respond to the HUP signal. This keeps QuestDB alive after you close the terminal window where you started it. questdb.exe [start|stop|status|install|remove] \\ [-d dir] [-f] [-j JAVA_HOME] [-t tag] Option Description install Installs the Windows QuestDB service. The service will start automatically at startup. remove Removes the Windows QuestDB service. It will no longer start at startup. -d Expects a dir directory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see the default root section below. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deployi"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console . Without this option, the Web Console is cached and deployed only when missing. -j Windows only! This option allows to specify a path to JAVA_HOME . Enjoy QuestDB ​ Congratulations! 🎉 QuestDB is now running. The QuestDB Web Console is available by default at: http://localhost:9000 . Also by default, QuestDB will use the following ports: 9000 - REST API and Web Console 9000 - InfluxDB Line Protocol (ILP) 8812 - Postgres Wire Protocol (PGWire) 9003 - Min health server With that, you're ready to bring your data and enjoy the high performance and reliability of QuestDB. Bring your data ​ Now... Time to really blast-off. 🚀 Next up: Bring your data - the life blood of any database. Choose from one of our premium ingest-only language clients: C & C++ High-performance client for systems programming and embedded applications. Read more .NET Cross-platform client for building applications with .NET technologies. Read more Go An open-source programming language supported by Go"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "r systems programming and embedded applications. Read more .NET Cross-platform client for building applications with .NET technologies. Read more Go An open-source programming language supported by Google with built-in concurrency. Read more Java Platform-independent client for enterprise applications and Android development. Read more Node.js Node.js® is an open-source, cross-platform JavaScript runtime environment. Read more Python Python is a programming language that lets you work quickly and integrate systems more effectively. Read more Rust Systems programming language focused on safety, speed, and concurrency. Read more Want more options? See the ingestion overview . Create new data ​ No data yet and still want to trial QuestDB? There are several quick options: QuestDB demo instance : Hosted, fully loaded and ready to go. Quickly explore the Web Console and SQL syntax. Create my first data set guide : create tables, use rnd_ functions and make your own data. Sample dataset repos : IoT, e-commerce, finance or git logs? Check them out! Quick start repos : Code-based quick starts that cover ingestion, querying and data visualization using common programming languages and use ca"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": ": IoT, e-commerce, finance or git logs? Check them out! Quick start repos : Code-based quick starts that cover ingestion, querying and data visualization using common programming languages and use cases. Also, a cat in a tracksuit. Time series streaming analytics template : A handy template for near real-time analytics using open source technologies. Learn QuestDB ​ For operators or developers looking for next steps to run an efficient instance, see: Capacity planning for recommended configurations for operating QuestDB in production Configuration to see all of the available options in your server.conf file Design for performance for tips and tricks Visualize with Grafana to create useful dashboards and visualizations from your data Looking for inspiration? Checkout our real-time crypto dashboard . Edit this page Previous Introduction Next Why QuestDB? Install QuestDB Docker Homebrew Binaries Run QuestDB Enjoy QuestDB Bring your data Create new data Learn QuestDB\n\nQuick start On this page Quick start This guide will get your first QuestDB instance running. As the goal is to, well, start quickly , we'll presume defaults. Once running, we'll provide guides for inserting data, config"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "page Quick start This guide will get your first QuestDB instance running. As the goal is to, well, start quickly , we'll presume defaults. Once running, we'll provide guides for inserting data, configuration and production hosting. QuestDB already running? Jump ahead! Select a first-party client or ingest method. Install QuestDB ​ Choose from the following options: Docker Homebrew Binaries Docker ​ To use Docker, one must have Docker. You can find installation guides for your platform on the official documentation . Once Docker is installed, you will need to pull QuestDB's image from Docker Hub and create a container: docker run \\ -p 9000:9000 -p 9009:9009 -p 8812:8812 -p 9003:9003 \\ questdb/questdb:9.1.0 For deeper instructions, see the Docker deployment guide . Homebrew ​ To install QuestDB via Homebrew , run the following command: brew install questdb On macOS, the location of the root directory of QuestDB and server configuration files depending on the chip: Apple Silicon (M1/M2/M*) chip: /opt/homebrew/var/questdb Intel chip: /usr/local/var/questdb Binaries ​ Download and run QuestDB via binaries. Select your platform of choice: Linux Windows Any (no JVM) Download the binary: q"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": ") chip: /opt/homebrew/var/questdb Intel chip: /usr/local/var/questdb Binaries ​ Download and run QuestDB via binaries. Select your platform of choice: Linux Windows Any (no JVM) Download the binary: questdb-9.1.0-rt-linux-x86-64.tar.gz Next, unpack it: tar -xvf questdb-9.1.0-rt-linux-x86-64.tar.gz The default directory becomes: $HOME/.questdb Download the executable: questdb-9.1.0-rt-windows-x86-64.tar.gz The default root directory becomes: C:\\Windows\\System32\\qdbroot Download the binary: questdb-9.1.0-no-jre-bin.tar.gz This package does not embed Java. Use this if there is no package for your platform, such as ARM Linux. Requires local Java 17. To check your installed version: java -version If you do not have Java, install one of the following: AdoptOpenJDK Amazon Corretto OpenJDK Oracle Java Other Java distributions might work but are not tested regularly. For example, it is known that Azul Zing 17 is incompatible. For environment variable, point JAVA_HOME to your Java 17 installation folder. Run QuestDB ​ Linux/No JVM macOS (Homebrew) Windows ./questdb.sh [start|stop|status] [-d dir] [-f] [-n] [-t tag] Option Description -d Expects a dir directory value which is a folder that wi"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "n folder. Run QuestDB ​ Linux/No JVM macOS (Homebrew) Windows ./questdb.sh [start|stop|status] [-d dir] [-f] [-n] [-t tag] Option Description -d Expects a dir directory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see the default root section below. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console . Without this option, the Web Console is cached and deployed only when missing. -n Do not respond to the HUP signal. This keeps QuestDB alive after you close the terminal window where you started it. questdb [start|stop|status] [-d dir] [-f] [-n] [-t tag] Option Description -d Expects a dir directory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see the default root section below. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "e default root section below. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console . Without this option, the Web Console is cached and deployed only when missing. -n Do not respond to the HUP signal. This keeps QuestDB alive after you close the terminal window where you started it. questdb.exe [start|stop|status|install|remove] \\ [-d dir] [-f] [-j JAVA_HOME] [-t tag] Option Description install Installs the Windows QuestDB service. The service will start automatically at startup. remove Removes the Windows QuestDB service. It will no longer start at startup. -d Expects a dir directory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see the default root section below. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console ."}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "r the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console . Without this option, the Web Console is cached and deployed only when missing. -j Windows only! This option allows to specify a path to JAVA_HOME . Enjoy QuestDB ​ Congratulations! 🎉 QuestDB is now running. The QuestDB Web Console is available by default at: http://localhost:9000 . Also by default, QuestDB will use the following ports: 9000 - REST API and Web Console 9000 - InfluxDB Line Protocol (ILP) 8812 - Postgres Wire Protocol (PGWire) 9003 - Min health server With that, you're ready to bring your data and enjoy the high performance and reliability of QuestDB. Bring your data ​ Now... Time to really blast-off. 🚀 Next up: Bring your data - the life blood of any database. Choose from one of our premium ingest-only language clients: C & C++ High-performance client for systems programming and embedded applications. Read more .NET Cross-platform client for building applications with .NET technologies. Read more Go An open-source programming language supported by Google with built-in c"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "g and embedded applications. Read more .NET Cross-platform client for building applications with .NET technologies. Read more Go An open-source programming language supported by Google with built-in concurrency. Read more Java Platform-independent client for enterprise applications and Android development. Read more Node.js Node.js® is an open-source, cross-platform JavaScript runtime environment. Read more Python Python is a programming language that lets you work quickly and integrate systems more effectively. Read more Rust Systems programming language focused on safety, speed, and concurrency. Read more Want more options? See the ingestion overview . Create new data ​ No data yet and still want to trial QuestDB? There are several quick options: QuestDB demo instance : Hosted, fully loaded and ready to go. Quickly explore the Web Console and SQL syntax. Create my first data set guide : create tables, use rnd_ functions and make your own data. Sample dataset repos : IoT, e-commerce, finance or git logs? Check them out! Quick start repos : Code-based quick starts that cover ingestion, querying and data visualization using common programming languages and use cases. Also, a cat in"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "finance or git logs? Check them out! Quick start repos : Code-based quick starts that cover ingestion, querying and data visualization using common programming languages and use cases. Also, a cat in a tracksuit. Time series streaming analytics template : A handy template for near real-time analytics using open source technologies. Learn QuestDB ​ For operators or developers looking for next steps to run an efficient instance, see: Capacity planning for recommended configurations for operating QuestDB in production Configuration to see all of the available options in your server.conf file Design for performance for tips and tricks Visualize with Grafana to create useful dashboards and visualizations from your data Looking for inspiration? Checkout our real-time crypto dashboard . Edit this page Previous Introduction Next Why QuestDB? Install QuestDB Docker Homebrew Binaries Run QuestDB Enjoy QuestDB Bring your data Create new data Learn QuestDB\n\nQuick start On this page Quick start This guide will get your first QuestDB instance running. As the goal is to, well, start quickly , we'll presume defaults. Once running, we'll provide guides for inserting data, configuration and producti"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "s guide will get your first QuestDB instance running. As the goal is to, well, start quickly , we'll presume defaults. Once running, we'll provide guides for inserting data, configuration and production hosting. QuestDB already running? Jump ahead! Select a first-party client or ingest method. Install QuestDB ​ Choose from the following options: Docker Homebrew Binaries Docker ​ To use Docker, one must have Docker. You can find installation guides for your platform on the official documentation . Once Docker is installed, you will need to pull QuestDB's image from Docker Hub and create a container: docker run \\ -p 9000:9000 -p 9009:9009 -p 8812:8812 -p 9003:9003 \\ questdb/questdb:9.1.0 For deeper instructions, see the Docker deployment guide . Homebrew ​ To install QuestDB via Homebrew , run the following command: brew install questdb On macOS, the location of the root directory of QuestDB and server configuration files depending on the chip: Apple Silicon (M1/M2/M*) chip: /opt/homebrew/var/questdb Intel chip: /usr/local/var/questdb Binaries ​ Download and run QuestDB via binaries. Select your platform of choice: Linux Windows Any (no JVM) Download the binary: questdb-9.1.0-rt-linu"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "w/var/questdb Intel chip: /usr/local/var/questdb Binaries ​ Download and run QuestDB via binaries. Select your platform of choice: Linux Windows Any (no JVM) Download the binary: questdb-9.1.0-rt-linux-x86-64.tar.gz Next, unpack it: tar -xvf questdb-9.1.0-rt-linux-x86-64.tar.gz The default directory becomes: $HOME/.questdb Download the executable: questdb-9.1.0-rt-windows-x86-64.tar.gz The default root directory becomes: C:\\Windows\\System32\\qdbroot Download the binary: questdb-9.1.0-no-jre-bin.tar.gz This package does not embed Java. Use this if there is no package for your platform, such as ARM Linux. Requires local Java 17. To check your installed version: java -version If you do not have Java, install one of the following: AdoptOpenJDK Amazon Corretto OpenJDK Oracle Java Other Java distributions might work but are not tested regularly. For example, it is known that Azul Zing 17 is incompatible. For environment variable, point JAVA_HOME to your Java 17 installation folder. Run QuestDB ​ Linux/No JVM macOS (Homebrew) Windows ./questdb.sh [start|stop|status] [-d dir] [-f] [-n] [-t tag] Option Description -d Expects a dir directory value which is a folder that will be used as QuestD"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "B ​ Linux/No JVM macOS (Homebrew) Windows ./questdb.sh [start|stop|status] [-d dir] [-f] [-n] [-t tag] Option Description -d Expects a dir directory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see the default root section below. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console . Without this option, the Web Console is cached and deployed only when missing. -n Do not respond to the HUP signal. This keeps QuestDB alive after you close the terminal window where you started it. questdb [start|stop|status] [-d dir] [-f] [-n] [-t tag] Option Description -d Expects a dir directory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see the default root section below. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the defa"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "on below. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console . Without this option, the Web Console is cached and deployed only when missing. -n Do not respond to the HUP signal. This keeps QuestDB alive after you close the terminal window where you started it. questdb.exe [start|stop|status|install|remove] \\ [-d dir] [-f] [-j JAVA_HOME] [-t tag] Option Description install Installs the Windows QuestDB service. The service will start automatically at startup. remove Removes the Windows QuestDB service. It will no longer start at startup. -d Expects a dir directory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see the default root section below. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console . Without this option"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console . Without this option, the Web Console is cached and deployed only when missing. -j Windows only! This option allows to specify a path to JAVA_HOME . Enjoy QuestDB ​ Congratulations! 🎉 QuestDB is now running. The QuestDB Web Console is available by default at: http://localhost:9000 . Also by default, QuestDB will use the following ports: 9000 - REST API and Web Console 9000 - InfluxDB Line Protocol (ILP) 8812 - Postgres Wire Protocol (PGWire) 9003 - Min health server With that, you're ready to bring your data and enjoy the high performance and reliability of QuestDB. Bring your data ​ Now... Time to really blast-off. 🚀 Next up: Bring your data - the life blood of any database. Choose from one of our premium ingest-only language clients: C & C++ High-performance client for systems programming and embedded applications. Read more .NET Cross-platform client for building applications with .NET technologies. Read more Go An open-source programming language supported by Google with built-in concurrency. Read mor"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "cations. Read more .NET Cross-platform client for building applications with .NET technologies. Read more Go An open-source programming language supported by Google with built-in concurrency. Read more Java Platform-independent client for enterprise applications and Android development. Read more Node.js Node.js® is an open-source, cross-platform JavaScript runtime environment. Read more Python Python is a programming language that lets you work quickly and integrate systems more effectively. Read more Rust Systems programming language focused on safety, speed, and concurrency. Read more Want more options? See the ingestion overview . Create new data ​ No data yet and still want to trial QuestDB? There are several quick options: QuestDB demo instance : Hosted, fully loaded and ready to go. Quickly explore the Web Console and SQL syntax. Create my first data set guide : create tables, use rnd_ functions and make your own data. Sample dataset repos : IoT, e-commerce, finance or git logs? Check them out! Quick start repos : Code-based quick starts that cover ingestion, querying and data visualization using common programming languages and use cases. Also, a cat in a tracksuit. Time se"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "Check them out! Quick start repos : Code-based quick starts that cover ingestion, querying and data visualization using common programming languages and use cases. Also, a cat in a tracksuit. Time series streaming analytics template : A handy template for near real-time analytics using open source technologies. Learn QuestDB ​ For operators or developers looking for next steps to run an efficient instance, see: Capacity planning for recommended configurations for operating QuestDB in production Configuration to see all of the available options in your server.conf file Design for performance for tips and tricks Visualize with Grafana to create useful dashboards and visualizations from your data Looking for inspiration? Checkout our real-time crypto dashboard . Edit this page Previous Introduction Next Why QuestDB?\n\nQuick start On this page Quick start This guide will get your first QuestDB instance running. As the goal is to, well, start quickly , we'll presume defaults. Once running, we'll provide guides for inserting data, configuration and production hosting. QuestDB already running? Jump ahead! Select a first-party client or ingest method. Install QuestDB ​ Choose from the foll"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "we'll provide guides for inserting data, configuration and production hosting. QuestDB already running? Jump ahead! Select a first-party client or ingest method. Install QuestDB ​ Choose from the following options: Docker Homebrew Binaries Docker ​ To use Docker, one must have Docker. You can find installation guides for your platform on the official documentation . Once Docker is installed, you will need to pull QuestDB's image from Docker Hub and create a container: docker run \\ -p 9000:9000 -p 9009:9009 -p 8812:8812 -p 9003:9003 \\ questdb/questdb:9.1.0 For deeper instructions, see the Docker deployment guide . Homebrew ​ To install QuestDB via Homebrew , run the following command: brew install questdb On macOS, the location of the root directory of QuestDB and server configuration files depending on the chip: Apple Silicon (M1/M2/M*) chip: /opt/homebrew/var/questdb Intel chip: /usr/local/var/questdb Binaries ​ Download and run QuestDB via binaries. Select your platform of choice: Linux Windows Any (no JVM) Download the binary: questdb-9.1.0-rt-linux-x86-64.tar.gz Next, unpack it: tar -xvf questdb-9.1.0-rt-linux-x86-64.tar.gz The default directory becomes: $HOME/.questdb Download"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "nux Windows Any (no JVM) Download the binary: questdb-9.1.0-rt-linux-x86-64.tar.gz Next, unpack it: tar -xvf questdb-9.1.0-rt-linux-x86-64.tar.gz The default directory becomes: $HOME/.questdb Download the executable: questdb-9.1.0-rt-windows-x86-64.tar.gz The default root directory becomes: C:\\Windows\\System32\\qdbroot Download the binary: questdb-9.1.0-no-jre-bin.tar.gz This package does not embed Java. Use this if there is no package for your platform, such as ARM Linux. Requires local Java 17. To check your installed version: java -version If you do not have Java, install one of the following: AdoptOpenJDK Amazon Corretto OpenJDK Oracle Java Other Java distributions might work but are not tested regularly. For example, it is known that Azul Zing 17 is incompatible. For environment variable, point JAVA_HOME to your Java 17 installation folder. Run QuestDB ​ Linux/No JVM macOS (Homebrew) Windows ./questdb.sh [start|stop|status] [-d dir] [-f] [-n] [-t tag] Option Description -d Expects a dir directory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see the default root section below. -t Expects a tag string value wh"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "a dir directory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see the default root section below. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console . Without this option, the Web Console is cached and deployed only when missing. -n Do not respond to the HUP signal. This keeps QuestDB alive after you close the terminal window where you started it. questdb [start|stop|status] [-d dir] [-f] [-n] [-t tag] Option Description -d Expects a dir directory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see the default root section below. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console . Without this option, the Web Console is cached and deployed only wh"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "ces and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console . Without this option, the Web Console is cached and deployed only when missing. -n Do not respond to the HUP signal. This keeps QuestDB alive after you close the terminal window where you started it. questdb.exe [start|stop|status|install|remove] \\ [-d dir] [-f] [-j JAVA_HOME] [-t tag] Option Description install Installs the Windows QuestDB service. The service will start automatically at startup. remove Removes the Windows QuestDB service. It will no longer start at startup. -d Expects a dir directory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see the default root section below. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console . Without this option, the Web Console is cached and deployed only when missing. -j Windows only! This option allows to specify a path to JAVA_HOME . Enjo"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "estdb . -f Force re-deploying the Web Console . Without this option, the Web Console is cached and deployed only when missing. -j Windows only! This option allows to specify a path to JAVA_HOME . Enjoy QuestDB ​ Congratulations! 🎉 QuestDB is now running. The QuestDB Web Console is available by default at: http://localhost:9000 . Also by default, QuestDB will use the following ports: 9000 - REST API and Web Console 9000 - InfluxDB Line Protocol (ILP) 8812 - Postgres Wire Protocol (PGWire) 9003 - Min health server With that, you're ready to bring your data and enjoy the high performance and reliability of QuestDB. Bring your data ​ Now... Time to really blast-off. 🚀 Next up: Bring your data - the life blood of any database. Choose from one of our premium ingest-only language clients: C & C++ High-performance client for systems programming and embedded applications. Read more .NET Cross-platform client for building applications with .NET technologies. Read more Go An open-source programming language supported by Google with built-in concurrency. Read more Java Platform-independent client for enterprise applications and Android development. Read more Node.js Node.js® is an open-source,"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "ng language supported by Google with built-in concurrency. Read more Java Platform-independent client for enterprise applications and Android development. Read more Node.js Node.js® is an open-source, cross-platform JavaScript runtime environment. Read more Python Python is a programming language that lets you work quickly and integrate systems more effectively. Read more Rust Systems programming language focused on safety, speed, and concurrency. Read more Want more options? See the ingestion overview . Create new data ​ No data yet and still want to trial QuestDB? There are several quick options: QuestDB demo instance : Hosted, fully loaded and ready to go. Quickly explore the Web Console and SQL syntax. Create my first data set guide : create tables, use rnd_ functions and make your own data. Sample dataset repos : IoT, e-commerce, finance or git logs? Check them out! Quick start repos : Code-based quick starts that cover ingestion, querying and data visualization using common programming languages and use cases. Also, a cat in a tracksuit. Time series streaming analytics template : A handy template for near real-time analytics using open source technologies. Learn QuestDB ​ For"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "amming languages and use cases. Also, a cat in a tracksuit. Time series streaming analytics template : A handy template for near real-time analytics using open source technologies. Learn QuestDB ​ For operators or developers looking for next steps to run an efficient instance, see: Capacity planning for recommended configurations for operating QuestDB in production Configuration to see all of the available options in your server.conf file Design for performance for tips and tricks Visualize with Grafana to create useful dashboards and visualizations from your data Looking for inspiration? Checkout our real-time crypto dashboard . Edit this page Previous Introduction Next Why QuestDB?\n\nQuick start\n\nOn this page\n\nQuick start This guide will get your first QuestDB instance running. As the goal is to, well, start quickly , we'll presume defaults. Once running, we'll provide guides for inserting data, configuration and production hosting. QuestDB already running? Jump ahead! Select a first-party client or ingest method. Install QuestDB ​ Choose from the following options: Docker Homebrew Binaries Docker ​ To use Docker, one must have Docker. You can find installation guides for your pla"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "party client or ingest method. Install QuestDB ​ Choose from the following options: Docker Homebrew Binaries Docker ​ To use Docker, one must have Docker. You can find installation guides for your platform on the official documentation . Once Docker is installed, you will need to pull QuestDB's image from Docker Hub and create a container: docker run \\ -p 9000:9000 -p 9009:9009 -p 8812:8812 -p 9003:9003 \\ questdb/questdb:9.1.0 For deeper instructions, see the Docker deployment guide . Homebrew ​ To install QuestDB via Homebrew , run the following command: brew install questdb On macOS, the location of the root directory of QuestDB and server configuration files depending on the chip: Apple Silicon (M1/M2/M*) chip: /opt/homebrew/var/questdb Intel chip: /usr/local/var/questdb Binaries ​ Download and run QuestDB via binaries. Select your platform of choice: Linux Windows Any (no JVM) Download the binary: questdb-9.1.0-rt-linux-x86-64.tar.gz Next, unpack it: tar -xvf questdb-9.1.0-rt-linux-x86-64.tar.gz The default directory becomes: $HOME/.questdb Download the executable: questdb-9.1.0-rt-windows-x86-64.tar.gz The default root directory becomes: C:\\Windows\\System32\\qdbroot Download th"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "-x86-64.tar.gz The default directory becomes: $HOME/.questdb Download the executable: questdb-9.1.0-rt-windows-x86-64.tar.gz The default root directory becomes: C:\\Windows\\System32\\qdbroot Download the binary: questdb-9.1.0-no-jre-bin.tar.gz This package does not embed Java. Use this if there is no package for your platform, such as ARM Linux. Requires local Java 17. To check your installed version: java -version If you do not have Java, install one of the following: AdoptOpenJDK Amazon Corretto OpenJDK Oracle Java Other Java distributions might work but are not tested regularly. For example, it is known that Azul Zing 17 is incompatible. For environment variable, point JAVA_HOME to your Java 17 installation folder. Run QuestDB ​ Linux/No JVM macOS (Homebrew) Windows ./questdb.sh [start|stop|status] [-d dir] [-f] [-n] [-t tag] Option Description -d Expects a dir directory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see the default root section below. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "see the default root section below. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console . Without this option, the Web Console is cached and deployed only when missing. -n Do not respond to the HUP signal. This keeps QuestDB alive after you close the terminal window where you started it. questdb [start|stop|status] [-d dir] [-f] [-n] [-t tag] Option Description -d Expects a dir directory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see the default root section below. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console . Without this option, the Web Console is cached and deployed only when missing. -n Do not respond to the HUP signal. This keeps QuestDB alive after you close the terminal window where you started it."}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": ". Without this option, the Web Console is cached and deployed only when missing. -n Do not respond to the HUP signal. This keeps QuestDB alive after you close the terminal window where you started it. questdb.exe [start|stop|status|install|remove] \\ [-d dir] [-f] [-j JAVA_HOME] [-t tag] Option Description install Installs the Windows QuestDB service. The service will start automatically at startup. remove Removes the Windows QuestDB service. It will no longer start at startup. -d Expects a dir directory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see the default root section below. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console . Without this option, the Web Console is cached and deployed only when missing. -j Windows only! This option allows to specify a path to JAVA_HOME . Enjoy QuestDB ​ Congratulations! 🎉 QuestDB is now running. The QuestDB Web Console is available by default at: http://localhost:9000 ."}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "indows only! This option allows to specify a path to JAVA_HOME . Enjoy QuestDB ​ Congratulations! 🎉 QuestDB is now running. The QuestDB Web Console is available by default at: http://localhost:9000 . Also by default, QuestDB will use the following ports: 9000 - REST API and Web Console 9000 - InfluxDB Line Protocol (ILP) 8812 - Postgres Wire Protocol (PGWire) 9003 - Min health server With that, you're ready to bring your data and enjoy the high performance and reliability of QuestDB. Bring your data ​ Now... Time to really blast-off. 🚀 Next up: Bring your data - the life blood of any database. Choose from one of our premium ingest-only language clients: C & C++ High-performance client for systems programming and embedded applications. Read more .NET Cross-platform client for building applications with .NET technologies. Read more Go An open-source programming language supported by Google with built-in concurrency. Read more Java Platform-independent client for enterprise applications and Android development. Read more Node.js Node.js® is an open-source, cross-platform JavaScript runtime environment. Read more Python Python is a programming language that lets you work quickly and in"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "nd Android development. Read more Node.js Node.js® is an open-source, cross-platform JavaScript runtime environment. Read more Python Python is a programming language that lets you work quickly and integrate systems more effectively. Read more Rust Systems programming language focused on safety, speed, and concurrency. Read more Want more options? See the ingestion overview . Create new data ​ No data yet and still want to trial QuestDB? There are several quick options: QuestDB demo instance : Hosted, fully loaded and ready to go. Quickly explore the Web Console and SQL syntax. Create my first data set guide : create tables, use rnd_ functions and make your own data. Sample dataset repos : IoT, e-commerce, finance or git logs? Check them out! Quick start repos : Code-based quick starts that cover ingestion, querying and data visualization using common programming languages and use cases. Also, a cat in a tracksuit. Time series streaming analytics template : A handy template for near real-time analytics using open source technologies. Learn QuestDB ​ For operators or developers looking for next steps to run an efficient instance, see: Capacity planning for recommended configurations"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Introduction", "text": "al-time analytics using open source technologies. Learn QuestDB ​ For operators or developers looking for next steps to run an efficient instance, see: Capacity planning for recommended configurations for operating QuestDB in production Configuration to see all of the available options in your server.conf file Design for performance for tips and tricks Visualize with Grafana to create useful dashboards and visualizations from your data Looking for inspiration? Checkout our real-time crypto dashboard ."}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Quick start", "text": "This guide will get your first QuestDB instance running.\n\nAs the goal is to, well, start quickly , we'll presume defaults.\n\nOnce running, we'll provide guides for inserting data, configuration and production hosting.\n\nQuestDB already running? Jump ahead! Select a first-party client or ingest method."}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Install QuestDB ​", "text": "Choose from the following options:\n\nDocker\n\nHomebrew\n\nBinaries"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Docker ​", "text": "To use Docker, one must have Docker. You can find installation guides for your platform on the official documentation .\n\nOnce Docker is installed, you will need to pull QuestDB's image from Docker Hub and create a container:\n\ndocker run \\ -p 9000:9000 -p 9009:9009 -p 8812:8812 -p 9003:9003 \\ questdb/questdb:9.1.0\n\ndocker run \\ -p 9000:9000 -p 9009:9009 -p 8812:8812 -p 9003:9003 \\ questdb/questdb:9.1.0\n\ndocker run \\ -p 9000:9000 -p 9009:9009 -p 8812:8812 -p 9003:9003 \\ questdb/questdb:9.1.0\n\ndocker run \\ -p 9000:9000 -p 9009:9009 -p 8812:8812 -p 9003:9003 \\ questdb/questdb:9.1.0\n\nFor deeper instructions, see the Docker deployment guide ."}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Homebrew ​", "text": "To install QuestDB via Homebrew , run the following command:\n\nbrew install questdb\n\nbrew install questdb\n\nbrew install questdb\n\nbrew install questdb\n\nOn macOS, the location of the root directory of QuestDB and server configuration files depending on the chip:\n\nApple Silicon (M1/M2/M*) chip: /opt/homebrew/var/questdb\n\nApple Silicon (M1/M2/M*) chip: /opt/homebrew/var/questdb\n\n/opt/homebrew/var/questdb\n\nIntel chip: /usr/local/var/questdb\n\nIntel chip: /usr/local/var/questdb\n\n/usr/local/var/questdb"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Binaries ​", "text": "Download and run QuestDB via binaries.\n\nSelect your platform of choice:\n\nLinux Windows Any (no JVM) Download the binary: questdb-9.1.0-rt-linux-x86-64.tar.gz Next, unpack it: tar -xvf questdb-9.1.0-rt-linux-x86-64.tar.gz The default directory becomes: $HOME/.questdb Download the executable: questdb-9.1.0-rt-windows-x86-64.tar.gz The default root directory becomes: C:\\Windows\\System32\\qdbroot Download the binary: questdb-9.1.0-no-jre-bin.tar.gz This package does not embed Java. Use this if there is no package for your platform, such as ARM Linux. Requires local Java 17. To check your installed version: java -version If you do not have Java, install one of the following: AdoptOpenJDK Amazon Corretto OpenJDK Oracle Java Other Java distributions might work but are not tested regularly. For example, it is known that Azul Zing 17 is incompatible. For environment variable, point JAVA_HOME to your Java 17 installation folder.\n\nLinux\n\nWindows\n\nAny (no JVM)\n\nDownload the binary: questdb-9.1.0-rt-linux-x86-64.tar.gz Next, unpack it: tar -xvf questdb-9.1.0-rt-linux-x86-64.tar.gz The default directory becomes: $HOME/.questdb Download the executable: questdb-9.1.0-rt-windows-x86-64.tar.gz The de"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Binaries ​", "text": "-linux-x86-64.tar.gz Next, unpack it: tar -xvf questdb-9.1.0-rt-linux-x86-64.tar.gz The default directory becomes: $HOME/.questdb Download the executable: questdb-9.1.0-rt-windows-x86-64.tar.gz The default root directory becomes: C:\\Windows\\System32\\qdbroot Download the binary: questdb-9.1.0-no-jre-bin.tar.gz This package does not embed Java. Use this if there is no package for your platform, such as ARM Linux. Requires local Java 17. To check your installed version: java -version If you do not have Java, install one of the following: AdoptOpenJDK Amazon Corretto OpenJDK Oracle Java Other Java distributions might work but are not tested regularly. For example, it is known that Azul Zing 17 is incompatible. For environment variable, point JAVA_HOME to your Java 17 installation folder.\n\nDownload the binary: questdb-9.1.0-rt-linux-x86-64.tar.gz Next, unpack it: tar -xvf questdb-9.1.0-rt-linux-x86-64.tar.gz The default directory becomes: $HOME/.questdb\n\ntar -xvf questdb-9.1.0-rt-linux-x86-64.tar.gz\n\ntar -xvf questdb-9.1.0-rt-linux-x86-64.tar.gz\n\ntar -xvf questdb-9.1.0-rt-linux-x86-64.tar.gz\n\ntar -xvf questdb-9.1.0-rt-linux-x86-64.tar.gz\n\n$HOME/.questdb\n\n$HOME/.questdb\n\n$HOME/.questdb"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Binaries ​", "text": "64.tar.gz\n\ntar -xvf questdb-9.1.0-rt-linux-x86-64.tar.gz\n\ntar -xvf questdb-9.1.0-rt-linux-x86-64.tar.gz\n\ntar -xvf questdb-9.1.0-rt-linux-x86-64.tar.gz\n\n$HOME/.questdb\n\n$HOME/.questdb\n\n$HOME/.questdb\n\n$HOME/.questdb\n\nDownload the executable: questdb-9.1.0-rt-windows-x86-64.tar.gz The default root directory becomes: C:\\Windows\\System32\\qdbroot\n\nC:\\Windows\\System32\\qdbroot\n\nC:\\Windows\\System32\\qdbroot\n\nC:\\Windows\\System32\\qdbroot\n\nC:\\Windows\\System32\\qdbroot\n\nDownload the binary: questdb-9.1.0-no-jre-bin.tar.gz This package does not embed Java. Use this if there is no package for your platform, such as ARM Linux. Requires local Java 17. To check your installed version: java -version If you do not have Java, install one of the following: AdoptOpenJDK Amazon Corretto OpenJDK Oracle Java Other Java distributions might work but are not tested regularly. For example, it is known that Azul Zing 17 is incompatible. For environment variable, point JAVA_HOME to your Java 17 installation folder.\n\nDownload the binary:\n\nquestdb-9.1.0-no-jre-bin.tar.gz\n\nThis package does not embed Java.\n\nUse this if there is no package for your platform, such as ARM Linux.\n\nRequires local Java 17.\n\nTo check your i"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Binaries ​", "text": "ownload the binary:\n\nquestdb-9.1.0-no-jre-bin.tar.gz\n\nThis package does not embed Java.\n\nUse this if there is no package for your platform, such as ARM Linux.\n\nRequires local Java 17.\n\nTo check your installed version:\n\njava -version\n\njava -version\n\njava -version\n\njava -version\n\nIf you do not have Java, install one of the following:\n\nAdoptOpenJDK\n\nAmazon Corretto\n\nOpenJDK\n\nOracle Java\n\nOther Java distributions might work but are not tested regularly. For example, it is known that Azul Zing 17 is incompatible.\n\nFor environment variable, point JAVA_HOME to your Java 17 installation folder.\n\nJAVA_HOME"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Run QuestDB ​", "text": "Linux/No JVM macOS (Homebrew) Windows ./questdb.sh [start|stop|status] [-d dir] [-f] [-n] [-t tag] Option Description -d Expects a dir directory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see the default root section below. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console . Without this option, the Web Console is cached and deployed only when missing. -n Do not respond to the HUP signal. This keeps QuestDB alive after you close the terminal window where you started it. questdb [start|stop|status] [-d dir] [-f] [-n] [-t tag] Option Description -d Expects a dir directory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see the default root section below. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Run QuestDB ​", "text": "elow. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console . Without this option, the Web Console is cached and deployed only when missing. -n Do not respond to the HUP signal. This keeps QuestDB alive after you close the terminal window where you started it. questdb.exe [start|stop|status|install|remove] \\ [-d dir] [-f] [-j JAVA_HOME] [-t tag] Option Description install Installs the Windows QuestDB service. The service will start automatically at startup. remove Removes the Windows QuestDB service. It will no longer start at startup. -d Expects a dir directory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see the default root section below. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console . Without this option, th"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Run QuestDB ​", "text": "on allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console . Without this option, the Web Console is cached and deployed only when missing. -j Windows only! This option allows to specify a path to JAVA_HOME .\n\nLinux/No JVM\n\nmacOS (Homebrew)\n\nWindows\n\n./questdb.sh [start|stop|status] [-d dir] [-f] [-n] [-t tag] Option Description -d Expects a dir directory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see the default root section below. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console . Without this option, the Web Console is cached and deployed only when missing. -n Do not respond to the HUP signal. This keeps QuestDB alive after you close the terminal window where you started it. questdb [start|stop|status] [-d dir] [-f] [-n] [-t tag] Option Description -d Expects a dir directory value which is a"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Run QuestDB ​", "text": "is keeps QuestDB alive after you close the terminal window where you started it. questdb [start|stop|status] [-d dir] [-f] [-n] [-t tag] Option Description -d Expects a dir directory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see the default root section below. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console . Without this option, the Web Console is cached and deployed only when missing. -n Do not respond to the HUP signal. This keeps QuestDB alive after you close the terminal window where you started it. questdb.exe [start|stop|status|install|remove] \\ [-d dir] [-f] [-j JAVA_HOME] [-t tag] Option Description install Installs the Windows QuestDB service. The service will start automatically at startup. remove Removes the Windows QuestDB service. It will no longer start at startup. -d Expects a dir directory value which is a folder that will be used as QuestDB's root directory. For more information and the de"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Run QuestDB ​", "text": "moves the Windows QuestDB service. It will no longer start at startup. -d Expects a dir directory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see the default root section below. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console . Without this option, the Web Console is cached and deployed only when missing. -j Windows only! This option allows to specify a path to JAVA_HOME .\n\n./questdb.sh [start|stop|status] [-d dir] [-f] [-n] [-t tag] Option Description -d Expects a dir directory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see the default root section below. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console . Without this option, the Web C"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Run QuestDB ​", "text": "ws users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console . Without this option, the Web Console is cached and deployed only when missing. -n Do not respond to the HUP signal. This keeps QuestDB alive after you close the terminal window where you started it.\n\n./questdb.sh [start|stop|status] [-d dir] [-f] [-n] [-t tag]\n\n./questdb.sh [start|stop|status] [-d dir] [-f] [-n] [-t tag]\n\n./questdb.sh [start|stop|status] [-d dir] [-f] [-n] [-t tag]\n\n./questdb.sh [start|stop|status] [-d dir] [-f] [-n] [-t tag]\n\n-d\n\ndir\n\n-t\n\ntag\n\nquestdb\n\n-f\n\n-n\n\nquestdb [start|stop|status] [-d dir] [-f] [-n] [-t tag] Option Description -d Expects a dir directory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see the default root section below. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console . Without this option, the Web Console"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Run QuestDB ​", "text": "s to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console . Without this option, the Web Console is cached and deployed only when missing. -n Do not respond to the HUP signal. This keeps QuestDB alive after you close the terminal window where you started it.\n\nquestdb [start|stop|status] [-d dir] [-f] [-n] [-t tag]\n\nquestdb [start|stop|status] [-d dir] [-f] [-n] [-t tag]\n\nquestdb [start|stop|status] [-d dir] [-f] [-n] [-t tag]\n\nquestdb [start|stop|status] [-d dir] [-f] [-n] [-t tag]\n\n-d\n\ndir\n\n-t\n\ntag\n\nquestdb\n\n-f\n\n-n\n\nquestdb.exe [start|stop|status|install|remove] \\ [-d dir] [-f] [-j JAVA_HOME] [-t tag] Option Description install Installs the Windows QuestDB service. The service will start automatically at startup. remove Removes the Windows QuestDB service. It will no longer start at startup. -d Expects a dir directory value which is a folder that will be used as QuestDB's root directory. For more information and the default values, see the default root section below. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several Ques"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Run QuestDB ​", "text": "ory. For more information and the default values, see the default root section below. -t Expects a tag string value which will be as a tag for the service. This option allows users to run several QuestDB services and manage them separately. If this option is omitted, the default tag will be questdb . -f Force re-deploying the Web Console . Without this option, the Web Console is cached and deployed only when missing. -j Windows only! This option allows to specify a path to JAVA_HOME .\n\nquestdb.exe [start|stop|status|install|remove] \\ [-d dir] [-f] [-j JAVA_HOME] [-t tag]\n\nquestdb.exe [start|stop|status|install|remove] \\ [-d dir] [-f] [-j JAVA_HOME] [-t tag]\n\nquestdb.exe [start|stop|status|install|remove] \\ [-d dir] [-f] [-j JAVA_HOME] [-t tag]\n\nquestdb.exe [start|stop|status|install|remove] \\ [-d dir] [-f] [-j JAVA_HOME] [-t tag]\n\ninstall\n\nremove\n\n-d\n\ndir\n\n-t\n\ntag\n\nquestdb\n\n-f\n\n-j\n\nJAVA_HOME"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Enjoy QuestDB ​", "text": "Congratulations! 🎉 QuestDB is now running.\n\nThe QuestDB Web Console is available by default at: http://localhost:9000 .\n\nAlso by default, QuestDB will use the following ports:\n\n9000 - REST API and Web Console\n\n9000\n\n9000 - InfluxDB Line Protocol (ILP)\n\n9000\n\n8812 - Postgres Wire Protocol (PGWire)\n\n8812\n\n9003 - Min health server\n\n9003\n\nWith that, you're ready to bring your data and enjoy the high performance and reliability of QuestDB."}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Bring your data ​", "text": "Now... Time to really blast-off. 🚀\n\nNext up: Bring your data - the life blood of any database.\n\nChoose from one of our premium ingest-only language clients:\n\nC & C++ High-performance client for systems programming and embedded applications. Read more .NET Cross-platform client for building applications with .NET technologies. Read more Go An open-source programming language supported by Google with built-in concurrency. Read more Java Platform-independent client for enterprise applications and Android development. Read more Node.js Node.js® is an open-source, cross-platform JavaScript runtime environment. Read more Python Python is a programming language that lets you work quickly and integrate systems more effectively. Read more Rust Systems programming language focused on safety, speed, and concurrency. Read more\n\nC & C++ High-performance client for systems programming and embedded applications. Read more .NET Cross-platform client for building applications with .NET technologies. Read more Go An open-source programming language supported by Google with built-in concurrency. Read more Java Platform-independent client for enterprise applications and Android development. Read more"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Bring your data ​", "text": "ad more Go An open-source programming language supported by Google with built-in concurrency. Read more Java Platform-independent client for enterprise applications and Android development. Read more Node.js Node.js® is an open-source, cross-platform JavaScript runtime environment. Read more Python Python is a programming language that lets you work quickly and integrate systems more effectively. Read more Rust Systems programming language focused on safety, speed, and concurrency. Read more\n\nC & C++ High-performance client for systems programming and embedded applications. Read more\n\nC & C++ High-performance client for systems programming and embedded applications. Read more"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "C & C++", "text": "High-performance client for systems programming and embedded applications.\n\nRead more\n\n.NET Cross-platform client for building applications with .NET technologies. Read more\n\n.NET Cross-platform client for building applications with .NET technologies. Read more"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": ".NET", "text": "Cross-platform client for building applications with .NET technologies.\n\nRead more\n\nGo An open-source programming language supported by Google with built-in concurrency. Read more\n\nGo An open-source programming language supported by Google with built-in concurrency. Read more"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Go", "text": "An open-source programming language supported by Google with built-in concurrency.\n\nRead more\n\nJava Platform-independent client for enterprise applications and Android development. Read more\n\nJava Platform-independent client for enterprise applications and Android development. Read more"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Java", "text": "Platform-independent client for enterprise applications and Android development.\n\nRead more\n\nNode.js Node.js® is an open-source, cross-platform JavaScript runtime environment. Read more\n\nNode.js Node.js® is an open-source, cross-platform JavaScript runtime environment. Read more"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Node.js", "text": "Node.js® is an open-source, cross-platform JavaScript runtime environment.\n\nRead more\n\nPython Python is a programming language that lets you work quickly and integrate systems more effectively. Read more\n\nPython Python is a programming language that lets you work quickly and integrate systems more effectively. Read more"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Python", "text": "Python is a programming language that lets you work quickly and integrate systems more effectively.\n\nRead more\n\nRust Systems programming language focused on safety, speed, and concurrency. Read more\n\nRust Systems programming language focused on safety, speed, and concurrency. Read more"}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Rust", "text": "Systems programming language focused on safety, speed, and concurrency.\n\nRead more\n\nWant more options? See the ingestion overview ."}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Create new data ​", "text": "No data yet and still want to trial QuestDB?\n\nThere are several quick options:\n\nQuestDB demo instance : Hosted, fully loaded and ready to go. Quickly explore the Web Console and SQL syntax.\n\nCreate my first data set guide : create tables, use rnd_ functions and make your own data.\n\nrnd_\n\nSample dataset repos : IoT, e-commerce, finance or git logs? Check them out!\n\nQuick start repos : Code-based quick starts that cover ingestion, querying and data visualization using common programming languages and use cases. Also, a cat in a tracksuit.\n\nTime series streaming analytics template : A handy template for near real-time analytics using open source technologies."}
{"source_url": "https://questdb.com/docs/quick-start", "title": "Quick start", "section": "Learn QuestDB ​", "text": "For operators or developers looking for next steps to run an efficient instance, see:\n\nCapacity planning for recommended configurations for operating QuestDB in production\n\nConfiguration to see all of the available options in your server.conf file\n\nserver.conf\n\nDesign for performance for tips and tricks\n\nVisualize with Grafana to create useful dashboards and visualizations from your data Looking for inspiration? Checkout our real-time crypto dashboard .\n\nLooking for inspiration? Checkout our real-time crypto dashboard .\n\nEdit this page\n\nEdit this page\n\nPrevious\n\nIntroduction\n\nNext\n\nWhy QuestDB?\n\nInstall QuestDB Docker Homebrew Binaries Run QuestDB Enjoy QuestDB Bring your data Create new data Learn QuestDB\n\nInstall QuestDB Docker Homebrew Binaries Run QuestDB Enjoy QuestDB Bring your data Create new data Learn QuestDB\n\nInstall QuestDB Docker Homebrew Binaries\n\nDocker\n\nHomebrew\n\nBinaries\n\nRun QuestDB\n\nEnjoy QuestDB\n\nBring your data Create new data\n\nCreate new data\n\nLearn QuestDB"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Introduction", "text": "Why QuestDB? On this page Why QuestDB? This pages provides a brief overview on: Top QuestDB features Benefits of QuestDB Where to next? Support Just want to build? Jump to the quick start guide. Top QuestDB features ​ QuestDB is applied within cutting edge use cases around the world. Developers are most enthusiastic about the following key features: Massive ingestion handling & throughput ​ If you are running into throughput bottlenecks using an existing storage engine or time series database, QuestDB can help. High performance deduplication & out-of-order indexing ​ High data cardinality will not lead to performance degradation. Hardware efficiency ​ Strong, cost-saving performance on very mninimal hardware, including sensors and Raspberry Pi. SQL with time series extensions ​ Fast, SIMD-optimized SQL extensions to cruise through querying and analysis. No obscure domain-specific languages required. Greatest hits include: SAMPLE BY summarizes data into chunks based on a specified time interval, from a year to a microsecond WHERE IN to compress time ranges into concise intervals LATEST ON for latest values within multiple series within a table ASOF JOIN to associate timestamps betwe"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Introduction", "text": "interval, from a year to a microsecond WHERE IN to compress time ranges into concise intervals LATEST ON for latest values within multiple series within a table ASOF JOIN to associate timestamps between a series based on proximity; no extra indices required Materialized Views for pre-computing and automatically refreshing complex query results to optimize performance Benefits of QuestDB ​ To avoid ingestion bottlenecks, high performance data ingestion is essential. But performance is only part of the story. Efficiency measures how well a database performs relative to its available resources. QuestDB, on maximal hardware, significantly outperforms peers: Benchmark results for QuestDB 7.3.10, InfluxDB 2.7.4 and Timescale 2.14.2 However, on less robust hardware the difference is even more pronounced, as seen in the following benchmark. Even on hardware as light as a Raspberry Pi 5, QuestDB outperforms competitors on stronger hardware: QuestDB on an RPi5 outperforming competitors on optimal hardware Beyond performance and efficiency, with a specialized time-series database , you don't need to worry about: out-of-order data duplicates exactly one semantics frequency of ingestion many ot"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Introduction", "text": "l hardware Beyond performance and efficiency, with a specialized time-series database , you don't need to worry about: out-of-order data duplicates exactly one semantics frequency of ingestion many other details you will find in demanding real-world scenarios QuestDB provides simplified, hyper-fast data ingestion with tremendous efficiency and therefore value. Write blazing-fast queries and create real-time Grafana via familiar SQL: Navigate time with SQL Demo this query SELECT timestamp, symbol, first(price) AS open, last(price) AS close, min(price), max(price), sum(amount) AS volume FROM trades WHERE timestamp > dateadd('d', -1, now()) SAMPLE BY 15m; Intrigued? The best way to see whether QuestDB is right for you is to try it out. Click Demo this query in the snippet above to visit our demo instance and experiment. To bring your own data and learn more, keep reading! Where to next? ​ You'll be inserting data and generating valuable queries in little time. First, the quick start guide will get you running. Choose from one of our premium ingest-only language clients: C & C++ High-performance client for systems programming and embedded applications. Read more .NET Cross-platform cli"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Introduction", "text": "will get you running. Choose from one of our premium ingest-only language clients: C & C++ High-performance client for systems programming and embedded applications. Read more .NET Cross-platform client for building applications with .NET technologies. Read more Go An open-source programming language supported by Google with built-in concurrency. Read more Java Platform-independent client for enterprise applications and Android development. Read more Node.js Node.js® is an open-source, cross-platform JavaScript runtime environment. Read more Python Python is a programming language that lets you work quickly and integrate systems more effectively. Read more Rust Systems programming language focused on safety, speed, and concurrency. Read more From there, you can learn more about what's to offer. Ingestion overview want to see all available ingestion options? Checkout the overview. Query & SQL Overview learn how to query QuestDB Web Console for quick SQL queries, charting and CSV upload/export functionality Grafana guide to visualize your data as beautiful and functional charts. Capacity planning to optimize your QuestDB deployment for production workloads. Support ​ We are happy to"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Introduction", "text": "/export functionality Grafana guide to visualize your data as beautiful and functional charts. Capacity planning to optimize your QuestDB deployment for production workloads. Support ​ We are happy to help with any question you may have. The team loves a good performance optimization challenge! Feel free to reach out using the following channels: Raise an issue on GitHub Join our community forums QuestDB on Stack Overflow or email us at hello@questdb.io Edit this page Previous Quick start Next Schema Design Top QuestDB features Benefits of QuestDB Where to next? Support\n\nWhy QuestDB? On this page Why QuestDB? This pages provides a brief overview on: Top QuestDB features Benefits of QuestDB Where to next? Support Just want to build? Jump to the quick start guide. Top QuestDB features ​ QuestDB is applied within cutting edge use cases around the world. Developers are most enthusiastic about the following key features: Massive ingestion handling & throughput ​ If you are running into throughput bottlenecks using an existing storage engine or time series database, QuestDB can help. High performance deduplication & out-of-order indexing ​ High data cardinality will not lead to performan"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Introduction", "text": "ughput bottlenecks using an existing storage engine or time series database, QuestDB can help. High performance deduplication & out-of-order indexing ​ High data cardinality will not lead to performance degradation. Hardware efficiency ​ Strong, cost-saving performance on very mninimal hardware, including sensors and Raspberry Pi. SQL with time series extensions ​ Fast, SIMD-optimized SQL extensions to cruise through querying and analysis. No obscure domain-specific languages required. Greatest hits include: SAMPLE BY summarizes data into chunks based on a specified time interval, from a year to a microsecond WHERE IN to compress time ranges into concise intervals LATEST ON for latest values within multiple series within a table ASOF JOIN to associate timestamps between a series based on proximity; no extra indices required Materialized Views for pre-computing and automatically refreshing complex query results to optimize performance Benefits of QuestDB ​ To avoid ingestion bottlenecks, high performance data ingestion is essential. But performance is only part of the story. Efficiency measures how well a database performs relative to its available resources. QuestDB, on maximal har"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Introduction", "text": ", high performance data ingestion is essential. But performance is only part of the story. Efficiency measures how well a database performs relative to its available resources. QuestDB, on maximal hardware, significantly outperforms peers: Benchmark results for QuestDB 7.3.10, InfluxDB 2.7.4 and Timescale 2.14.2 However, on less robust hardware the difference is even more pronounced, as seen in the following benchmark. Even on hardware as light as a Raspberry Pi 5, QuestDB outperforms competitors on stronger hardware: QuestDB on an RPi5 outperforming competitors on optimal hardware Beyond performance and efficiency, with a specialized time-series database , you don't need to worry about: out-of-order data duplicates exactly one semantics frequency of ingestion many other details you will find in demanding real-world scenarios QuestDB provides simplified, hyper-fast data ingestion with tremendous efficiency and therefore value. Write blazing-fast queries and create real-time Grafana via familiar SQL: Navigate time with SQL Demo this query SELECT timestamp, symbol, first(price) AS open, last(price) AS close, min(price), max(price), sum(amount) AS volume FROM trades WHERE timestamp >"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Introduction", "text": "familiar SQL: Navigate time with SQL Demo this query SELECT timestamp, symbol, first(price) AS open, last(price) AS close, min(price), max(price), sum(amount) AS volume FROM trades WHERE timestamp > dateadd('d', -1, now()) SAMPLE BY 15m; Intrigued? The best way to see whether QuestDB is right for you is to try it out. Click Demo this query in the snippet above to visit our demo instance and experiment. To bring your own data and learn more, keep reading! Where to next? ​ You'll be inserting data and generating valuable queries in little time. First, the quick start guide will get you running. Choose from one of our premium ingest-only language clients: C & C++ High-performance client for systems programming and embedded applications. Read more .NET Cross-platform client for building applications with .NET technologies. Read more Go An open-source programming language supported by Google with built-in concurrency. Read more Java Platform-independent client for enterprise applications and Android development. Read more Node.js Node.js® is an open-source, cross-platform JavaScript runtime environment. Read more Python Python is a programming language that lets you work quickly and in"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Introduction", "text": "nd Android development. Read more Node.js Node.js® is an open-source, cross-platform JavaScript runtime environment. Read more Python Python is a programming language that lets you work quickly and integrate systems more effectively. Read more Rust Systems programming language focused on safety, speed, and concurrency. Read more From there, you can learn more about what's to offer. Ingestion overview want to see all available ingestion options? Checkout the overview. Query & SQL Overview learn how to query QuestDB Web Console for quick SQL queries, charting and CSV upload/export functionality Grafana guide to visualize your data as beautiful and functional charts. Capacity planning to optimize your QuestDB deployment for production workloads. Support ​ We are happy to help with any question you may have. The team loves a good performance optimization challenge! Feel free to reach out using the following channels: Raise an issue on GitHub Join our community forums QuestDB on Stack Overflow or email us at hello@questdb.io Edit this page Previous Quick start Next Schema Design Top QuestDB features Benefits of QuestDB Where to next? Support\n\nWhy QuestDB? On this page Why QuestDB? This"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Introduction", "text": "low or email us at hello@questdb.io Edit this page Previous Quick start Next Schema Design Top QuestDB features Benefits of QuestDB Where to next? Support\n\nWhy QuestDB? On this page Why QuestDB? This pages provides a brief overview on: Top QuestDB features Benefits of QuestDB Where to next? Support Just want to build? Jump to the quick start guide. Top QuestDB features ​ QuestDB is applied within cutting edge use cases around the world. Developers are most enthusiastic about the following key features: Massive ingestion handling & throughput ​ If you are running into throughput bottlenecks using an existing storage engine or time series database, QuestDB can help. High performance deduplication & out-of-order indexing ​ High data cardinality will not lead to performance degradation. Hardware efficiency ​ Strong, cost-saving performance on very mninimal hardware, including sensors and Raspberry Pi. SQL with time series extensions ​ Fast, SIMD-optimized SQL extensions to cruise through querying and analysis. No obscure domain-specific languages required. Greatest hits include: SAMPLE BY summarizes data into chunks based on a specified time interval, from a year to a microsecond WHERE"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Introduction", "text": "uerying and analysis. No obscure domain-specific languages required. Greatest hits include: SAMPLE BY summarizes data into chunks based on a specified time interval, from a year to a microsecond WHERE IN to compress time ranges into concise intervals LATEST ON for latest values within multiple series within a table ASOF JOIN to associate timestamps between a series based on proximity; no extra indices required Materialized Views for pre-computing and automatically refreshing complex query results to optimize performance Benefits of QuestDB ​ To avoid ingestion bottlenecks, high performance data ingestion is essential. But performance is only part of the story. Efficiency measures how well a database performs relative to its available resources. QuestDB, on maximal hardware, significantly outperforms peers: Benchmark results for QuestDB 7.3.10, InfluxDB 2.7.4 and Timescale 2.14.2 However, on less robust hardware the difference is even more pronounced, as seen in the following benchmark. Even on hardware as light as a Raspberry Pi 5, QuestDB outperforms competitors on stronger hardware: QuestDB on an RPi5 outperforming competitors on optimal hardware Beyond performance and efficiency"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Introduction", "text": "Even on hardware as light as a Raspberry Pi 5, QuestDB outperforms competitors on stronger hardware: QuestDB on an RPi5 outperforming competitors on optimal hardware Beyond performance and efficiency, with a specialized time-series database , you don't need to worry about: out-of-order data duplicates exactly one semantics frequency of ingestion many other details you will find in demanding real-world scenarios QuestDB provides simplified, hyper-fast data ingestion with tremendous efficiency and therefore value. Write blazing-fast queries and create real-time Grafana via familiar SQL: Navigate time with SQL Demo this query SELECT timestamp, symbol, first(price) AS open, last(price) AS close, min(price), max(price), sum(amount) AS volume FROM trades WHERE timestamp > dateadd('d', -1, now()) SAMPLE BY 15m; Intrigued? The best way to see whether QuestDB is right for you is to try it out. Click Demo this query in the snippet above to visit our demo instance and experiment. To bring your own data and learn more, keep reading! Where to next? ​ You'll be inserting data and generating valuable queries in little time. First, the quick start guide will get you running. Choose from one of ou"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Introduction", "text": "wn data and learn more, keep reading! Where to next? ​ You'll be inserting data and generating valuable queries in little time. First, the quick start guide will get you running. Choose from one of our premium ingest-only language clients: C & C++ High-performance client for systems programming and embedded applications. Read more .NET Cross-platform client for building applications with .NET technologies. Read more Go An open-source programming language supported by Google with built-in concurrency. Read more Java Platform-independent client for enterprise applications and Android development. Read more Node.js Node.js® is an open-source, cross-platform JavaScript runtime environment. Read more Python Python is a programming language that lets you work quickly and integrate systems more effectively. Read more Rust Systems programming language focused on safety, speed, and concurrency. Read more From there, you can learn more about what's to offer. Ingestion overview want to see all available ingestion options? Checkout the overview. Query & SQL Overview learn how to query QuestDB Web Console for quick SQL queries, charting and CSV upload/export functionality Grafana guide to visua"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Introduction", "text": "ailable ingestion options? Checkout the overview. Query & SQL Overview learn how to query QuestDB Web Console for quick SQL queries, charting and CSV upload/export functionality Grafana guide to visualize your data as beautiful and functional charts. Capacity planning to optimize your QuestDB deployment for production workloads. Support ​ We are happy to help with any question you may have. The team loves a good performance optimization challenge! Feel free to reach out using the following channels: Raise an issue on GitHub Join our community forums QuestDB on Stack Overflow or email us at hello@questdb.io Edit this page Previous Quick start Next Schema Design\n\nWhy QuestDB? On this page Why QuestDB? This pages provides a brief overview on: Top QuestDB features Benefits of QuestDB Where to next? Support Just want to build? Jump to the quick start guide. Top QuestDB features ​ QuestDB is applied within cutting edge use cases around the world. Developers are most enthusiastic about the following key features: Massive ingestion handling & throughput ​ If you are running into throughput bottlenecks using an existing storage engine or time series database, QuestDB can help. High performa"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Introduction", "text": "llowing key features: Massive ingestion handling & throughput ​ If you are running into throughput bottlenecks using an existing storage engine or time series database, QuestDB can help. High performance deduplication & out-of-order indexing ​ High data cardinality will not lead to performance degradation. Hardware efficiency ​ Strong, cost-saving performance on very mninimal hardware, including sensors and Raspberry Pi. SQL with time series extensions ​ Fast, SIMD-optimized SQL extensions to cruise through querying and analysis. No obscure domain-specific languages required. Greatest hits include: SAMPLE BY summarizes data into chunks based on a specified time interval, from a year to a microsecond WHERE IN to compress time ranges into concise intervals LATEST ON for latest values within multiple series within a table ASOF JOIN to associate timestamps between a series based on proximity; no extra indices required Materialized Views for pre-computing and automatically refreshing complex query results to optimize performance Benefits of QuestDB ​ To avoid ingestion bottlenecks, high performance data ingestion is essential. But performance is only part of the story. Efficiency measur"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Introduction", "text": "x query results to optimize performance Benefits of QuestDB ​ To avoid ingestion bottlenecks, high performance data ingestion is essential. But performance is only part of the story. Efficiency measures how well a database performs relative to its available resources. QuestDB, on maximal hardware, significantly outperforms peers: Benchmark results for QuestDB 7.3.10, InfluxDB 2.7.4 and Timescale 2.14.2 However, on less robust hardware the difference is even more pronounced, as seen in the following benchmark. Even on hardware as light as a Raspberry Pi 5, QuestDB outperforms competitors on stronger hardware: QuestDB on an RPi5 outperforming competitors on optimal hardware Beyond performance and efficiency, with a specialized time-series database , you don't need to worry about: out-of-order data duplicates exactly one semantics frequency of ingestion many other details you will find in demanding real-world scenarios QuestDB provides simplified, hyper-fast data ingestion with tremendous efficiency and therefore value. Write blazing-fast queries and create real-time Grafana via familiar SQL: Navigate time with SQL Demo this query SELECT timestamp, symbol, first(price) AS open, last(p"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Introduction", "text": "efficiency and therefore value. Write blazing-fast queries and create real-time Grafana via familiar SQL: Navigate time with SQL Demo this query SELECT timestamp, symbol, first(price) AS open, last(price) AS close, min(price), max(price), sum(amount) AS volume FROM trades WHERE timestamp > dateadd('d', -1, now()) SAMPLE BY 15m; Intrigued? The best way to see whether QuestDB is right for you is to try it out. Click Demo this query in the snippet above to visit our demo instance and experiment. To bring your own data and learn more, keep reading! Where to next? ​ You'll be inserting data and generating valuable queries in little time. First, the quick start guide will get you running. Choose from one of our premium ingest-only language clients: C & C++ High-performance client for systems programming and embedded applications. Read more .NET Cross-platform client for building applications with .NET technologies. Read more Go An open-source programming language supported by Google with built-in concurrency. Read more Java Platform-independent client for enterprise applications and Android development. Read more Node.js Node.js® is an open-source, cross-platform JavaScript runtime envi"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Introduction", "text": "ilt-in concurrency. Read more Java Platform-independent client for enterprise applications and Android development. Read more Node.js Node.js® is an open-source, cross-platform JavaScript runtime environment. Read more Python Python is a programming language that lets you work quickly and integrate systems more effectively. Read more Rust Systems programming language focused on safety, speed, and concurrency. Read more From there, you can learn more about what's to offer. Ingestion overview want to see all available ingestion options? Checkout the overview. Query & SQL Overview learn how to query QuestDB Web Console for quick SQL queries, charting and CSV upload/export functionality Grafana guide to visualize your data as beautiful and functional charts. Capacity planning to optimize your QuestDB deployment for production workloads. Support ​ We are happy to help with any question you may have. The team loves a good performance optimization challenge! Feel free to reach out using the following channels: Raise an issue on GitHub Join our community forums QuestDB on Stack Overflow or email us at hello@questdb.io Edit this page Previous Quick start Next Schema Design\n\nWhy QuestDB?\n\nOn"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Introduction", "text": "ollowing channels: Raise an issue on GitHub Join our community forums QuestDB on Stack Overflow or email us at hello@questdb.io Edit this page Previous Quick start Next Schema Design\n\nWhy QuestDB?\n\nOn this page\n\nWhy QuestDB? This pages provides a brief overview on: Top QuestDB features Benefits of QuestDB Where to next? Support Just want to build? Jump to the quick start guide. Top QuestDB features ​ QuestDB is applied within cutting edge use cases around the world. Developers are most enthusiastic about the following key features: Massive ingestion handling & throughput ​ If you are running into throughput bottlenecks using an existing storage engine or time series database, QuestDB can help. High performance deduplication & out-of-order indexing ​ High data cardinality will not lead to performance degradation. Hardware efficiency ​ Strong, cost-saving performance on very mninimal hardware, including sensors and Raspberry Pi. SQL with time series extensions ​ Fast, SIMD-optimized SQL extensions to cruise through querying and analysis. No obscure domain-specific languages required. Greatest hits include: SAMPLE BY summarizes data into chunks based on a specified time interval, from"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Introduction", "text": "extensions to cruise through querying and analysis. No obscure domain-specific languages required. Greatest hits include: SAMPLE BY summarizes data into chunks based on a specified time interval, from a year to a microsecond WHERE IN to compress time ranges into concise intervals LATEST ON for latest values within multiple series within a table ASOF JOIN to associate timestamps between a series based on proximity; no extra indices required Materialized Views for pre-computing and automatically refreshing complex query results to optimize performance Benefits of QuestDB ​ To avoid ingestion bottlenecks, high performance data ingestion is essential. But performance is only part of the story. Efficiency measures how well a database performs relative to its available resources. QuestDB, on maximal hardware, significantly outperforms peers: Benchmark results for QuestDB 7.3.10, InfluxDB 2.7.4 and Timescale 2.14.2 However, on less robust hardware the difference is even more pronounced, as seen in the following benchmark. Even on hardware as light as a Raspberry Pi 5, QuestDB outperforms competitors on stronger hardware: QuestDB on an RPi5 outperforming competitors on optimal hardware Bey"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Introduction", "text": "en in the following benchmark. Even on hardware as light as a Raspberry Pi 5, QuestDB outperforms competitors on stronger hardware: QuestDB on an RPi5 outperforming competitors on optimal hardware Beyond performance and efficiency, with a specialized time-series database , you don't need to worry about: out-of-order data duplicates exactly one semantics frequency of ingestion many other details you will find in demanding real-world scenarios QuestDB provides simplified, hyper-fast data ingestion with tremendous efficiency and therefore value. Write blazing-fast queries and create real-time Grafana via familiar SQL: Navigate time with SQL Demo this query SELECT timestamp, symbol, first(price) AS open, last(price) AS close, min(price), max(price), sum(amount) AS volume FROM trades WHERE timestamp > dateadd('d', -1, now()) SAMPLE BY 15m; Intrigued? The best way to see whether QuestDB is right for you is to try it out. Click Demo this query in the snippet above to visit our demo instance and experiment. To bring your own data and learn more, keep reading! Where to next? ​ You'll be inserting data and generating valuable queries in little time. First, the quick start guide will get you"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Introduction", "text": "nd experiment. To bring your own data and learn more, keep reading! Where to next? ​ You'll be inserting data and generating valuable queries in little time. First, the quick start guide will get you running. Choose from one of our premium ingest-only language clients: C & C++ High-performance client for systems programming and embedded applications. Read more .NET Cross-platform client for building applications with .NET technologies. Read more Go An open-source programming language supported by Google with built-in concurrency. Read more Java Platform-independent client for enterprise applications and Android development. Read more Node.js Node.js® is an open-source, cross-platform JavaScript runtime environment. Read more Python Python is a programming language that lets you work quickly and integrate systems more effectively. Read more Rust Systems programming language focused on safety, speed, and concurrency. Read more From there, you can learn more about what's to offer. Ingestion overview want to see all available ingestion options? Checkout the overview. Query & SQL Overview learn how to query QuestDB Web Console for quick SQL queries, charting and CSV upload/export functi"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Introduction", "text": "on overview want to see all available ingestion options? Checkout the overview. Query & SQL Overview learn how to query QuestDB Web Console for quick SQL queries, charting and CSV upload/export functionality Grafana guide to visualize your data as beautiful and functional charts. Capacity planning to optimize your QuestDB deployment for production workloads. Support ​ We are happy to help with any question you may have. The team loves a good performance optimization challenge! Feel free to reach out using the following channels: Raise an issue on GitHub Join our community forums QuestDB on Stack Overflow or email us at hello@questdb.io"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Why QuestDB?", "text": "This pages provides a brief overview on:\n\nTop QuestDB features\n\nBenefits of QuestDB\n\nWhere to next?\n\nSupport\n\nJust want to build? Jump to the quick start guide."}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Top QuestDB features ​", "text": "QuestDB is applied within cutting edge use cases around the world.\n\nDevelopers are most enthusiastic about the following key features:\n\nIf you are running into throughput bottlenecks using an existing storage engine or time series database, QuestDB can help.\n\nHigh data cardinality will not lead to performance degradation.\n\nStrong, cost-saving performance on very mninimal hardware, including sensors and Raspberry Pi.\n\nFast, SIMD-optimized SQL extensions to cruise through querying and analysis.\n\nNo obscure domain-specific languages required.\n\nGreatest hits include:\n\nSAMPLE BY summarizes data into chunks based on a specified time interval, from a year to a microsecond\n\nSAMPLE BY\n\nWHERE IN to compress time ranges into concise intervals\n\nWHERE IN\n\nLATEST ON for latest values within multiple series within a table\n\nLATEST ON\n\nASOF JOIN to associate timestamps between a series based on proximity; no extra indices required\n\nASOF JOIN\n\nMaterialized Views for pre-computing and automatically refreshing complex query results to optimize performance"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Benefits of QuestDB ​", "text": "To avoid ingestion bottlenecks, high performance data ingestion is essential.\n\nBut performance is only part of the story.\n\nEfficiency measures how well a database performs relative to its available resources.\n\nQuestDB, on maximal hardware, significantly outperforms peers:\n\nBenchmark results for QuestDB 7.3.10, InfluxDB 2.7.4 and Timescale 2.14.2\n\nHowever, on less robust hardware the difference is even more pronounced, as seen in the following benchmark.\n\nEven on hardware as light as a Raspberry Pi 5, QuestDB outperforms competitors on stronger hardware:\n\nQuestDB on an RPi5 outperforming competitors on optimal hardware\n\nBeyond performance and efficiency, with a specialized time-series database , you don't need to worry about:\n\nout-of-order data\n\nduplicates\n\nexactly one semantics\n\nfrequency of ingestion\n\nmany other details you will find in demanding real-world scenarios\n\nQuestDB provides simplified, hyper-fast data ingestion with tremendous efficiency and therefore value.\n\nWrite blazing-fast queries and create real-time Grafana via familiar SQL:\n\nNavigate time with SQL Demo this query SELECT timestamp, symbol, first(price) AS open, last(price) AS close, min(price), max(price), sum(am"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Benefits of QuestDB ​", "text": "fast queries and create real-time Grafana via familiar SQL:\n\nNavigate time with SQL Demo this query SELECT timestamp, symbol, first(price) AS open, last(price) AS close, min(price), max(price), sum(amount) AS volume FROM trades WHERE timestamp > dateadd('d', -1, now()) SAMPLE BY 15m;\n\nNavigate time with SQL Demo this query\n\nSELECT timestamp, symbol, first(price) AS open, last(price) AS close, min(price), max(price), sum(amount) AS volume FROM trades WHERE timestamp > dateadd('d', -1, now()) SAMPLE BY 15m;\n\nSELECT timestamp, symbol, first(price) AS open, last(price) AS close, min(price), max(price), sum(amount) AS volume FROM trades WHERE timestamp > dateadd('d', -1, now()) SAMPLE BY 15m;\n\nSELECT timestamp, symbol, first(price) AS open, last(price) AS close, min(price), max(price), sum(amount) AS volume FROM trades WHERE timestamp > dateadd('d', -1, now()) SAMPLE BY 15m;\n\nIntrigued? The best way to see whether QuestDB is right for you is to try it out.\n\nClick Demo this query in the snippet above to visit our demo instance and experiment.\n\nTo bring your own data and learn more, keep reading!"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Where to next? ​", "text": "You'll be inserting data and generating valuable queries in little time.\n\nFirst, the quick start guide will get you running.\n\nChoose from one of our premium ingest-only language clients:\n\nC & C++ High-performance client for systems programming and embedded applications. Read more .NET Cross-platform client for building applications with .NET technologies. Read more Go An open-source programming language supported by Google with built-in concurrency. Read more Java Platform-independent client for enterprise applications and Android development. Read more Node.js Node.js® is an open-source, cross-platform JavaScript runtime environment. Read more Python Python is a programming language that lets you work quickly and integrate systems more effectively. Read more Rust Systems programming language focused on safety, speed, and concurrency. Read more\n\nC & C++ High-performance client for systems programming and embedded applications. Read more .NET Cross-platform client for building applications with .NET technologies. Read more Go An open-source programming language supported by Google with built-in concurrency. Read more Java Platform-independent client for enterprise applications and A"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Where to next? ​", "text": "ons with .NET technologies. Read more Go An open-source programming language supported by Google with built-in concurrency. Read more Java Platform-independent client for enterprise applications and Android development. Read more Node.js Node.js® is an open-source, cross-platform JavaScript runtime environment. Read more Python Python is a programming language that lets you work quickly and integrate systems more effectively. Read more Rust Systems programming language focused on safety, speed, and concurrency. Read more\n\nC & C++ High-performance client for systems programming and embedded applications. Read more\n\nC & C++ High-performance client for systems programming and embedded applications. Read more"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "C & C++", "text": "High-performance client for systems programming and embedded applications.\n\nRead more\n\n.NET Cross-platform client for building applications with .NET technologies. Read more\n\n.NET Cross-platform client for building applications with .NET technologies. Read more"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": ".NET", "text": "Cross-platform client for building applications with .NET technologies.\n\nRead more\n\nGo An open-source programming language supported by Google with built-in concurrency. Read more\n\nGo An open-source programming language supported by Google with built-in concurrency. Read more"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Go", "text": "An open-source programming language supported by Google with built-in concurrency.\n\nRead more\n\nJava Platform-independent client for enterprise applications and Android development. Read more\n\nJava Platform-independent client for enterprise applications and Android development. Read more"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Java", "text": "Platform-independent client for enterprise applications and Android development.\n\nRead more\n\nNode.js Node.js® is an open-source, cross-platform JavaScript runtime environment. Read more\n\nNode.js Node.js® is an open-source, cross-platform JavaScript runtime environment. Read more"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Node.js", "text": "Node.js® is an open-source, cross-platform JavaScript runtime environment.\n\nRead more\n\nPython Python is a programming language that lets you work quickly and integrate systems more effectively. Read more\n\nPython Python is a programming language that lets you work quickly and integrate systems more effectively. Read more"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Python", "text": "Python is a programming language that lets you work quickly and integrate systems more effectively.\n\nRead more\n\nRust Systems programming language focused on safety, speed, and concurrency. Read more\n\nRust Systems programming language focused on safety, speed, and concurrency. Read more"}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Rust", "text": "Systems programming language focused on safety, speed, and concurrency.\n\nRead more\n\nFrom there, you can learn more about what's to offer.\n\nIngestion overview want to see all available ingestion options? Checkout the overview.\n\nQuery & SQL Overview learn how to query QuestDB\n\nWeb Console for quick SQL queries, charting and CSV upload/export functionality\n\nGrafana guide to visualize your data as beautiful and functional charts.\n\nCapacity planning to optimize your QuestDB deployment for production workloads."}
{"source_url": "https://questdb.com/docs/why-questdb", "title": "Why QuestDB?", "section": "Support ​", "text": "We are happy to help with any question you may have.\n\nThe team loves a good performance optimization challenge!\n\nFeel free to reach out using the following channels:\n\nRaise an issue on GitHub\n\nJoin our community forums\n\nQuestDB on Stack Overflow\n\nor email us at hello@questdb.io\n\nEdit this page\n\nEdit this page\n\nPrevious\n\nQuick start\n\nNext\n\nSchema Design\n\nTop QuestDB features Benefits of QuestDB Where to next? Support\n\nTop QuestDB features Benefits of QuestDB Where to next? Support\n\nTop QuestDB features\n\nBenefits of QuestDB\n\nWhere to next?\n\nSupport"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "Schema Design On this page Schema Design This guide covers key concepts and best practices to take full advantage of QuestDB's performance-oriented architecture, highlighting some important differences with most databases. QuestDB's single database model ​ QuestDB has a single database per instance . Unlike PostgreSQL and other database engines, where you may have multiple databases or multiple schemas within an instance, in QuestDB, you operate within a single namespace. The default database is named qdb , and this can be changed via configuration. However, unlike a standard SQL database, there is no need to issue USE DATABASE commands. Once connected, you can immediately start querying and inserting data. Multi-tenancy considerations ​ If you need multi-tenancy , you must manage table names manually, often by using prefixes for different datasets. Since QuestDB does not support multiple schemas, this is the primary way to segment data. In QuestDB Enterprise, you can enforce permissions per table to restrict access , allowing finer control over multi-tenant environments. Here are common patterns for implementing multi-tenancy: Customer-specific tables ​ -- Customer-specific tradin"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "ns per table to restrict access , allowing finer control over multi-tenant environments. Here are common patterns for implementing multi-tenancy: Customer-specific tables ​ -- Customer-specific trading data CREATE TABLE customer1_trades ( timestamp TIMESTAMP, symbol SYMBOL, price DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY; CREATE TABLE customer2_trades ( timestamp TIMESTAMP, symbol SYMBOL, price DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY; Environment or region-based separation ​ -- Production vs. Development environments CREATE TABLE prod_metrics ( timestamp TIMESTAMP, metric_name SYMBOL, value DOUBLE ) TIMESTAMP(timestamp); CREATE TABLE dev_metrics ( timestamp TIMESTAMP, metric_name SYMBOL, value DOUBLE ) TIMESTAMP(timestamp); -- Regional data separation CREATE TABLE eu_users ( timestamp TIMESTAMP, user_id SYMBOL, action SYMBOL ) TIMESTAMP(timestamp); CREATE TABLE us_users ( timestamp TIMESTAMP, user_id SYMBOL, action SYMBOL ) TIMESTAMP(timestamp); Department or team-based separation ​ -- Department-specific analytics CREATE TABLE sales_daily_stats ( timestamp TIMESTAMP, region SYMBOL, revenue DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY; CREATE TABLE marketing_campaign_"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "aration ​ -- Department-specific analytics CREATE TABLE sales_daily_stats ( timestamp TIMESTAMP, region SYMBOL, revenue DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY; CREATE TABLE marketing_campaign_metrics ( timestamp TIMESTAMP, campaign_id SYMBOL, clicks LONG, impressions LONG ) TIMESTAMP(timestamp) PARTITION BY DAY; tip When using table prefixes for multi-tenancy: Use consistent naming conventions (e.g., always <tenant>_<table> ) Consider using uppercase for tenant identifiers to improve readability Document your naming convention in your team's schema design guidelines PostgreSQL protocol compatibility ​ QuestDB is not a PostgreSQL database but is compatible with the PostgreSQL wire protocol . This means you can connect using PostgreSQL-compatible libraries and clients and execute SQL commands. However, compatibility with PostgreSQL system catalogs, metadata queries, data types, and functions is limited. While most PostgreSQL-compatible low-level libraries work with QuestDB, some higher-level components that depend heavily on PostgreSQL metadata might fail. If you encounter such a case, please report it as an issue on GitHub so we can track it. Creating a schema in QuestDB ​ R"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "higher-level components that depend heavily on PostgreSQL metadata might fail. If you encounter such a case, please report it as an issue on GitHub so we can track it. Creating a schema in QuestDB ​ Recommended approach ​ The easiest way to create a schema is through the Web Console or by sending SQL commands using: The REST API ( CREATE TABLE statements) The PostgreSQL wire protocol clients Schema auto-creation with ILP protocol ​ When using the Influx Line Protocol (ILP) , QuestDB automatically creates tables and columns based on incoming data. This is useful for users migrating from InfluxDB or using tools like InfluxDB client libraries or Telegraf , as they can send data directly to QuestDB without pre-defining schemas. However, this comes with limitations: QuestDB applies the default settings to auto-created tables and columns (e.g., partitioning, symbol capacity, and data types). You cannot modify partitioning or symbol capacity later . You cannot auto-create the IPv4 data type. Sending an IP address as a string will create a VARCHAR column. You can disable column auto-creation via configuration . The designated timestamp and partitioning strategy ​ QuestDB is designed for ti"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "Sending an IP address as a string will create a VARCHAR column. You can disable column auto-creation via configuration . The designated timestamp and partitioning strategy ​ QuestDB is designed for time-series workloads. The database engine is optimized to perform exceptionally well for time-series queries. One of the most important optimizations in QuestDB is that data is physically stored and ordered by incremental timestamp. Therefore, the user must choose the designated timestamp when creating a table. The designated timestamp is crucial in QuestDB. It directly affects: How QuestDB partitions data (by hour, day, week, month, or year). Physical data storage order , as data is always stored sorted by the designated timestamp . Query efficiency , since QuestDB prunes partitions based on the timestamp range in your query, reducing disk I/O. Insertion performance , because out-of-order data forces QuestDB to rewrite partitions , slowing down ingestion. Partitioning guidelines ​ When choosing the partition resolution for your tables, consider the time ranges you will query most frequently and keep in mind the following: Avoid very large partitions : A partition should be at most a fe"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "osing the partition resolution for your tables, consider the time ranges you will query most frequently and keep in mind the following: Avoid very large partitions : A partition should be at most a few gigabytes . Avoid too many small partitions : Querying more partitions means opening more files. Query efficiency : When filtering data, QuestDB prunes partitions, but querying many partitions results in more disk operations. If most of your queries span a monthly range, weekly or daily partitioning sounds sensible, but hourly partitioning might slow down your queries. Data ingestion performance : If data arrives out of order, QuestDB rewrites the active partition, impacting performance. Columnar storage model and table density ​ QuestDB is columnar , meaning: Columns are stored separately , allowing fast queries on specific columns without loading unnecessary data. Each column is stored in one or two files per partition : The more columns you include in a SELECT and the more partitions the query spans, the more files will need to be opened and cached into working memory. Sparse vs. dense tables ​ QuestDB handles wide tables efficiently due to its columnar architecture, as it will op"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "the query spans, the more files will need to be opened and cached into working memory. Sparse vs. dense tables ​ QuestDB handles wide tables efficiently due to its columnar architecture, as it will open only the column files referenced in each query. Null values take storage space , so it is recommended to avoid sparse tables where possible. Dense tables (where most columns have values) are more efficient in terms of storage and query performance. If you cannot design a dense table, consider creating different tables for distinct record structures. Data types and best practices ​ Symbols (recommended for categorical data) ​ QuestDB introduces a specialized SYMBOL data type. Symbols are dictionary-encoded and optimized for filtering and grouping: Use symbols for categorical data with a limited number of unique values (e.g., country codes, stock tickers, factory floor IDs). Symbols are fine for storing up to a few million distinct values but should be avoided beyond that. Avoid using a SYMBOL for columns that would be considered a PRIMARY KEY in other databases. If very high cardinality is expected , use VARCHAR instead of SYMBOL . Symbols are compact on disk , reducing storage overh"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "YMBOL for columns that would be considered a PRIMARY KEY in other databases. If very high cardinality is expected , use VARCHAR instead of SYMBOL . Symbols are compact on disk , reducing storage overhead. Symbol capacity defaults to 256 , but it will dynamically expand as needed, causing temporary slowdowns. If you expect high cardinality, define the symbol capacity at table creation time to avoid performance issues. Timestamps ​ All timestamps in QuestDB are stored in UTC at Microsecond resolution : Even if you can ingest data sending timestamps in nanoseconds, nanosecond precision is not retained. The TIMESTAMP type is recommended over DATETIME , unless you have checked the data types reference and you know what you are doing. At query time, you can apply a time zone conversion for display purposes . Strings vs. varchar ​ Avoid STRING : It is a legacy data type. Use VARCHAR instead for general string storage. UUIDs ​ QuestDB has a dedicated UUID type, which is more efficient than storing UUIDs as VARCHAR . Other data types ​ Booleans : true / false values are supported. Bytes : BYTES type allows storing raw binary data. IPv4 : QuestDB has a dedicated IPv4 type for optimized IP st"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "ring UUIDs as VARCHAR . Other data types ​ Booleans : true / false values are supported. Bytes : BYTES type allows storing raw binary data. IPv4 : QuestDB has a dedicated IPv4 type for optimized IP storage and filtering. Several numeric datatypes are supported. Geo : QuestDB provides spatial support via geohashes . Referential integrity, constraints, and deduplication ​ QuestDB does not enforce PRIMARY KEYS , FOREIGN KEYS , or NOT NULL constraints. Joins between tables work even without referential integrity , as long as the data types on the join condition are compatible. Duplicate data is allowed by default , but UPSERT KEYS can be defined to ensure uniqueness . Deduplication in QuestDB happens on an exact timestamp and optionally a set of other columns ( UPSERT KEYS ) . Deduplication has no noticeable performance penalty . Retention strategies with TTL and materialized views ​ Since individual row deletions are not supported , data retention is managed via: Setting a TTL retention period per table to control partition expiration. Materialized views : QuestDB automatically refreshes materialized views , storing aggregated data at lower granularity. You can also apply TTL expirati"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "eriod per table to control partition expiration. Materialized views : QuestDB automatically refreshes materialized views , storing aggregated data at lower granularity. You can also apply TTL expiration on the base table. Schema decisions that cannot be easily changed ​ Some table properties cannot be modified after creation , including: The designated timestamp (cannot be altered once set). Partitioning strategy (cannot be changed later). Symbol capacity (must be defined upfront, otherwise defaults apply). For changes, the typical workaround is: Create a new column with the updated configuration. Copy data from the old column into the new one. Drop the old column and rename the new one. If changes affect table-wide properties (e.g., partitioning, timestamp column, or WAL settings), create a new table with the required properties, insert data from the old table , drop the old table, and rename the new table. Examples of schema translations from other databases ​ -- PostgreSQL CREATE TABLE metrics ( timestamp TIMESTAMP PRIMARY KEY, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE PRECISION NOT NULL ); CREATE INDEX ON metrics ("}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "BLE metrics ( timestamp TIMESTAMP PRIMARY KEY, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE PRECISION NOT NULL ); CREATE INDEX ON metrics (name, timestamp); -- UPSERT behavior in PostgreSQL INSERT INTO metrics (timestamp, name, description, unit, id, value) VALUES (...) ON CONFLICT (timestamp, name) DO UPDATE SET description = EXCLUDED.description, unit = EXCLUDED.unit, value = EXCLUDED.value; -- Timescale CREATE TABLE metrics ( timestamp TIMESTAMPTZ NOT NULL, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE PRECISION NOT NULL ); SELECT create_hypertable('metrics', 'timestamp'); CREATE INDEX ON metrics (name, timestamp); -- UPSERT behavior in Timescale INSERT INTO metrics (timestamp, name, description, unit, id, value) VALUES (...) ON CONFLICT (timestamp, name) DO UPDATE SET description = EXCLUDED.description, unit = EXCLUDED.unit, value = EXCLUDED.value; Create sample table with deduplication/upsert for DuckDB -- DuckDB CREATE TABLE metrics ( timestamp TIMESTAMP NOT NULL, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, v"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "table with deduplication/upsert for DuckDB -- DuckDB CREATE TABLE metrics ( timestamp TIMESTAMP NOT NULL, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE NOT NULL ); CREATE INDEX ON metrics (name, timestamp); -- UPSERT behavior in DuckDB INSERT INTO metrics (timestamp, name, description, unit, id, value) VALUES (?, ?, ?, ?, ?, ?) ON CONFLICT (timestamp, name) DO UPDATE SET description = EXCLUDED.description, unit = EXCLUDED.unit, value = EXCLUDED.value; Create sample table with eventual upserts for ClickHouse -- ClickHouse CREATE TABLE metrics ( timestamp DateTime, name String, description String, unit String, id UUID, value Float64 ) ENGINE = ReplacingMergeTree ORDER BY (name, timestamp); Create sample measure (table) for InfluxDB -- InfluxDB measurement measurement: metrics name (tag) description (tag) unit (tag) id (tag) value (field) Create sample table with deduplication/upsert for QuestDB -- QuestDB equivalent CREATE TABLE metrics ( timestamp TIMESTAMP, -- Explicit timestamp for time-series queries name SYMBOL CAPACITY 50000, -- Optimized for high-cardinality categorical values description VARCHAR, -- Free-text descrip"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "metrics ( timestamp TIMESTAMP, -- Explicit timestamp for time-series queries name SYMBOL CAPACITY 50000, -- Optimized for high-cardinality categorical values description VARCHAR, -- Free-text description, not ideal for SYMBOL indexing unit SYMBOL CAPACITY 256, -- Limited set of unit types, efficient as SYMBOL id UUID, -- UUID optimized for unique identifiers value DOUBLE -- Numeric measurement field ) TIMESTAMP(timestamp) PARTITION BY DAY WAL DEDUP UPSERT KEYS(timestamp, name); Edit this page Previous Why QuestDB? Next Overview QuestDB's single database model Multi-tenancy considerations PostgreSQL protocol compatibility Creating a schema in QuestDB Recommended approach Schema auto-creation with ILP protocol The designated timestamp and partitioning strategy Partitioning guidelines Columnar storage model and table density Sparse vs. dense tables Data types and best practices Symbols (recommended for categorical data) Timestamps Strings vs. varchar UUIDs Other data types Referential integrity, constraints, and deduplication Retention strategies with TTL and materialized views Schema decisions that cannot be easily changed Examples of schema translations from other databases\n\nSchema"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "tegrity, constraints, and deduplication Retention strategies with TTL and materialized views Schema decisions that cannot be easily changed Examples of schema translations from other databases\n\nSchema Design On this page Schema Design This guide covers key concepts and best practices to take full advantage of QuestDB's performance-oriented architecture, highlighting some important differences with most databases. QuestDB's single database model ​ QuestDB has a single database per instance . Unlike PostgreSQL and other database engines, where you may have multiple databases or multiple schemas within an instance, in QuestDB, you operate within a single namespace. The default database is named qdb , and this can be changed via configuration. However, unlike a standard SQL database, there is no need to issue USE DATABASE commands. Once connected, you can immediately start querying and inserting data. Multi-tenancy considerations ​ If you need multi-tenancy , you must manage table names manually, often by using prefixes for different datasets. Since QuestDB does not support multiple schemas, this is the primary way to segment data. In QuestDB Enterprise, you can enforce permissions per"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "nually, often by using prefixes for different datasets. Since QuestDB does not support multiple schemas, this is the primary way to segment data. In QuestDB Enterprise, you can enforce permissions per table to restrict access , allowing finer control over multi-tenant environments. Here are common patterns for implementing multi-tenancy: Customer-specific tables ​ -- Customer-specific trading data CREATE TABLE customer1_trades ( timestamp TIMESTAMP, symbol SYMBOL, price DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY; CREATE TABLE customer2_trades ( timestamp TIMESTAMP, symbol SYMBOL, price DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY; Environment or region-based separation ​ -- Production vs. Development environments CREATE TABLE prod_metrics ( timestamp TIMESTAMP, metric_name SYMBOL, value DOUBLE ) TIMESTAMP(timestamp); CREATE TABLE dev_metrics ( timestamp TIMESTAMP, metric_name SYMBOL, value DOUBLE ) TIMESTAMP(timestamp); -- Regional data separation CREATE TABLE eu_users ( timestamp TIMESTAMP, user_id SYMBOL, action SYMBOL ) TIMESTAMP(timestamp); CREATE TABLE us_users ( timestamp TIMESTAMP, user_id SYMBOL, action SYMBOL ) TIMESTAMP(timestamp); Department or team-based separatio"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "p TIMESTAMP, user_id SYMBOL, action SYMBOL ) TIMESTAMP(timestamp); CREATE TABLE us_users ( timestamp TIMESTAMP, user_id SYMBOL, action SYMBOL ) TIMESTAMP(timestamp); Department or team-based separation ​ -- Department-specific analytics CREATE TABLE sales_daily_stats ( timestamp TIMESTAMP, region SYMBOL, revenue DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY; CREATE TABLE marketing_campaign_metrics ( timestamp TIMESTAMP, campaign_id SYMBOL, clicks LONG, impressions LONG ) TIMESTAMP(timestamp) PARTITION BY DAY; tip When using table prefixes for multi-tenancy: Use consistent naming conventions (e.g., always <tenant>_<table> ) Consider using uppercase for tenant identifiers to improve readability Document your naming convention in your team's schema design guidelines PostgreSQL protocol compatibility ​ QuestDB is not a PostgreSQL database but is compatible with the PostgreSQL wire protocol . This means you can connect using PostgreSQL-compatible libraries and clients and execute SQL commands. However, compatibility with PostgreSQL system catalogs, metadata queries, data types, and functions is limited. While most PostgreSQL-compatible low-level libraries work with QuestDB, some higher"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "nds. However, compatibility with PostgreSQL system catalogs, metadata queries, data types, and functions is limited. While most PostgreSQL-compatible low-level libraries work with QuestDB, some higher-level components that depend heavily on PostgreSQL metadata might fail. If you encounter such a case, please report it as an issue on GitHub so we can track it. Creating a schema in QuestDB ​ Recommended approach ​ The easiest way to create a schema is through the Web Console or by sending SQL commands using: The REST API ( CREATE TABLE statements) The PostgreSQL wire protocol clients Schema auto-creation with ILP protocol ​ When using the Influx Line Protocol (ILP) , QuestDB automatically creates tables and columns based on incoming data. This is useful for users migrating from InfluxDB or using tools like InfluxDB client libraries or Telegraf , as they can send data directly to QuestDB without pre-defining schemas. However, this comes with limitations: QuestDB applies the default settings to auto-created tables and columns (e.g., partitioning, symbol capacity, and data types). You cannot modify partitioning or symbol capacity later . You cannot auto-create the IPv4 data type. Sendin"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "gs to auto-created tables and columns (e.g., partitioning, symbol capacity, and data types). You cannot modify partitioning or symbol capacity later . You cannot auto-create the IPv4 data type. Sending an IP address as a string will create a VARCHAR column. You can disable column auto-creation via configuration . The designated timestamp and partitioning strategy ​ QuestDB is designed for time-series workloads. The database engine is optimized to perform exceptionally well for time-series queries. One of the most important optimizations in QuestDB is that data is physically stored and ordered by incremental timestamp. Therefore, the user must choose the designated timestamp when creating a table. The designated timestamp is crucial in QuestDB. It directly affects: How QuestDB partitions data (by hour, day, week, month, or year). Physical data storage order , as data is always stored sorted by the designated timestamp . Query efficiency , since QuestDB prunes partitions based on the timestamp range in your query, reducing disk I/O. Insertion performance , because out-of-order data forces QuestDB to rewrite partitions , slowing down ingestion. Partitioning guidelines ​ When choosing"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "estamp range in your query, reducing disk I/O. Insertion performance , because out-of-order data forces QuestDB to rewrite partitions , slowing down ingestion. Partitioning guidelines ​ When choosing the partition resolution for your tables, consider the time ranges you will query most frequently and keep in mind the following: Avoid very large partitions : A partition should be at most a few gigabytes . Avoid too many small partitions : Querying more partitions means opening more files. Query efficiency : When filtering data, QuestDB prunes partitions, but querying many partitions results in more disk operations. If most of your queries span a monthly range, weekly or daily partitioning sounds sensible, but hourly partitioning might slow down your queries. Data ingestion performance : If data arrives out of order, QuestDB rewrites the active partition, impacting performance. Columnar storage model and table density ​ QuestDB is columnar , meaning: Columns are stored separately , allowing fast queries on specific columns without loading unnecessary data. Each column is stored in one or two files per partition : The more columns you include in a SELECT and the more partitions the qu"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "ing fast queries on specific columns without loading unnecessary data. Each column is stored in one or two files per partition : The more columns you include in a SELECT and the more partitions the query spans, the more files will need to be opened and cached into working memory. Sparse vs. dense tables ​ QuestDB handles wide tables efficiently due to its columnar architecture, as it will open only the column files referenced in each query. Null values take storage space , so it is recommended to avoid sparse tables where possible. Dense tables (where most columns have values) are more efficient in terms of storage and query performance. If you cannot design a dense table, consider creating different tables for distinct record structures. Data types and best practices ​ Symbols (recommended for categorical data) ​ QuestDB introduces a specialized SYMBOL data type. Symbols are dictionary-encoded and optimized for filtering and grouping: Use symbols for categorical data with a limited number of unique values (e.g., country codes, stock tickers, factory floor IDs). Symbols are fine for storing up to a few million distinct values but should be avoided beyond that. Avoid using a SYMBOL"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "mber of unique values (e.g., country codes, stock tickers, factory floor IDs). Symbols are fine for storing up to a few million distinct values but should be avoided beyond that. Avoid using a SYMBOL for columns that would be considered a PRIMARY KEY in other databases. If very high cardinality is expected , use VARCHAR instead of SYMBOL . Symbols are compact on disk , reducing storage overhead. Symbol capacity defaults to 256 , but it will dynamically expand as needed, causing temporary slowdowns. If you expect high cardinality, define the symbol capacity at table creation time to avoid performance issues. Timestamps ​ All timestamps in QuestDB are stored in UTC at Microsecond resolution : Even if you can ingest data sending timestamps in nanoseconds, nanosecond precision is not retained. The TIMESTAMP type is recommended over DATETIME , unless you have checked the data types reference and you know what you are doing. At query time, you can apply a time zone conversion for display purposes . Strings vs. varchar ​ Avoid STRING : It is a legacy data type. Use VARCHAR instead for general string storage. UUIDs ​ QuestDB has a dedicated UUID type, which is more efficient than storing U"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "poses . Strings vs. varchar ​ Avoid STRING : It is a legacy data type. Use VARCHAR instead for general string storage. UUIDs ​ QuestDB has a dedicated UUID type, which is more efficient than storing UUIDs as VARCHAR . Other data types ​ Booleans : true / false values are supported. Bytes : BYTES type allows storing raw binary data. IPv4 : QuestDB has a dedicated IPv4 type for optimized IP storage and filtering. Several numeric datatypes are supported. Geo : QuestDB provides spatial support via geohashes . Referential integrity, constraints, and deduplication ​ QuestDB does not enforce PRIMARY KEYS , FOREIGN KEYS , or NOT NULL constraints. Joins between tables work even without referential integrity , as long as the data types on the join condition are compatible. Duplicate data is allowed by default , but UPSERT KEYS can be defined to ensure uniqueness . Deduplication in QuestDB happens on an exact timestamp and optionally a set of other columns ( UPSERT KEYS ) . Deduplication has no noticeable performance penalty . Retention strategies with TTL and materialized views ​ Since individual row deletions are not supported , data retention is managed via: Setting a TTL retention period"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "oticeable performance penalty . Retention strategies with TTL and materialized views ​ Since individual row deletions are not supported , data retention is managed via: Setting a TTL retention period per table to control partition expiration. Materialized views : QuestDB automatically refreshes materialized views , storing aggregated data at lower granularity. You can also apply TTL expiration on the base table. Schema decisions that cannot be easily changed ​ Some table properties cannot be modified after creation , including: The designated timestamp (cannot be altered once set). Partitioning strategy (cannot be changed later). Symbol capacity (must be defined upfront, otherwise defaults apply). For changes, the typical workaround is: Create a new column with the updated configuration. Copy data from the old column into the new one. Drop the old column and rename the new one. If changes affect table-wide properties (e.g., partitioning, timestamp column, or WAL settings), create a new table with the required properties, insert data from the old table , drop the old table, and rename the new table. Examples of schema translations from other databases ​ -- PostgreSQL CREATE TABLE me"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "table with the required properties, insert data from the old table , drop the old table, and rename the new table. Examples of schema translations from other databases ​ -- PostgreSQL CREATE TABLE metrics ( timestamp TIMESTAMP PRIMARY KEY, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE PRECISION NOT NULL ); CREATE INDEX ON metrics (name, timestamp); -- UPSERT behavior in PostgreSQL INSERT INTO metrics (timestamp, name, description, unit, id, value) VALUES (...) ON CONFLICT (timestamp, name) DO UPDATE SET description = EXCLUDED.description, unit = EXCLUDED.unit, value = EXCLUDED.value; -- Timescale CREATE TABLE metrics ( timestamp TIMESTAMPTZ NOT NULL, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE PRECISION NOT NULL ); SELECT create_hypertable('metrics', 'timestamp'); CREATE INDEX ON metrics (name, timestamp); -- UPSERT behavior in Timescale INSERT INTO metrics (timestamp, name, description, unit, id, value) VALUES (...) ON CONFLICT (timestamp, name) DO UPDATE SET description = EXCLUDED.description, unit = EXCLUDED.unit, value = EXCLUDED.value; Create sample table"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "amp, name, description, unit, id, value) VALUES (...) ON CONFLICT (timestamp, name) DO UPDATE SET description = EXCLUDED.description, unit = EXCLUDED.unit, value = EXCLUDED.value; Create sample table with deduplication/upsert for DuckDB -- DuckDB CREATE TABLE metrics ( timestamp TIMESTAMP NOT NULL, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE NOT NULL ); CREATE INDEX ON metrics (name, timestamp); -- UPSERT behavior in DuckDB INSERT INTO metrics (timestamp, name, description, unit, id, value) VALUES (?, ?, ?, ?, ?, ?) ON CONFLICT (timestamp, name) DO UPDATE SET description = EXCLUDED.description, unit = EXCLUDED.unit, value = EXCLUDED.value; Create sample table with eventual upserts for ClickHouse -- ClickHouse CREATE TABLE metrics ( timestamp DateTime, name String, description String, unit String, id UUID, value Float64 ) ENGINE = ReplacingMergeTree ORDER BY (name, timestamp); Create sample measure (table) for InfluxDB -- InfluxDB measurement measurement: metrics name (tag) description (tag) unit (tag) id (tag) value (field) Create sample table with deduplication/upsert for QuestDB -- QuestDB equivalent CREATE TABLE metri"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "luxDB measurement measurement: metrics name (tag) description (tag) unit (tag) id (tag) value (field) Create sample table with deduplication/upsert for QuestDB -- QuestDB equivalent CREATE TABLE metrics ( timestamp TIMESTAMP, -- Explicit timestamp for time-series queries name SYMBOL CAPACITY 50000, -- Optimized for high-cardinality categorical values description VARCHAR, -- Free-text description, not ideal for SYMBOL indexing unit SYMBOL CAPACITY 256, -- Limited set of unit types, efficient as SYMBOL id UUID, -- UUID optimized for unique identifiers value DOUBLE -- Numeric measurement field ) TIMESTAMP(timestamp) PARTITION BY DAY WAL DEDUP UPSERT KEYS(timestamp, name); Edit this page Previous Why QuestDB? Next Overview QuestDB's single database model Multi-tenancy considerations PostgreSQL protocol compatibility Creating a schema in QuestDB Recommended approach Schema auto-creation with ILP protocol The designated timestamp and partitioning strategy Partitioning guidelines Columnar storage model and table density Sparse vs. dense tables Data types and best practices Symbols (recommended for categorical data) Timestamps Strings vs. varchar UUIDs Other data types Referential integrit"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "rage model and table density Sparse vs. dense tables Data types and best practices Symbols (recommended for categorical data) Timestamps Strings vs. varchar UUIDs Other data types Referential integrity, constraints, and deduplication Retention strategies with TTL and materialized views Schema decisions that cannot be easily changed Examples of schema translations from other databases\n\nSchema Design On this page Schema Design This guide covers key concepts and best practices to take full advantage of QuestDB's performance-oriented architecture, highlighting some important differences with most databases. QuestDB's single database model ​ QuestDB has a single database per instance . Unlike PostgreSQL and other database engines, where you may have multiple databases or multiple schemas within an instance, in QuestDB, you operate within a single namespace. The default database is named qdb , and this can be changed via configuration. However, unlike a standard SQL database, there is no need to issue USE DATABASE commands. Once connected, you can immediately start querying and inserting data. Multi-tenancy considerations ​ If you need multi-tenancy , you must manage table names manually"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "d to issue USE DATABASE commands. Once connected, you can immediately start querying and inserting data. Multi-tenancy considerations ​ If you need multi-tenancy , you must manage table names manually, often by using prefixes for different datasets. Since QuestDB does not support multiple schemas, this is the primary way to segment data. In QuestDB Enterprise, you can enforce permissions per table to restrict access , allowing finer control over multi-tenant environments. Here are common patterns for implementing multi-tenancy: Customer-specific tables ​ -- Customer-specific trading data CREATE TABLE customer1_trades ( timestamp TIMESTAMP, symbol SYMBOL, price DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY; CREATE TABLE customer2_trades ( timestamp TIMESTAMP, symbol SYMBOL, price DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY; Environment or region-based separation ​ -- Production vs. Development environments CREATE TABLE prod_metrics ( timestamp TIMESTAMP, metric_name SYMBOL, value DOUBLE ) TIMESTAMP(timestamp); CREATE TABLE dev_metrics ( timestamp TIMESTAMP, metric_name SYMBOL, value DOUBLE ) TIMESTAMP(timestamp); -- Regional data separation CREATE TABLE eu_users ( timestamp TIME"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "UBLE ) TIMESTAMP(timestamp); CREATE TABLE dev_metrics ( timestamp TIMESTAMP, metric_name SYMBOL, value DOUBLE ) TIMESTAMP(timestamp); -- Regional data separation CREATE TABLE eu_users ( timestamp TIMESTAMP, user_id SYMBOL, action SYMBOL ) TIMESTAMP(timestamp); CREATE TABLE us_users ( timestamp TIMESTAMP, user_id SYMBOL, action SYMBOL ) TIMESTAMP(timestamp); Department or team-based separation ​ -- Department-specific analytics CREATE TABLE sales_daily_stats ( timestamp TIMESTAMP, region SYMBOL, revenue DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY; CREATE TABLE marketing_campaign_metrics ( timestamp TIMESTAMP, campaign_id SYMBOL, clicks LONG, impressions LONG ) TIMESTAMP(timestamp) PARTITION BY DAY; tip When using table prefixes for multi-tenancy: Use consistent naming conventions (e.g., always <tenant>_<table> ) Consider using uppercase for tenant identifiers to improve readability Document your naming convention in your team's schema design guidelines PostgreSQL protocol compatibility ​ QuestDB is not a PostgreSQL database but is compatible with the PostgreSQL wire protocol . This means you can connect using PostgreSQL-compatible libraries and clients and execute SQL commands. H"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "ty ​ QuestDB is not a PostgreSQL database but is compatible with the PostgreSQL wire protocol . This means you can connect using PostgreSQL-compatible libraries and clients and execute SQL commands. However, compatibility with PostgreSQL system catalogs, metadata queries, data types, and functions is limited. While most PostgreSQL-compatible low-level libraries work with QuestDB, some higher-level components that depend heavily on PostgreSQL metadata might fail. If you encounter such a case, please report it as an issue on GitHub so we can track it. Creating a schema in QuestDB ​ Recommended approach ​ The easiest way to create a schema is through the Web Console or by sending SQL commands using: The REST API ( CREATE TABLE statements) The PostgreSQL wire protocol clients Schema auto-creation with ILP protocol ​ When using the Influx Line Protocol (ILP) , QuestDB automatically creates tables and columns based on incoming data. This is useful for users migrating from InfluxDB or using tools like InfluxDB client libraries or Telegraf , as they can send data directly to QuestDB without pre-defining schemas. However, this comes with limitations: QuestDB applies the default settings to"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "ools like InfluxDB client libraries or Telegraf , as they can send data directly to QuestDB without pre-defining schemas. However, this comes with limitations: QuestDB applies the default settings to auto-created tables and columns (e.g., partitioning, symbol capacity, and data types). You cannot modify partitioning or symbol capacity later . You cannot auto-create the IPv4 data type. Sending an IP address as a string will create a VARCHAR column. You can disable column auto-creation via configuration . The designated timestamp and partitioning strategy ​ QuestDB is designed for time-series workloads. The database engine is optimized to perform exceptionally well for time-series queries. One of the most important optimizations in QuestDB is that data is physically stored and ordered by incremental timestamp. Therefore, the user must choose the designated timestamp when creating a table. The designated timestamp is crucial in QuestDB. It directly affects: How QuestDB partitions data (by hour, day, week, month, or year). Physical data storage order , as data is always stored sorted by the designated timestamp . Query efficiency , since QuestDB prunes partitions based on the timestamp"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "hour, day, week, month, or year). Physical data storage order , as data is always stored sorted by the designated timestamp . Query efficiency , since QuestDB prunes partitions based on the timestamp range in your query, reducing disk I/O. Insertion performance , because out-of-order data forces QuestDB to rewrite partitions , slowing down ingestion. Partitioning guidelines ​ When choosing the partition resolution for your tables, consider the time ranges you will query most frequently and keep in mind the following: Avoid very large partitions : A partition should be at most a few gigabytes . Avoid too many small partitions : Querying more partitions means opening more files. Query efficiency : When filtering data, QuestDB prunes partitions, but querying many partitions results in more disk operations. If most of your queries span a monthly range, weekly or daily partitioning sounds sensible, but hourly partitioning might slow down your queries. Data ingestion performance : If data arrives out of order, QuestDB rewrites the active partition, impacting performance. Columnar storage model and table density ​ QuestDB is columnar , meaning: Columns are stored separately , allowing fa"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "rrives out of order, QuestDB rewrites the active partition, impacting performance. Columnar storage model and table density ​ QuestDB is columnar , meaning: Columns are stored separately , allowing fast queries on specific columns without loading unnecessary data. Each column is stored in one or two files per partition : The more columns you include in a SELECT and the more partitions the query spans, the more files will need to be opened and cached into working memory. Sparse vs. dense tables ​ QuestDB handles wide tables efficiently due to its columnar architecture, as it will open only the column files referenced in each query. Null values take storage space , so it is recommended to avoid sparse tables where possible. Dense tables (where most columns have values) are more efficient in terms of storage and query performance. If you cannot design a dense table, consider creating different tables for distinct record structures. Data types and best practices ​ Symbols (recommended for categorical data) ​ QuestDB introduces a specialized SYMBOL data type. Symbols are dictionary-encoded and optimized for filtering and grouping: Use symbols for categorical data with a limited number o"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "categorical data) ​ QuestDB introduces a specialized SYMBOL data type. Symbols are dictionary-encoded and optimized for filtering and grouping: Use symbols for categorical data with a limited number of unique values (e.g., country codes, stock tickers, factory floor IDs). Symbols are fine for storing up to a few million distinct values but should be avoided beyond that. Avoid using a SYMBOL for columns that would be considered a PRIMARY KEY in other databases. If very high cardinality is expected , use VARCHAR instead of SYMBOL . Symbols are compact on disk , reducing storage overhead. Symbol capacity defaults to 256 , but it will dynamically expand as needed, causing temporary slowdowns. If you expect high cardinality, define the symbol capacity at table creation time to avoid performance issues. Timestamps ​ All timestamps in QuestDB are stored in UTC at Microsecond resolution : Even if you can ingest data sending timestamps in nanoseconds, nanosecond precision is not retained. The TIMESTAMP type is recommended over DATETIME , unless you have checked the data types reference and you know what you are doing. At query time, you can apply a time zone conversion for display purposes"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "IMESTAMP type is recommended over DATETIME , unless you have checked the data types reference and you know what you are doing. At query time, you can apply a time zone conversion for display purposes . Strings vs. varchar ​ Avoid STRING : It is a legacy data type. Use VARCHAR instead for general string storage. UUIDs ​ QuestDB has a dedicated UUID type, which is more efficient than storing UUIDs as VARCHAR . Other data types ​ Booleans : true / false values are supported. Bytes : BYTES type allows storing raw binary data. IPv4 : QuestDB has a dedicated IPv4 type for optimized IP storage and filtering. Several numeric datatypes are supported. Geo : QuestDB provides spatial support via geohashes . Referential integrity, constraints, and deduplication ​ QuestDB does not enforce PRIMARY KEYS , FOREIGN KEYS , or NOT NULL constraints. Joins between tables work even without referential integrity , as long as the data types on the join condition are compatible. Duplicate data is allowed by default , but UPSERT KEYS can be defined to ensure uniqueness . Deduplication in QuestDB happens on an exact timestamp and optionally a set of other columns ( UPSERT KEYS ) . Deduplication has no noticea"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "ault , but UPSERT KEYS can be defined to ensure uniqueness . Deduplication in QuestDB happens on an exact timestamp and optionally a set of other columns ( UPSERT KEYS ) . Deduplication has no noticeable performance penalty . Retention strategies with TTL and materialized views ​ Since individual row deletions are not supported , data retention is managed via: Setting a TTL retention period per table to control partition expiration. Materialized views : QuestDB automatically refreshes materialized views , storing aggregated data at lower granularity. You can also apply TTL expiration on the base table. Schema decisions that cannot be easily changed ​ Some table properties cannot be modified after creation , including: The designated timestamp (cannot be altered once set). Partitioning strategy (cannot be changed later). Symbol capacity (must be defined upfront, otherwise defaults apply). For changes, the typical workaround is: Create a new column with the updated configuration. Copy data from the old column into the new one. Drop the old column and rename the new one. If changes affect table-wide properties (e.g., partitioning, timestamp column, or WAL settings), create a new table"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "ta from the old column into the new one. Drop the old column and rename the new one. If changes affect table-wide properties (e.g., partitioning, timestamp column, or WAL settings), create a new table with the required properties, insert data from the old table , drop the old table, and rename the new table. Examples of schema translations from other databases ​ -- PostgreSQL CREATE TABLE metrics ( timestamp TIMESTAMP PRIMARY KEY, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE PRECISION NOT NULL ); CREATE INDEX ON metrics (name, timestamp); -- UPSERT behavior in PostgreSQL INSERT INTO metrics (timestamp, name, description, unit, id, value) VALUES (...) ON CONFLICT (timestamp, name) DO UPDATE SET description = EXCLUDED.description, unit = EXCLUDED.unit, value = EXCLUDED.value; -- Timescale CREATE TABLE metrics ( timestamp TIMESTAMPTZ NOT NULL, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE PRECISION NOT NULL ); SELECT create_hypertable('metrics', 'timestamp'); CREATE INDEX ON metrics (name, timestamp); -- UPSERT behavior in Timescale INSERT INTO metrics (timestamp, n"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "Y KEY, value DOUBLE PRECISION NOT NULL ); SELECT create_hypertable('metrics', 'timestamp'); CREATE INDEX ON metrics (name, timestamp); -- UPSERT behavior in Timescale INSERT INTO metrics (timestamp, name, description, unit, id, value) VALUES (...) ON CONFLICT (timestamp, name) DO UPDATE SET description = EXCLUDED.description, unit = EXCLUDED.unit, value = EXCLUDED.value; Create sample table with deduplication/upsert for DuckDB -- DuckDB CREATE TABLE metrics ( timestamp TIMESTAMP NOT NULL, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE NOT NULL ); CREATE INDEX ON metrics (name, timestamp); -- UPSERT behavior in DuckDB INSERT INTO metrics (timestamp, name, description, unit, id, value) VALUES (?, ?, ?, ?, ?, ?) ON CONFLICT (timestamp, name) DO UPDATE SET description = EXCLUDED.description, unit = EXCLUDED.unit, value = EXCLUDED.value; Create sample table with eventual upserts for ClickHouse -- ClickHouse CREATE TABLE metrics ( timestamp DateTime, name String, description String, unit String, id UUID, value Float64 ) ENGINE = ReplacingMergeTree ORDER BY (name, timestamp); Create sample measure (table) for InfluxDB -- InfluxDB"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "mestamp DateTime, name String, description String, unit String, id UUID, value Float64 ) ENGINE = ReplacingMergeTree ORDER BY (name, timestamp); Create sample measure (table) for InfluxDB -- InfluxDB measurement measurement: metrics name (tag) description (tag) unit (tag) id (tag) value (field) Create sample table with deduplication/upsert for QuestDB -- QuestDB equivalent CREATE TABLE metrics ( timestamp TIMESTAMP, -- Explicit timestamp for time-series queries name SYMBOL CAPACITY 50000, -- Optimized for high-cardinality categorical values description VARCHAR, -- Free-text description, not ideal for SYMBOL indexing unit SYMBOL CAPACITY 256, -- Limited set of unit types, efficient as SYMBOL id UUID, -- UUID optimized for unique identifiers value DOUBLE -- Numeric measurement field ) TIMESTAMP(timestamp) PARTITION BY DAY WAL DEDUP UPSERT KEYS(timestamp, name); Edit this page Previous Why QuestDB? Next Overview\n\nSchema Design On this page Schema Design This guide covers key concepts and best practices to take full advantage of QuestDB's performance-oriented architecture, highlighting some important differences with most databases. QuestDB's single database model ​ QuestDB has a singl"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "best practices to take full advantage of QuestDB's performance-oriented architecture, highlighting some important differences with most databases. QuestDB's single database model ​ QuestDB has a single database per instance . Unlike PostgreSQL and other database engines, where you may have multiple databases or multiple schemas within an instance, in QuestDB, you operate within a single namespace. The default database is named qdb , and this can be changed via configuration. However, unlike a standard SQL database, there is no need to issue USE DATABASE commands. Once connected, you can immediately start querying and inserting data. Multi-tenancy considerations ​ If you need multi-tenancy , you must manage table names manually, often by using prefixes for different datasets. Since QuestDB does not support multiple schemas, this is the primary way to segment data. In QuestDB Enterprise, you can enforce permissions per table to restrict access , allowing finer control over multi-tenant environments. Here are common patterns for implementing multi-tenancy: Customer-specific tables ​ -- Customer-specific trading data CREATE TABLE customer1_trades ( timestamp TIMESTAMP, symbol SYMBOL, p"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "nvironments. Here are common patterns for implementing multi-tenancy: Customer-specific tables ​ -- Customer-specific trading data CREATE TABLE customer1_trades ( timestamp TIMESTAMP, symbol SYMBOL, price DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY; CREATE TABLE customer2_trades ( timestamp TIMESTAMP, symbol SYMBOL, price DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY; Environment or region-based separation ​ -- Production vs. Development environments CREATE TABLE prod_metrics ( timestamp TIMESTAMP, metric_name SYMBOL, value DOUBLE ) TIMESTAMP(timestamp); CREATE TABLE dev_metrics ( timestamp TIMESTAMP, metric_name SYMBOL, value DOUBLE ) TIMESTAMP(timestamp); -- Regional data separation CREATE TABLE eu_users ( timestamp TIMESTAMP, user_id SYMBOL, action SYMBOL ) TIMESTAMP(timestamp); CREATE TABLE us_users ( timestamp TIMESTAMP, user_id SYMBOL, action SYMBOL ) TIMESTAMP(timestamp); Department or team-based separation ​ -- Department-specific analytics CREATE TABLE sales_daily_stats ( timestamp TIMESTAMP, region SYMBOL, revenue DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY; CREATE TABLE marketing_campaign_metrics ( timestamp TIMESTAMP, campaign_id SYMBOL, clicks LONG, impressions"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "timestamp TIMESTAMP, region SYMBOL, revenue DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY; CREATE TABLE marketing_campaign_metrics ( timestamp TIMESTAMP, campaign_id SYMBOL, clicks LONG, impressions LONG ) TIMESTAMP(timestamp) PARTITION BY DAY; tip When using table prefixes for multi-tenancy: Use consistent naming conventions (e.g., always <tenant>_<table> ) Consider using uppercase for tenant identifiers to improve readability Document your naming convention in your team's schema design guidelines PostgreSQL protocol compatibility ​ QuestDB is not a PostgreSQL database but is compatible with the PostgreSQL wire protocol . This means you can connect using PostgreSQL-compatible libraries and clients and execute SQL commands. However, compatibility with PostgreSQL system catalogs, metadata queries, data types, and functions is limited. While most PostgreSQL-compatible low-level libraries work with QuestDB, some higher-level components that depend heavily on PostgreSQL metadata might fail. If you encounter such a case, please report it as an issue on GitHub so we can track it. Creating a schema in QuestDB ​ Recommended approach ​ The easiest way to create a schema is through the Web"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "l. If you encounter such a case, please report it as an issue on GitHub so we can track it. Creating a schema in QuestDB ​ Recommended approach ​ The easiest way to create a schema is through the Web Console or by sending SQL commands using: The REST API ( CREATE TABLE statements) The PostgreSQL wire protocol clients Schema auto-creation with ILP protocol ​ When using the Influx Line Protocol (ILP) , QuestDB automatically creates tables and columns based on incoming data. This is useful for users migrating from InfluxDB or using tools like InfluxDB client libraries or Telegraf , as they can send data directly to QuestDB without pre-defining schemas. However, this comes with limitations: QuestDB applies the default settings to auto-created tables and columns (e.g., partitioning, symbol capacity, and data types). You cannot modify partitioning or symbol capacity later . You cannot auto-create the IPv4 data type. Sending an IP address as a string will create a VARCHAR column. You can disable column auto-creation via configuration . The designated timestamp and partitioning strategy ​ QuestDB is designed for time-series workloads. The database engine is optimized to perform exceptional"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "ble column auto-creation via configuration . The designated timestamp and partitioning strategy ​ QuestDB is designed for time-series workloads. The database engine is optimized to perform exceptionally well for time-series queries. One of the most important optimizations in QuestDB is that data is physically stored and ordered by incremental timestamp. Therefore, the user must choose the designated timestamp when creating a table. The designated timestamp is crucial in QuestDB. It directly affects: How QuestDB partitions data (by hour, day, week, month, or year). Physical data storage order , as data is always stored sorted by the designated timestamp . Query efficiency , since QuestDB prunes partitions based on the timestamp range in your query, reducing disk I/O. Insertion performance , because out-of-order data forces QuestDB to rewrite partitions , slowing down ingestion. Partitioning guidelines ​ When choosing the partition resolution for your tables, consider the time ranges you will query most frequently and keep in mind the following: Avoid very large partitions : A partition should be at most a few gigabytes . Avoid too many small partitions : Querying more partitions mea"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "will query most frequently and keep in mind the following: Avoid very large partitions : A partition should be at most a few gigabytes . Avoid too many small partitions : Querying more partitions means opening more files. Query efficiency : When filtering data, QuestDB prunes partitions, but querying many partitions results in more disk operations. If most of your queries span a monthly range, weekly or daily partitioning sounds sensible, but hourly partitioning might slow down your queries. Data ingestion performance : If data arrives out of order, QuestDB rewrites the active partition, impacting performance. Columnar storage model and table density ​ QuestDB is columnar , meaning: Columns are stored separately , allowing fast queries on specific columns without loading unnecessary data. Each column is stored in one or two files per partition : The more columns you include in a SELECT and the more partitions the query spans, the more files will need to be opened and cached into working memory. Sparse vs. dense tables ​ QuestDB handles wide tables efficiently due to its columnar architecture, as it will open only the column files referenced in each query. Null values take storage"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "ng memory. Sparse vs. dense tables ​ QuestDB handles wide tables efficiently due to its columnar architecture, as it will open only the column files referenced in each query. Null values take storage space , so it is recommended to avoid sparse tables where possible. Dense tables (where most columns have values) are more efficient in terms of storage and query performance. If you cannot design a dense table, consider creating different tables for distinct record structures. Data types and best practices ​ Symbols (recommended for categorical data) ​ QuestDB introduces a specialized SYMBOL data type. Symbols are dictionary-encoded and optimized for filtering and grouping: Use symbols for categorical data with a limited number of unique values (e.g., country codes, stock tickers, factory floor IDs). Symbols are fine for storing up to a few million distinct values but should be avoided beyond that. Avoid using a SYMBOL for columns that would be considered a PRIMARY KEY in other databases. If very high cardinality is expected , use VARCHAR instead of SYMBOL . Symbols are compact on disk , reducing storage overhead. Symbol capacity defaults to 256 , but it will dynamically expand as nee"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "If very high cardinality is expected , use VARCHAR instead of SYMBOL . Symbols are compact on disk , reducing storage overhead. Symbol capacity defaults to 256 , but it will dynamically expand as needed, causing temporary slowdowns. If you expect high cardinality, define the symbol capacity at table creation time to avoid performance issues. Timestamps ​ All timestamps in QuestDB are stored in UTC at Microsecond resolution : Even if you can ingest data sending timestamps in nanoseconds, nanosecond precision is not retained. The TIMESTAMP type is recommended over DATETIME , unless you have checked the data types reference and you know what you are doing. At query time, you can apply a time zone conversion for display purposes . Strings vs. varchar ​ Avoid STRING : It is a legacy data type. Use VARCHAR instead for general string storage. UUIDs ​ QuestDB has a dedicated UUID type, which is more efficient than storing UUIDs as VARCHAR . Other data types ​ Booleans : true / false values are supported. Bytes : BYTES type allows storing raw binary data. IPv4 : QuestDB has a dedicated IPv4 type for optimized IP storage and filtering. Several numeric datatypes are supported. Geo : QuestDB"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "e supported. Bytes : BYTES type allows storing raw binary data. IPv4 : QuestDB has a dedicated IPv4 type for optimized IP storage and filtering. Several numeric datatypes are supported. Geo : QuestDB provides spatial support via geohashes . Referential integrity, constraints, and deduplication ​ QuestDB does not enforce PRIMARY KEYS , FOREIGN KEYS , or NOT NULL constraints. Joins between tables work even without referential integrity , as long as the data types on the join condition are compatible. Duplicate data is allowed by default , but UPSERT KEYS can be defined to ensure uniqueness . Deduplication in QuestDB happens on an exact timestamp and optionally a set of other columns ( UPSERT KEYS ) . Deduplication has no noticeable performance penalty . Retention strategies with TTL and materialized views ​ Since individual row deletions are not supported , data retention is managed via: Setting a TTL retention period per table to control partition expiration. Materialized views : QuestDB automatically refreshes materialized views , storing aggregated data at lower granularity. You can also apply TTL expiration on the base table. Schema decisions that cannot be easily changed ​ Some"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "B automatically refreshes materialized views , storing aggregated data at lower granularity. You can also apply TTL expiration on the base table. Schema decisions that cannot be easily changed ​ Some table properties cannot be modified after creation , including: The designated timestamp (cannot be altered once set). Partitioning strategy (cannot be changed later). Symbol capacity (must be defined upfront, otherwise defaults apply). For changes, the typical workaround is: Create a new column with the updated configuration. Copy data from the old column into the new one. Drop the old column and rename the new one. If changes affect table-wide properties (e.g., partitioning, timestamp column, or WAL settings), create a new table with the required properties, insert data from the old table , drop the old table, and rename the new table. Examples of schema translations from other databases ​ -- PostgreSQL CREATE TABLE metrics ( timestamp TIMESTAMP PRIMARY KEY, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE PRECISION NOT NULL ); CREATE INDEX ON metrics (name, timestamp); -- UPSERT behavior in PostgreSQL INSERT INTO metrics (time"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "escription VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE PRECISION NOT NULL ); CREATE INDEX ON metrics (name, timestamp); -- UPSERT behavior in PostgreSQL INSERT INTO metrics (timestamp, name, description, unit, id, value) VALUES (...) ON CONFLICT (timestamp, name) DO UPDATE SET description = EXCLUDED.description, unit = EXCLUDED.unit, value = EXCLUDED.value; -- Timescale CREATE TABLE metrics ( timestamp TIMESTAMPTZ NOT NULL, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE PRECISION NOT NULL ); SELECT create_hypertable('metrics', 'timestamp'); CREATE INDEX ON metrics (name, timestamp); -- UPSERT behavior in Timescale INSERT INTO metrics (timestamp, name, description, unit, id, value) VALUES (...) ON CONFLICT (timestamp, name) DO UPDATE SET description = EXCLUDED.description, unit = EXCLUDED.unit, value = EXCLUDED.value; Create sample table with deduplication/upsert for DuckDB -- DuckDB CREATE TABLE metrics ( timestamp TIMESTAMP NOT NULL, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE NOT NULL ); CREATE INDEX ON metrics (name, timestamp); -- UPSERT"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "timestamp TIMESTAMP NOT NULL, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE NOT NULL ); CREATE INDEX ON metrics (name, timestamp); -- UPSERT behavior in DuckDB INSERT INTO metrics (timestamp, name, description, unit, id, value) VALUES (?, ?, ?, ?, ?, ?) ON CONFLICT (timestamp, name) DO UPDATE SET description = EXCLUDED.description, unit = EXCLUDED.unit, value = EXCLUDED.value; Create sample table with eventual upserts for ClickHouse -- ClickHouse CREATE TABLE metrics ( timestamp DateTime, name String, description String, unit String, id UUID, value Float64 ) ENGINE = ReplacingMergeTree ORDER BY (name, timestamp); Create sample measure (table) for InfluxDB -- InfluxDB measurement measurement: metrics name (tag) description (tag) unit (tag) id (tag) value (field) Create sample table with deduplication/upsert for QuestDB -- QuestDB equivalent CREATE TABLE metrics ( timestamp TIMESTAMP, -- Explicit timestamp for time-series queries name SYMBOL CAPACITY 50000, -- Optimized for high-cardinality categorical values description VARCHAR, -- Free-text description, not ideal for SYMBOL indexing unit SYMBOL CAPACITY 256, -- Limited set"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "s name SYMBOL CAPACITY 50000, -- Optimized for high-cardinality categorical values description VARCHAR, -- Free-text description, not ideal for SYMBOL indexing unit SYMBOL CAPACITY 256, -- Limited set of unit types, efficient as SYMBOL id UUID, -- UUID optimized for unique identifiers value DOUBLE -- Numeric measurement field ) TIMESTAMP(timestamp) PARTITION BY DAY WAL DEDUP UPSERT KEYS(timestamp, name); Edit this page Previous Why QuestDB? Next Overview\n\nSchema Design\n\nOn this page\n\nSchema Design This guide covers key concepts and best practices to take full advantage of QuestDB's performance-oriented architecture, highlighting some important differences with most databases. QuestDB's single database model ​ QuestDB has a single database per instance . Unlike PostgreSQL and other database engines, where you may have multiple databases or multiple schemas within an instance, in QuestDB, you operate within a single namespace. The default database is named qdb , and this can be changed via configuration. However, unlike a standard SQL database, there is no need to issue USE DATABASE commands. Once connected, you can immediately start querying and inserting data. Multi-tenancy conside"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "ia configuration. However, unlike a standard SQL database, there is no need to issue USE DATABASE commands. Once connected, you can immediately start querying and inserting data. Multi-tenancy considerations ​ If you need multi-tenancy , you must manage table names manually, often by using prefixes for different datasets. Since QuestDB does not support multiple schemas, this is the primary way to segment data. In QuestDB Enterprise, you can enforce permissions per table to restrict access , allowing finer control over multi-tenant environments. Here are common patterns for implementing multi-tenancy: Customer-specific tables ​ -- Customer-specific trading data CREATE TABLE customer1_trades ( timestamp TIMESTAMP, symbol SYMBOL, price DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY; CREATE TABLE customer2_trades ( timestamp TIMESTAMP, symbol SYMBOL, price DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY; Environment or region-based separation ​ -- Production vs. Development environments CREATE TABLE prod_metrics ( timestamp TIMESTAMP, metric_name SYMBOL, value DOUBLE ) TIMESTAMP(timestamp); CREATE TABLE dev_metrics ( timestamp TIMESTAMP, metric_name SYMBOL, value DOUBLE ) TIMESTAMP(time"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "ATE TABLE prod_metrics ( timestamp TIMESTAMP, metric_name SYMBOL, value DOUBLE ) TIMESTAMP(timestamp); CREATE TABLE dev_metrics ( timestamp TIMESTAMP, metric_name SYMBOL, value DOUBLE ) TIMESTAMP(timestamp); -- Regional data separation CREATE TABLE eu_users ( timestamp TIMESTAMP, user_id SYMBOL, action SYMBOL ) TIMESTAMP(timestamp); CREATE TABLE us_users ( timestamp TIMESTAMP, user_id SYMBOL, action SYMBOL ) TIMESTAMP(timestamp); Department or team-based separation ​ -- Department-specific analytics CREATE TABLE sales_daily_stats ( timestamp TIMESTAMP, region SYMBOL, revenue DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY; CREATE TABLE marketing_campaign_metrics ( timestamp TIMESTAMP, campaign_id SYMBOL, clicks LONG, impressions LONG ) TIMESTAMP(timestamp) PARTITION BY DAY; tip When using table prefixes for multi-tenancy: Use consistent naming conventions (e.g., always <tenant>_<table> ) Consider using uppercase for tenant identifiers to improve readability Document your naming convention in your team's schema design guidelines PostgreSQL protocol compatibility ​ QuestDB is not a PostgreSQL database but is compatible with the PostgreSQL wire protocol . This means you can connect usi"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "on in your team's schema design guidelines PostgreSQL protocol compatibility ​ QuestDB is not a PostgreSQL database but is compatible with the PostgreSQL wire protocol . This means you can connect using PostgreSQL-compatible libraries and clients and execute SQL commands. However, compatibility with PostgreSQL system catalogs, metadata queries, data types, and functions is limited. While most PostgreSQL-compatible low-level libraries work with QuestDB, some higher-level components that depend heavily on PostgreSQL metadata might fail. If you encounter such a case, please report it as an issue on GitHub so we can track it. Creating a schema in QuestDB ​ Recommended approach ​ The easiest way to create a schema is through the Web Console or by sending SQL commands using: The REST API ( CREATE TABLE statements) The PostgreSQL wire protocol clients Schema auto-creation with ILP protocol ​ When using the Influx Line Protocol (ILP) , QuestDB automatically creates tables and columns based on incoming data. This is useful for users migrating from InfluxDB or using tools like InfluxDB client libraries or Telegraf , as they can send data directly to QuestDB without pre-defining schemas. Howe"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "incoming data. This is useful for users migrating from InfluxDB or using tools like InfluxDB client libraries or Telegraf , as they can send data directly to QuestDB without pre-defining schemas. However, this comes with limitations: QuestDB applies the default settings to auto-created tables and columns (e.g., partitioning, symbol capacity, and data types). You cannot modify partitioning or symbol capacity later . You cannot auto-create the IPv4 data type. Sending an IP address as a string will create a VARCHAR column. You can disable column auto-creation via configuration . The designated timestamp and partitioning strategy ​ QuestDB is designed for time-series workloads. The database engine is optimized to perform exceptionally well for time-series queries. One of the most important optimizations in QuestDB is that data is physically stored and ordered by incremental timestamp. Therefore, the user must choose the designated timestamp when creating a table. The designated timestamp is crucial in QuestDB. It directly affects: How QuestDB partitions data (by hour, day, week, month, or year). Physical data storage order , as data is always stored sorted by the designated timestamp ."}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "s crucial in QuestDB. It directly affects: How QuestDB partitions data (by hour, day, week, month, or year). Physical data storage order , as data is always stored sorted by the designated timestamp . Query efficiency , since QuestDB prunes partitions based on the timestamp range in your query, reducing disk I/O. Insertion performance , because out-of-order data forces QuestDB to rewrite partitions , slowing down ingestion. Partitioning guidelines ​ When choosing the partition resolution for your tables, consider the time ranges you will query most frequently and keep in mind the following: Avoid very large partitions : A partition should be at most a few gigabytes . Avoid too many small partitions : Querying more partitions means opening more files. Query efficiency : When filtering data, QuestDB prunes partitions, but querying many partitions results in more disk operations. If most of your queries span a monthly range, weekly or daily partitioning sounds sensible, but hourly partitioning might slow down your queries. Data ingestion performance : If data arrives out of order, QuestDB rewrites the active partition, impacting performance. Columnar storage model and table density ​"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "oning might slow down your queries. Data ingestion performance : If data arrives out of order, QuestDB rewrites the active partition, impacting performance. Columnar storage model and table density ​ QuestDB is columnar , meaning: Columns are stored separately , allowing fast queries on specific columns without loading unnecessary data. Each column is stored in one or two files per partition : The more columns you include in a SELECT and the more partitions the query spans, the more files will need to be opened and cached into working memory. Sparse vs. dense tables ​ QuestDB handles wide tables efficiently due to its columnar architecture, as it will open only the column files referenced in each query. Null values take storage space , so it is recommended to avoid sparse tables where possible. Dense tables (where most columns have values) are more efficient in terms of storage and query performance. If you cannot design a dense table, consider creating different tables for distinct record structures. Data types and best practices ​ Symbols (recommended for categorical data) ​ QuestDB introduces a specialized SYMBOL data type. Symbols are dictionary-encoded and optimized for filter"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "cord structures. Data types and best practices ​ Symbols (recommended for categorical data) ​ QuestDB introduces a specialized SYMBOL data type. Symbols are dictionary-encoded and optimized for filtering and grouping: Use symbols for categorical data with a limited number of unique values (e.g., country codes, stock tickers, factory floor IDs). Symbols are fine for storing up to a few million distinct values but should be avoided beyond that. Avoid using a SYMBOL for columns that would be considered a PRIMARY KEY in other databases. If very high cardinality is expected , use VARCHAR instead of SYMBOL . Symbols are compact on disk , reducing storage overhead. Symbol capacity defaults to 256 , but it will dynamically expand as needed, causing temporary slowdowns. If you expect high cardinality, define the symbol capacity at table creation time to avoid performance issues. Timestamps ​ All timestamps in QuestDB are stored in UTC at Microsecond resolution : Even if you can ingest data sending timestamps in nanoseconds, nanosecond precision is not retained. The TIMESTAMP type is recommended over DATETIME , unless you have checked the data types reference and you know what you are doing."}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "ing timestamps in nanoseconds, nanosecond precision is not retained. The TIMESTAMP type is recommended over DATETIME , unless you have checked the data types reference and you know what you are doing. At query time, you can apply a time zone conversion for display purposes . Strings vs. varchar ​ Avoid STRING : It is a legacy data type. Use VARCHAR instead for general string storage. UUIDs ​ QuestDB has a dedicated UUID type, which is more efficient than storing UUIDs as VARCHAR . Other data types ​ Booleans : true / false values are supported. Bytes : BYTES type allows storing raw binary data. IPv4 : QuestDB has a dedicated IPv4 type for optimized IP storage and filtering. Several numeric datatypes are supported. Geo : QuestDB provides spatial support via geohashes . Referential integrity, constraints, and deduplication ​ QuestDB does not enforce PRIMARY KEYS , FOREIGN KEYS , or NOT NULL constraints. Joins between tables work even without referential integrity , as long as the data types on the join condition are compatible. Duplicate data is allowed by default , but UPSERT KEYS can be defined to ensure uniqueness . Deduplication in QuestDB happens on an exact timestamp and option"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "pes on the join condition are compatible. Duplicate data is allowed by default , but UPSERT KEYS can be defined to ensure uniqueness . Deduplication in QuestDB happens on an exact timestamp and optionally a set of other columns ( UPSERT KEYS ) . Deduplication has no noticeable performance penalty . Retention strategies with TTL and materialized views ​ Since individual row deletions are not supported , data retention is managed via: Setting a TTL retention period per table to control partition expiration. Materialized views : QuestDB automatically refreshes materialized views , storing aggregated data at lower granularity. You can also apply TTL expiration on the base table. Schema decisions that cannot be easily changed ​ Some table properties cannot be modified after creation , including: The designated timestamp (cannot be altered once set). Partitioning strategy (cannot be changed later). Symbol capacity (must be defined upfront, otherwise defaults apply). For changes, the typical workaround is: Create a new column with the updated configuration. Copy data from the old column into the new one. Drop the old column and rename the new one. If changes affect table-wide properties ("}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "workaround is: Create a new column with the updated configuration. Copy data from the old column into the new one. Drop the old column and rename the new one. If changes affect table-wide properties (e.g., partitioning, timestamp column, or WAL settings), create a new table with the required properties, insert data from the old table , drop the old table, and rename the new table. Examples of schema translations from other databases ​ -- PostgreSQL CREATE TABLE metrics ( timestamp TIMESTAMP PRIMARY KEY, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE PRECISION NOT NULL ); CREATE INDEX ON metrics (name, timestamp); -- UPSERT behavior in PostgreSQL INSERT INTO metrics (timestamp, name, description, unit, id, value) VALUES (...) ON CONFLICT (timestamp, name) DO UPDATE SET description = EXCLUDED.description, unit = EXCLUDED.unit, value = EXCLUDED.value; -- Timescale CREATE TABLE metrics ( timestamp TIMESTAMPTZ NOT NULL, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE PRECISION NOT NULL ); SELECT create_hypertable('metrics', 'timestamp'); CREATE INDEX ON metrics (name, tim"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE PRECISION NOT NULL ); SELECT create_hypertable('metrics', 'timestamp'); CREATE INDEX ON metrics (name, timestamp); -- UPSERT behavior in Timescale INSERT INTO metrics (timestamp, name, description, unit, id, value) VALUES (...) ON CONFLICT (timestamp, name) DO UPDATE SET description = EXCLUDED.description, unit = EXCLUDED.unit, value = EXCLUDED.value; Create sample table with deduplication/upsert for DuckDB -- DuckDB CREATE TABLE metrics ( timestamp TIMESTAMP NOT NULL, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE NOT NULL ); CREATE INDEX ON metrics (name, timestamp); -- UPSERT behavior in DuckDB INSERT INTO metrics (timestamp, name, description, unit, id, value) VALUES (?, ?, ?, ?, ?, ?) ON CONFLICT (timestamp, name) DO UPDATE SET description = EXCLUDED.description, unit = EXCLUDED.unit, value = EXCLUDED.value; Create sample table with eventual upserts for ClickHouse -- ClickHouse CREATE TABLE metrics ( timestamp DateTime, name String, description String, unit String, id UUID, value Float64 ) ENGINE = ReplacingMergeTree ORDER BY"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Introduction", "text": "th eventual upserts for ClickHouse -- ClickHouse CREATE TABLE metrics ( timestamp DateTime, name String, description String, unit String, id UUID, value Float64 ) ENGINE = ReplacingMergeTree ORDER BY (name, timestamp); Create sample measure (table) for InfluxDB -- InfluxDB measurement measurement: metrics name (tag) description (tag) unit (tag) id (tag) value (field) Create sample table with deduplication/upsert for QuestDB -- QuestDB equivalent CREATE TABLE metrics ( timestamp TIMESTAMP, -- Explicit timestamp for time-series queries name SYMBOL CAPACITY 50000, -- Optimized for high-cardinality categorical values description VARCHAR, -- Free-text description, not ideal for SYMBOL indexing unit SYMBOL CAPACITY 256, -- Limited set of unit types, efficient as SYMBOL id UUID, -- UUID optimized for unique identifiers value DOUBLE -- Numeric measurement field ) TIMESTAMP(timestamp) PARTITION BY DAY WAL DEDUP UPSERT KEYS(timestamp, name);"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Schema Design", "text": "This guide covers key concepts and best practices to take full advantage of QuestDB's performance-oriented architecture, highlighting some important differences with most databases."}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "QuestDB's single database model ​", "text": "QuestDB has a single database per instance . Unlike PostgreSQL and other database engines, where you may have multiple databases or multiple schemas within an instance, in QuestDB, you operate within a single namespace.\n\nThe default database is named qdb , and this can be changed via configuration. However, unlike a standard SQL database, there is no need to issue USE DATABASE commands. Once connected, you can immediately start querying and inserting data.\n\nqdb\n\nUSE DATABASE"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Multi-tenancy considerations ​", "text": "If you need multi-tenancy , you must manage table names manually, often by using prefixes for different datasets. Since QuestDB does not support multiple schemas, this is the primary way to segment data. In QuestDB Enterprise, you can enforce permissions per table to restrict access , allowing finer control over multi-tenant environments.\n\nHere are common patterns for implementing multi-tenancy:\n\n-- Customer-specific trading data CREATE TABLE customer1_trades ( timestamp TIMESTAMP, symbol SYMBOL, price DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY; CREATE TABLE customer2_trades ( timestamp TIMESTAMP, symbol SYMBOL, price DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY;\n\n-- Customer-specific trading data CREATE TABLE customer1_trades ( timestamp TIMESTAMP, symbol SYMBOL, price DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY; CREATE TABLE customer2_trades ( timestamp TIMESTAMP, symbol SYMBOL, price DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY;\n\n-- Customer-specific trading data CREATE TABLE customer1_trades ( timestamp TIMESTAMP, symbol SYMBOL, price DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY; CREATE TABLE customer2_trades ( timestamp TIMESTAMP, symbol SYMBOL, price DOUBLE ) TIM"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Multi-tenancy considerations ​", "text": "ABLE customer1_trades ( timestamp TIMESTAMP, symbol SYMBOL, price DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY; CREATE TABLE customer2_trades ( timestamp TIMESTAMP, symbol SYMBOL, price DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY;\n\n-- Customer-specific trading data CREATE TABLE customer1_trades ( timestamp TIMESTAMP, symbol SYMBOL, price DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY; CREATE TABLE customer2_trades ( timestamp TIMESTAMP, symbol SYMBOL, price DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY;\n\n-- Production vs. Development environments CREATE TABLE prod_metrics ( timestamp TIMESTAMP, metric_name SYMBOL, value DOUBLE ) TIMESTAMP(timestamp); CREATE TABLE dev_metrics ( timestamp TIMESTAMP, metric_name SYMBOL, value DOUBLE ) TIMESTAMP(timestamp); -- Regional data separation CREATE TABLE eu_users ( timestamp TIMESTAMP, user_id SYMBOL, action SYMBOL ) TIMESTAMP(timestamp); CREATE TABLE us_users ( timestamp TIMESTAMP, user_id SYMBOL, action SYMBOL ) TIMESTAMP(timestamp);\n\n-- Production vs. Development environments CREATE TABLE prod_metrics ( timestamp TIMESTAMP, metric_name SYMBOL, value DOUBLE ) TIMESTAMP(timestamp); CREATE TABLE dev_metrics ( timestamp TIMESTAMP, metric"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Multi-tenancy considerations ​", "text": "Production vs. Development environments CREATE TABLE prod_metrics ( timestamp TIMESTAMP, metric_name SYMBOL, value DOUBLE ) TIMESTAMP(timestamp); CREATE TABLE dev_metrics ( timestamp TIMESTAMP, metric_name SYMBOL, value DOUBLE ) TIMESTAMP(timestamp); -- Regional data separation CREATE TABLE eu_users ( timestamp TIMESTAMP, user_id SYMBOL, action SYMBOL ) TIMESTAMP(timestamp); CREATE TABLE us_users ( timestamp TIMESTAMP, user_id SYMBOL, action SYMBOL ) TIMESTAMP(timestamp);\n\n-- Production vs. Development environments CREATE TABLE prod_metrics ( timestamp TIMESTAMP, metric_name SYMBOL, value DOUBLE ) TIMESTAMP(timestamp); CREATE TABLE dev_metrics ( timestamp TIMESTAMP, metric_name SYMBOL, value DOUBLE ) TIMESTAMP(timestamp); -- Regional data separation CREATE TABLE eu_users ( timestamp TIMESTAMP, user_id SYMBOL, action SYMBOL ) TIMESTAMP(timestamp); CREATE TABLE us_users ( timestamp TIMESTAMP, user_id SYMBOL, action SYMBOL ) TIMESTAMP(timestamp);\n\n-- Production vs. Development environments CREATE TABLE prod_metrics ( timestamp TIMESTAMP, metric_name SYMBOL, value DOUBLE ) TIMESTAMP(timestamp); CREATE TABLE dev_metrics ( timestamp TIMESTAMP, metric_name SYMBOL, value DOUBLE ) TIMESTAMP"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Multi-tenancy considerations ​", "text": "s CREATE TABLE prod_metrics ( timestamp TIMESTAMP, metric_name SYMBOL, value DOUBLE ) TIMESTAMP(timestamp); CREATE TABLE dev_metrics ( timestamp TIMESTAMP, metric_name SYMBOL, value DOUBLE ) TIMESTAMP(timestamp); -- Regional data separation CREATE TABLE eu_users ( timestamp TIMESTAMP, user_id SYMBOL, action SYMBOL ) TIMESTAMP(timestamp); CREATE TABLE us_users ( timestamp TIMESTAMP, user_id SYMBOL, action SYMBOL ) TIMESTAMP(timestamp);\n\n-- Department-specific analytics CREATE TABLE sales_daily_stats ( timestamp TIMESTAMP, region SYMBOL, revenue DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY; CREATE TABLE marketing_campaign_metrics ( timestamp TIMESTAMP, campaign_id SYMBOL, clicks LONG, impressions LONG ) TIMESTAMP(timestamp) PARTITION BY DAY;\n\n-- Department-specific analytics CREATE TABLE sales_daily_stats ( timestamp TIMESTAMP, region SYMBOL, revenue DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY; CREATE TABLE marketing_campaign_metrics ( timestamp TIMESTAMP, campaign_id SYMBOL, clicks LONG, impressions LONG ) TIMESTAMP(timestamp) PARTITION BY DAY;\n\n-- Department-specific analytics CREATE TABLE sales_daily_stats ( timestamp TIMESTAMP, region SYMBOL, revenue DOUBLE ) TIMESTAMP(times"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Multi-tenancy considerations ​", "text": "LONG, impressions LONG ) TIMESTAMP(timestamp) PARTITION BY DAY;\n\n-- Department-specific analytics CREATE TABLE sales_daily_stats ( timestamp TIMESTAMP, region SYMBOL, revenue DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY; CREATE TABLE marketing_campaign_metrics ( timestamp TIMESTAMP, campaign_id SYMBOL, clicks LONG, impressions LONG ) TIMESTAMP(timestamp) PARTITION BY DAY;\n\n-- Department-specific analytics CREATE TABLE sales_daily_stats ( timestamp TIMESTAMP, region SYMBOL, revenue DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY; CREATE TABLE marketing_campaign_metrics ( timestamp TIMESTAMP, campaign_id SYMBOL, clicks LONG, impressions LONG ) TIMESTAMP(timestamp) PARTITION BY DAY;\n\ntip When using table prefixes for multi-tenancy: Use consistent naming conventions (e.g., always <tenant>_<table> ) Consider using uppercase for tenant identifiers to improve readability Document your naming convention in your team's schema design guidelines\n\ntip\n\nWhen using table prefixes for multi-tenancy: Use consistent naming conventions (e.g., always <tenant>_<table> ) Consider using uppercase for tenant identifiers to improve readability Document your naming convention in your team's schema design"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Multi-tenancy considerations ​", "text": ": Use consistent naming conventions (e.g., always <tenant>_<table> ) Consider using uppercase for tenant identifiers to improve readability Document your naming convention in your team's schema design guidelines\n\nWhen using table prefixes for multi-tenancy:\n\nUse consistent naming conventions (e.g., always <tenant>_<table> )\n\n<tenant>_<table>\n\nConsider using uppercase for tenant identifiers to improve readability\n\nDocument your naming convention in your team's schema design guidelines"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "PostgreSQL protocol compatibility ​", "text": "QuestDB is not a PostgreSQL database but is compatible with the PostgreSQL wire protocol . This means you can connect using PostgreSQL-compatible libraries and clients and execute SQL commands. However, compatibility with PostgreSQL system catalogs, metadata queries, data types, and functions is limited.\n\nWhile most PostgreSQL-compatible low-level libraries work with QuestDB, some higher-level components that depend heavily on PostgreSQL metadata might fail. If you encounter such a case, please report it as an issue on GitHub so we can track it."}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Recommended approach ​", "text": "The easiest way to create a schema is through the Web Console or by sending SQL commands using:\n\nThe REST API ( CREATE TABLE statements)\n\nCREATE TABLE\n\nThe PostgreSQL wire protocol clients"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Schema auto-creation with ILP protocol ​", "text": "When using the Influx Line Protocol (ILP) , QuestDB automatically creates tables and columns based on incoming data. This is useful for users migrating from InfluxDB or using tools like InfluxDB client libraries or Telegraf , as they can send data directly to QuestDB without pre-defining schemas. However, this comes with limitations:\n\nQuestDB applies the default settings to auto-created tables and columns (e.g., partitioning, symbol capacity, and data types).\n\nYou cannot modify partitioning or symbol capacity later .\n\nYou cannot auto-create the IPv4 data type. Sending an IP address as a string will create a VARCHAR column.\n\nIPv4\n\nVARCHAR\n\nYou can disable column auto-creation via configuration ."}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "The designated timestamp and partitioning strategy ​", "text": "QuestDB is designed for time-series workloads. The database engine is optimized to perform exceptionally well for time-series queries. One of the most important optimizations in QuestDB is that data is physically stored and ordered by incremental timestamp. Therefore, the user must choose the designated timestamp when creating a table.\n\nThe designated timestamp is crucial in QuestDB. It directly affects:\n\nHow QuestDB partitions data (by hour, day, week, month, or year).\n\nPhysical data storage order , as data is always stored sorted by the designated timestamp .\n\nQuery efficiency , since QuestDB prunes partitions based on the timestamp range in your query, reducing disk I/O.\n\nInsertion performance , because out-of-order data forces QuestDB to rewrite partitions , slowing down ingestion."}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Partitioning guidelines ​", "text": "When choosing the partition resolution for your tables, consider the time ranges you will query most frequently and keep in mind the following:\n\nAvoid very large partitions : A partition should be at most a few gigabytes .\n\nAvoid too many small partitions : Querying more partitions means opening more files.\n\nQuery efficiency : When filtering data, QuestDB prunes partitions, but querying many partitions results in more disk operations. If most of your queries span a monthly range, weekly or daily partitioning sounds sensible, but hourly partitioning might slow down your queries.\n\nData ingestion performance : If data arrives out of order, QuestDB rewrites the active partition, impacting performance."}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Columnar storage model and table density ​", "text": "QuestDB is columnar , meaning:\n\nColumns are stored separately , allowing fast queries on specific columns without loading unnecessary data.\n\nEach column is stored in one or two files per partition : The more columns you include in a SELECT and the more partitions the query spans, the more files will need to be opened and cached into working memory.\n\nSELECT"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Sparse vs. dense tables ​", "text": "QuestDB handles wide tables efficiently due to its columnar architecture, as it will open only the column files referenced in each query.\n\nNull values take storage space , so it is recommended to avoid sparse tables where possible.\n\nDense tables (where most columns have values) are more efficient in terms of storage and query performance. If you cannot design a dense table, consider creating different tables for distinct record structures."}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Symbols (recommended for categorical data) ​", "text": "QuestDB introduces a specialized SYMBOL data type. Symbols are dictionary-encoded and optimized for filtering and grouping:\n\nSYMBOL\n\nUse symbols for categorical data with a limited number of unique values (e.g., country codes, stock tickers, factory floor IDs).\n\nSymbols are fine for storing up to a few million distinct values but should be avoided beyond that.\n\nAvoid using a SYMBOL for columns that would be considered a PRIMARY KEY in other databases.\n\nSYMBOL\n\nPRIMARY KEY\n\nIf very high cardinality is expected , use VARCHAR instead of SYMBOL .\n\nVARCHAR\n\nSYMBOL\n\nSymbols are compact on disk , reducing storage overhead.\n\nSymbol capacity defaults to 256 , but it will dynamically expand as needed, causing temporary slowdowns.\n\nIf you expect high cardinality, define the symbol capacity at table creation time to avoid performance issues."}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Timestamps ​", "text": "All timestamps in QuestDB are stored in UTC at Microsecond resolution : Even if you can ingest data sending timestamps in nanoseconds, nanosecond precision is not retained.\n\nThe TIMESTAMP type is recommended over DATETIME , unless you have checked the data types reference and you know what you are doing.\n\nTIMESTAMP\n\nDATETIME\n\nAt query time, you can apply a time zone conversion for display purposes ."}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Strings vs. varchar ​", "text": "Avoid STRING : It is a legacy data type.\n\nSTRING\n\nUse VARCHAR instead for general string storage.\n\nVARCHAR"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "UUIDs ​", "text": "QuestDB has a dedicated UUID type, which is more efficient than storing UUIDs as VARCHAR .\n\nUUID\n\nVARCHAR"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Other data types ​", "text": "Booleans : true / false values are supported.\n\ntrue\n\nfalse\n\nBytes : BYTES type allows storing raw binary data.\n\nBYTES\n\nIPv4 : QuestDB has a dedicated IPv4 type for optimized IP storage and filtering.\n\nIPv4\n\nSeveral numeric datatypes are supported.\n\nGeo : QuestDB provides spatial support via geohashes ."}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Referential integrity, constraints, and deduplication ​", "text": "QuestDB does not enforce PRIMARY KEYS , FOREIGN KEYS , or NOT NULL constraints.\n\nPRIMARY KEYS\n\nFOREIGN KEYS\n\nNOT NULL\n\nJoins between tables work even without referential integrity , as long as the data types on the join condition are compatible.\n\nDuplicate data is allowed by default , but UPSERT KEYS can be defined to ensure uniqueness .\n\nUPSERT KEYS\n\nDeduplication in QuestDB happens on an exact timestamp and optionally a set of other columns ( UPSERT KEYS ) .\n\nUPSERT KEYS\n\nDeduplication has no noticeable performance penalty ."}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Retention strategies with TTL and materialized views ​", "text": "Since individual row deletions are not supported , data retention is managed via:\n\nSetting a TTL retention period per table to control partition expiration.\n\nMaterialized views : QuestDB automatically refreshes materialized views , storing aggregated data at lower granularity. You can also apply TTL expiration on the base table."}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Schema decisions that cannot be easily changed ​", "text": "Some table properties cannot be modified after creation , including:\n\nThe designated timestamp (cannot be altered once set).\n\nPartitioning strategy (cannot be changed later).\n\nSymbol capacity (must be defined upfront, otherwise defaults apply).\n\nFor changes, the typical workaround is:\n\nCreate a new column with the updated configuration.\n\nCopy data from the old column into the new one.\n\nDrop the old column and rename the new one.\n\nIf changes affect table-wide properties (e.g., partitioning, timestamp column, or WAL settings), create a new table with the required properties, insert data from the old table , drop the old table, and rename the new table."}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Examples of schema translations from other databases ​", "text": "-- PostgreSQL CREATE TABLE metrics ( timestamp TIMESTAMP PRIMARY KEY, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE PRECISION NOT NULL ); CREATE INDEX ON metrics (name, timestamp); -- UPSERT behavior in PostgreSQL INSERT INTO metrics (timestamp, name, description, unit, id, value) VALUES (...) ON CONFLICT (timestamp, name) DO UPDATE SET description = EXCLUDED.description, unit = EXCLUDED.unit, value = EXCLUDED.value;\n\n-- PostgreSQL CREATE TABLE metrics ( timestamp TIMESTAMP PRIMARY KEY, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE PRECISION NOT NULL ); CREATE INDEX ON metrics (name, timestamp); -- UPSERT behavior in PostgreSQL INSERT INTO metrics (timestamp, name, description, unit, id, value) VALUES (...) ON CONFLICT (timestamp, name) DO UPDATE SET description = EXCLUDED.description, unit = EXCLUDED.unit, value = EXCLUDED.value;\n\n-- PostgreSQL CREATE TABLE metrics ( timestamp TIMESTAMP PRIMARY KEY, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE PRECISION NOT NULL ); CREATE INDEX ON metrics (name, timesta"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Examples of schema translations from other databases ​", "text": "timestamp TIMESTAMP PRIMARY KEY, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE PRECISION NOT NULL ); CREATE INDEX ON metrics (name, timestamp); -- UPSERT behavior in PostgreSQL INSERT INTO metrics (timestamp, name, description, unit, id, value) VALUES (...) ON CONFLICT (timestamp, name) DO UPDATE SET description = EXCLUDED.description, unit = EXCLUDED.unit, value = EXCLUDED.value;\n\n-- PostgreSQL CREATE TABLE metrics ( timestamp TIMESTAMP PRIMARY KEY, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE PRECISION NOT NULL ); CREATE INDEX ON metrics (name, timestamp); -- UPSERT behavior in PostgreSQL INSERT INTO metrics (timestamp, name, description, unit, id, value) VALUES (...) ON CONFLICT (timestamp, name) DO UPDATE SET description = EXCLUDED.description, unit = EXCLUDED.unit, value = EXCLUDED.value;\n\n-- Timescale CREATE TABLE metrics ( timestamp TIMESTAMPTZ NOT NULL, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE PRECISION NOT NULL ); SELECT create_hypertable('metrics', 'timestamp'); CREATE INDEX ON metrics (n"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Examples of schema translations from other databases ​", "text": "VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE PRECISION NOT NULL ); SELECT create_hypertable('metrics', 'timestamp'); CREATE INDEX ON metrics (name, timestamp); -- UPSERT behavior in Timescale INSERT INTO metrics (timestamp, name, description, unit, id, value) VALUES (...) ON CONFLICT (timestamp, name) DO UPDATE SET description = EXCLUDED.description, unit = EXCLUDED.unit, value = EXCLUDED.value;\n\n-- Timescale CREATE TABLE metrics ( timestamp TIMESTAMPTZ NOT NULL, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE PRECISION NOT NULL ); SELECT create_hypertable('metrics', 'timestamp'); CREATE INDEX ON metrics (name, timestamp); -- UPSERT behavior in Timescale INSERT INTO metrics (timestamp, name, description, unit, id, value) VALUES (...) ON CONFLICT (timestamp, name) DO UPDATE SET description = EXCLUDED.description, unit = EXCLUDED.unit, value = EXCLUDED.value;\n\n-- Timescale CREATE TABLE metrics ( timestamp TIMESTAMPTZ NOT NULL, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE PRECISION NOT NULL ); SELECT create_hype"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Examples of schema translations from other databases ​", "text": "REATE TABLE metrics ( timestamp TIMESTAMPTZ NOT NULL, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE PRECISION NOT NULL ); SELECT create_hypertable('metrics', 'timestamp'); CREATE INDEX ON metrics (name, timestamp); -- UPSERT behavior in Timescale INSERT INTO metrics (timestamp, name, description, unit, id, value) VALUES (...) ON CONFLICT (timestamp, name) DO UPDATE SET description = EXCLUDED.description, unit = EXCLUDED.unit, value = EXCLUDED.value;\n\n-- Timescale CREATE TABLE metrics ( timestamp TIMESTAMPTZ NOT NULL, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE PRECISION NOT NULL ); SELECT create_hypertable('metrics', 'timestamp'); CREATE INDEX ON metrics (name, timestamp); -- UPSERT behavior in Timescale INSERT INTO metrics (timestamp, name, description, unit, id, value) VALUES (...) ON CONFLICT (timestamp, name) DO UPDATE SET description = EXCLUDED.description, unit = EXCLUDED.unit, value = EXCLUDED.value;\n\nCreate sample table with deduplication/upsert for DuckDB -- DuckDB CREATE TABLE metrics ( timestamp TIMESTAMP NOT NULL, name VARCHAR(255) NOT NULL, descript"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Examples of schema translations from other databases ​", "text": "t = EXCLUDED.unit, value = EXCLUDED.value;\n\nCreate sample table with deduplication/upsert for DuckDB -- DuckDB CREATE TABLE metrics ( timestamp TIMESTAMP NOT NULL, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE NOT NULL ); CREATE INDEX ON metrics (name, timestamp); -- UPSERT behavior in DuckDB INSERT INTO metrics (timestamp, name, description, unit, id, value) VALUES (?, ?, ?, ?, ?, ?) ON CONFLICT (timestamp, name) DO UPDATE SET description = EXCLUDED.description, unit = EXCLUDED.unit, value = EXCLUDED.value;\n\nCreate sample table with deduplication/upsert for DuckDB\n\n-- DuckDB CREATE TABLE metrics ( timestamp TIMESTAMP NOT NULL, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE NOT NULL ); CREATE INDEX ON metrics (name, timestamp); -- UPSERT behavior in DuckDB INSERT INTO metrics (timestamp, name, description, unit, id, value) VALUES (?, ?, ?, ?, ?, ?) ON CONFLICT (timestamp, name) DO UPDATE SET description = EXCLUDED.description, unit = EXCLUDED.unit, value = EXCLUDED.value;\n\n-- DuckDB CREATE TABLE metrics ( timestamp TIMESTAMP NOT NULL, name VARCHAR(255) NOT NULL, de"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Examples of schema translations from other databases ​", "text": "mp, name) DO UPDATE SET description = EXCLUDED.description, unit = EXCLUDED.unit, value = EXCLUDED.value;\n\n-- DuckDB CREATE TABLE metrics ( timestamp TIMESTAMP NOT NULL, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE NOT NULL ); CREATE INDEX ON metrics (name, timestamp); -- UPSERT behavior in DuckDB INSERT INTO metrics (timestamp, name, description, unit, id, value) VALUES (?, ?, ?, ?, ?, ?) ON CONFLICT (timestamp, name) DO UPDATE SET description = EXCLUDED.description, unit = EXCLUDED.unit, value = EXCLUDED.value;\n\n-- DuckDB CREATE TABLE metrics ( timestamp TIMESTAMP NOT NULL, name VARCHAR(255) NOT NULL, description VARCHAR(500), unit VARCHAR(50), id UUID PRIMARY KEY, value DOUBLE NOT NULL ); CREATE INDEX ON metrics (name, timestamp); -- UPSERT behavior in DuckDB INSERT INTO metrics (timestamp, name, description, unit, id, value) VALUES (?, ?, ?, ?, ?, ?) ON CONFLICT (timestamp, name) DO UPDATE SET description = EXCLUDED.description, unit = EXCLUDED.unit, value = EXCLUDED.value;\n\nCreate sample table with eventual upserts for ClickHouse -- ClickHouse CREATE TABLE metrics ( timestamp DateTime, name String, description String"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Examples of schema translations from other databases ​", "text": "iption, unit = EXCLUDED.unit, value = EXCLUDED.value;\n\nCreate sample table with eventual upserts for ClickHouse -- ClickHouse CREATE TABLE metrics ( timestamp DateTime, name String, description String, unit String, id UUID, value Float64 ) ENGINE = ReplacingMergeTree ORDER BY (name, timestamp);\n\nCreate sample table with eventual upserts for ClickHouse\n\n-- ClickHouse CREATE TABLE metrics ( timestamp DateTime, name String, description String, unit String, id UUID, value Float64 ) ENGINE = ReplacingMergeTree ORDER BY (name, timestamp);\n\n-- ClickHouse CREATE TABLE metrics ( timestamp DateTime, name String, description String, unit String, id UUID, value Float64 ) ENGINE = ReplacingMergeTree ORDER BY (name, timestamp);\n\n-- ClickHouse CREATE TABLE metrics ( timestamp DateTime, name String, description String, unit String, id UUID, value Float64 ) ENGINE = ReplacingMergeTree ORDER BY (name, timestamp);\n\nCreate sample measure (table) for InfluxDB -- InfluxDB measurement measurement: metrics name (tag) description (tag) unit (tag) id (tag) value (field)\n\nCreate sample measure (table) for InfluxDB\n\n-- InfluxDB measurement measurement: metrics name (tag) description (tag) unit (tag) id (tag)"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Examples of schema translations from other databases ​", "text": "me (tag) description (tag) unit (tag) id (tag) value (field)\n\nCreate sample measure (table) for InfluxDB\n\n-- InfluxDB measurement measurement: metrics name (tag) description (tag) unit (tag) id (tag) value (field)\n\n-- InfluxDB measurement measurement: metrics name (tag) description (tag) unit (tag) id (tag) value (field)\n\n-- InfluxDB measurement measurement: metrics name (tag) description (tag) unit (tag) id (tag) value (field)\n\nCreate sample table with deduplication/upsert for QuestDB -- QuestDB equivalent CREATE TABLE metrics ( timestamp TIMESTAMP, -- Explicit timestamp for time-series queries name SYMBOL CAPACITY 50000, -- Optimized for high-cardinality categorical values description VARCHAR, -- Free-text description, not ideal for SYMBOL indexing unit SYMBOL CAPACITY 256, -- Limited set of unit types, efficient as SYMBOL id UUID, -- UUID optimized for unique identifiers value DOUBLE -- Numeric measurement field ) TIMESTAMP(timestamp) PARTITION BY DAY WAL DEDUP UPSERT KEYS(timestamp, name);\n\nCreate sample table with deduplication/upsert for QuestDB\n\n-- QuestDB equivalent CREATE TABLE metrics ( timestamp TIMESTAMP, -- Explicit timestamp for time-series queries name SYMBOL CAPACIT"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Examples of schema translations from other databases ​", "text": ", name);\n\nCreate sample table with deduplication/upsert for QuestDB\n\n-- QuestDB equivalent CREATE TABLE metrics ( timestamp TIMESTAMP, -- Explicit timestamp for time-series queries name SYMBOL CAPACITY 50000, -- Optimized for high-cardinality categorical values description VARCHAR, -- Free-text description, not ideal for SYMBOL indexing unit SYMBOL CAPACITY 256, -- Limited set of unit types, efficient as SYMBOL id UUID, -- UUID optimized for unique identifiers value DOUBLE -- Numeric measurement field ) TIMESTAMP(timestamp) PARTITION BY DAY WAL DEDUP UPSERT KEYS(timestamp, name);\n\n-- QuestDB equivalent CREATE TABLE metrics ( timestamp TIMESTAMP, -- Explicit timestamp for time-series queries name SYMBOL CAPACITY 50000, -- Optimized for high-cardinality categorical values description VARCHAR, -- Free-text description, not ideal for SYMBOL indexing unit SYMBOL CAPACITY 256, -- Limited set of unit types, efficient as SYMBOL id UUID, -- UUID optimized for unique identifiers value DOUBLE -- Numeric measurement field ) TIMESTAMP(timestamp) PARTITION BY DAY WAL DEDUP UPSERT KEYS(timestamp, name);\n\n-- QuestDB equivalent CREATE TABLE metrics ( timestamp TIMESTAMP, -- Explicit timestamp for t"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Examples of schema translations from other databases ​", "text": "Numeric measurement field ) TIMESTAMP(timestamp) PARTITION BY DAY WAL DEDUP UPSERT KEYS(timestamp, name);\n\n-- QuestDB equivalent CREATE TABLE metrics ( timestamp TIMESTAMP, -- Explicit timestamp for time-series queries name SYMBOL CAPACITY 50000, -- Optimized for high-cardinality categorical values description VARCHAR, -- Free-text description, not ideal for SYMBOL indexing unit SYMBOL CAPACITY 256, -- Limited set of unit types, efficient as SYMBOL id UUID, -- UUID optimized for unique identifiers value DOUBLE -- Numeric measurement field ) TIMESTAMP(timestamp) PARTITION BY DAY WAL DEDUP UPSERT KEYS(timestamp, name);\n\nEdit this page\n\nEdit this page\n\nPrevious\n\nWhy QuestDB?\n\nNext\n\nOverview\n\nQuestDB's single database model Multi-tenancy considerations PostgreSQL protocol compatibility Creating a schema in QuestDB Recommended approach Schema auto-creation with ILP protocol The designated timestamp and partitioning strategy Partitioning guidelines Columnar storage model and table density Sparse vs. dense tables Data types and best practices Symbols (recommended for categorical data) Timestamps Strings vs. varchar UUIDs Other data types Referential integrity, constraints, and deduplicati"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Examples of schema translations from other databases ​", "text": "arse vs. dense tables Data types and best practices Symbols (recommended for categorical data) Timestamps Strings vs. varchar UUIDs Other data types Referential integrity, constraints, and deduplication Retention strategies with TTL and materialized views Schema decisions that cannot be easily changed Examples of schema translations from other databases\n\nQuestDB's single database model Multi-tenancy considerations PostgreSQL protocol compatibility Creating a schema in QuestDB Recommended approach Schema auto-creation with ILP protocol The designated timestamp and partitioning strategy Partitioning guidelines Columnar storage model and table density Sparse vs. dense tables Data types and best practices Symbols (recommended for categorical data) Timestamps Strings vs. varchar UUIDs Other data types Referential integrity, constraints, and deduplication Retention strategies with TTL and materialized views Schema decisions that cannot be easily changed Examples of schema translations from other databases\n\nQuestDB's single database model Multi-tenancy considerations\n\nMulti-tenancy considerations\n\nPostgreSQL protocol compatibility\n\nCreating a schema in QuestDB Recommended approach Schema"}
{"source_url": "https://questdb.com/docs/guides/schema-design-essentials", "title": "Schema Design", "section": "Examples of schema translations from other databases ​", "text": "ther databases\n\nQuestDB's single database model Multi-tenancy considerations\n\nMulti-tenancy considerations\n\nPostgreSQL protocol compatibility\n\nCreating a schema in QuestDB Recommended approach Schema auto-creation with ILP protocol\n\nRecommended approach\n\nSchema auto-creation with ILP protocol\n\nThe designated timestamp and partitioning strategy Partitioning guidelines\n\nPartitioning guidelines\n\nColumnar storage model and table density Sparse vs. dense tables\n\nSparse vs. dense tables\n\nData types and best practices Symbols (recommended for categorical data) Timestamps Strings vs. varchar UUIDs Other data types\n\nSymbols (recommended for categorical data)\n\nTimestamps\n\nStrings vs. varchar\n\nUUIDs\n\nOther data types\n\nReferential integrity, constraints, and deduplication\n\nRetention strategies with TTL and materialized views\n\nSchema decisions that cannot be easily changed\n\nExamples of schema translations from other databases"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Introduction", "text": "Ingestion Reference Overview On this page Ingestion overview QuestDB makes top performance \"data-in\" easy. This guide will prepare you to get the most out of (and into!) QuestDB. Choose from first-party clients, apply message brokers, event streaming platforms, queues, and more. First-party clients ​ Recommended! Our first party clients are the fastest way to insert data, and they excel with high volume, high cardinality data streaming. To start quickly, select your language: C & C++ High-performance client for systems programming and embedded applications. Read more .NET Cross-platform client for building applications with .NET technologies. Read more Go An open-source programming language supported by Google with built-in concurrency. Read more Java Platform-independent client for enterprise applications and Android development. Read more Node.js Node.js® is an open-source, cross-platform JavaScript runtime environment. Read more Python Python is a programming language that lets you work quickly and integrate systems more effectively. Read more Rust Systems programming language focused on safety, speed, and concurrency. Read more Our clients utitilize the InfluxDB Line Protocol ("}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Introduction", "text": "work quickly and integrate systems more effectively. Read more Rust Systems programming language focused on safety, speed, and concurrency. Read more Our clients utitilize the InfluxDB Line Protocol (ILP) which is an insert-only protocol that bypasses SQL INSERT statements, thus achieving significantly higher throughput. It also provides some key benefits: Automatic table creation : No need to define your schema upfront. Concurrent schema changes : Seamlessly handle multiple data streams with on-the-fly schema modifications Optimized batching : Use strong defaults or curate the size of your batches Health checks and feedback : Ensure your system's integrity with built-in health monitoring Automatic write retries : Reuse connections and retry after interruptions An example of \"data-in\" - via the line - appears as: trades,symbol=ETH-USD,side=sell price=2615.54,amount=0.00044 1646762637609765000\\n trades,symbol=BTC-USD,side=sell price=39269.98,amount=0.001 1646762637710419000\\n trades,symbol=ETH-USD,side=buy price=2615.4,amount=0.002 1646762637764098000\\n Once inside of QuestDB, it's yours to manipulate and query via extended SQL. Please note that table and column names must follow th"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Introduction", "text": "bol=ETH-USD,side=buy price=2615.4,amount=0.002 1646762637764098000\\n Once inside of QuestDB, it's yours to manipulate and query via extended SQL. Please note that table and column names must follow the QuestDB naming rules . Message brokers and queues ​ Recommended! QuestDB supports several excellent message brokers, event streaming platforms and/or queues. Checkout our quick start guides for the following: Flink Kafka Redpanda Telegraf Easy CSV upload ​ Recommended! For GUI-driven CSV upload which leverages the built-in REST HTTP API , use the Import CSV tab in the Web Console : For all CSV import methods, including using the APIs directly, see the CSV Import Guide . PostgreSQL Wire Protocol ​ QuestDB also supports the PostgreSQL Wire Protocol (PGWire) . It offers most PostgreSQL keywords and functions, including parameterized queries and psql on the command line. While PGWire is supported, we recommend applying the first-party clients or other tools if possible. This is to take advantage of maximum performance and overcome limitations in the protocol. Create new data ​ No data yet? Just starting? No worries. We've got you covered. There are several quick scaffolding options: Ques"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Introduction", "text": "tage of maximum performance and overcome limitations in the protocol. Create new data ​ No data yet? Just starting? No worries. We've got you covered. There are several quick scaffolding options: QuestDB demo instance : Hosted, fully loaded and ready to go. Quickly explore the Web Console and SQL syntax. Create my first data set guide : Create tables, use rnd_ functions and make your own data. Sample dataset repos : IoT, e-commerce, finance or git logs? Check them out! Quick start repos : Code-based quick starts that cover ingestion, querying and data visualization using common programming languages and use cases. Also, a cat in a tracksuit. Time series streaming analytics template : A handy template for near real-time analytics using open source technologies. Next step - queries ​ Depending on your infrastructure, it should now be apparent which ingestion method is worth pursuing. Of course, ingestion (data-in) is only half the battle. Your next best step? Learn how to query and explore data-out from the Query & SQL Overview . It might also be a solid bet to review timestamp basics . Edit this page Previous Schema Design Next Configuration string First-party clients Message broker"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Introduction", "text": "re data-out from the Query & SQL Overview . It might also be a solid bet to review timestamp basics . Edit this page Previous Schema Design Next Configuration string First-party clients Message brokers and queues Easy CSV upload PostgreSQL Wire Protocol Create new data Next step - queries\n\nIngestion Reference Overview On this page Ingestion overview QuestDB makes top performance \"data-in\" easy. This guide will prepare you to get the most out of (and into!) QuestDB. Choose from first-party clients, apply message brokers, event streaming platforms, queues, and more. First-party clients ​ Recommended! Our first party clients are the fastest way to insert data, and they excel with high volume, high cardinality data streaming. To start quickly, select your language: C & C++ High-performance client for systems programming and embedded applications. Read more .NET Cross-platform client for building applications with .NET technologies. Read more Go An open-source programming language supported by Google with built-in concurrency. Read more Java Platform-independent client for enterprise applications and Android development. Read more Node.js Node.js® is an open-source, cross-platform JavaS"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Introduction", "text": "by Google with built-in concurrency. Read more Java Platform-independent client for enterprise applications and Android development. Read more Node.js Node.js® is an open-source, cross-platform JavaScript runtime environment. Read more Python Python is a programming language that lets you work quickly and integrate systems more effectively. Read more Rust Systems programming language focused on safety, speed, and concurrency. Read more Our clients utitilize the InfluxDB Line Protocol (ILP) which is an insert-only protocol that bypasses SQL INSERT statements, thus achieving significantly higher throughput. It also provides some key benefits: Automatic table creation : No need to define your schema upfront. Concurrent schema changes : Seamlessly handle multiple data streams with on-the-fly schema modifications Optimized batching : Use strong defaults or curate the size of your batches Health checks and feedback : Ensure your system's integrity with built-in health monitoring Automatic write retries : Reuse connections and retry after interruptions An example of \"data-in\" - via the line - appears as: trades,symbol=ETH-USD,side=sell price=2615.54,amount=0.00044 1646762637609765000\\n t"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Introduction", "text": "rite retries : Reuse connections and retry after interruptions An example of \"data-in\" - via the line - appears as: trades,symbol=ETH-USD,side=sell price=2615.54,amount=0.00044 1646762637609765000\\n trades,symbol=BTC-USD,side=sell price=39269.98,amount=0.001 1646762637710419000\\n trades,symbol=ETH-USD,side=buy price=2615.4,amount=0.002 1646762637764098000\\n Once inside of QuestDB, it's yours to manipulate and query via extended SQL. Please note that table and column names must follow the QuestDB naming rules . Message brokers and queues ​ Recommended! QuestDB supports several excellent message brokers, event streaming platforms and/or queues. Checkout our quick start guides for the following: Flink Kafka Redpanda Telegraf Easy CSV upload ​ Recommended! For GUI-driven CSV upload which leverages the built-in REST HTTP API , use the Import CSV tab in the Web Console : For all CSV import methods, including using the APIs directly, see the CSV Import Guide . PostgreSQL Wire Protocol ​ QuestDB also supports the PostgreSQL Wire Protocol (PGWire) . It offers most PostgreSQL keywords and functions, including parameterized queries and psql on the command line. While PGWire is supported, we r"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Introduction", "text": "DB also supports the PostgreSQL Wire Protocol (PGWire) . It offers most PostgreSQL keywords and functions, including parameterized queries and psql on the command line. While PGWire is supported, we recommend applying the first-party clients or other tools if possible. This is to take advantage of maximum performance and overcome limitations in the protocol. Create new data ​ No data yet? Just starting? No worries. We've got you covered. There are several quick scaffolding options: QuestDB demo instance : Hosted, fully loaded and ready to go. Quickly explore the Web Console and SQL syntax. Create my first data set guide : Create tables, use rnd_ functions and make your own data. Sample dataset repos : IoT, e-commerce, finance or git logs? Check them out! Quick start repos : Code-based quick starts that cover ingestion, querying and data visualization using common programming languages and use cases. Also, a cat in a tracksuit. Time series streaming analytics template : A handy template for near real-time analytics using open source technologies. Next step - queries ​ Depending on your infrastructure, it should now be apparent which ingestion method is worth pursuing. Of course, ing"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Introduction", "text": "for near real-time analytics using open source technologies. Next step - queries ​ Depending on your infrastructure, it should now be apparent which ingestion method is worth pursuing. Of course, ingestion (data-in) is only half the battle. Your next best step? Learn how to query and explore data-out from the Query & SQL Overview . It might also be a solid bet to review timestamp basics . Edit this page Previous Schema Design Next Configuration string First-party clients Message brokers and queues Easy CSV upload PostgreSQL Wire Protocol Create new data Next step - queries\n\nIngestion Reference Overview On this page Ingestion overview QuestDB makes top performance \"data-in\" easy. This guide will prepare you to get the most out of (and into!) QuestDB. Choose from first-party clients, apply message brokers, event streaming platforms, queues, and more. First-party clients ​ Recommended! Our first party clients are the fastest way to insert data, and they excel with high volume, high cardinality data streaming. To start quickly, select your language: C & C++ High-performance client for systems programming and embedded applications. Read more .NET Cross-platform client for building appl"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Introduction", "text": "nality data streaming. To start quickly, select your language: C & C++ High-performance client for systems programming and embedded applications. Read more .NET Cross-platform client for building applications with .NET technologies. Read more Go An open-source programming language supported by Google with built-in concurrency. Read more Java Platform-independent client for enterprise applications and Android development. Read more Node.js Node.js® is an open-source, cross-platform JavaScript runtime environment. Read more Python Python is a programming language that lets you work quickly and integrate systems more effectively. Read more Rust Systems programming language focused on safety, speed, and concurrency. Read more Our clients utitilize the InfluxDB Line Protocol (ILP) which is an insert-only protocol that bypasses SQL INSERT statements, thus achieving significantly higher throughput. It also provides some key benefits: Automatic table creation : No need to define your schema upfront. Concurrent schema changes : Seamlessly handle multiple data streams with on-the-fly schema modifications Optimized batching : Use strong defaults or curate the size of your batches Health check"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Introduction", "text": "front. Concurrent schema changes : Seamlessly handle multiple data streams with on-the-fly schema modifications Optimized batching : Use strong defaults or curate the size of your batches Health checks and feedback : Ensure your system's integrity with built-in health monitoring Automatic write retries : Reuse connections and retry after interruptions An example of \"data-in\" - via the line - appears as: trades,symbol=ETH-USD,side=sell price=2615.54,amount=0.00044 1646762637609765000\\n trades,symbol=BTC-USD,side=sell price=39269.98,amount=0.001 1646762637710419000\\n trades,symbol=ETH-USD,side=buy price=2615.4,amount=0.002 1646762637764098000\\n Once inside of QuestDB, it's yours to manipulate and query via extended SQL. Please note that table and column names must follow the QuestDB naming rules . Message brokers and queues ​ Recommended! QuestDB supports several excellent message brokers, event streaming platforms and/or queues. Checkout our quick start guides for the following: Flink Kafka Redpanda Telegraf Easy CSV upload ​ Recommended! For GUI-driven CSV upload which leverages the built-in REST HTTP API , use the Import CSV tab in the Web Console : For all CSV import methods, inc"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Introduction", "text": "afka Redpanda Telegraf Easy CSV upload ​ Recommended! For GUI-driven CSV upload which leverages the built-in REST HTTP API , use the Import CSV tab in the Web Console : For all CSV import methods, including using the APIs directly, see the CSV Import Guide . PostgreSQL Wire Protocol ​ QuestDB also supports the PostgreSQL Wire Protocol (PGWire) . It offers most PostgreSQL keywords and functions, including parameterized queries and psql on the command line. While PGWire is supported, we recommend applying the first-party clients or other tools if possible. This is to take advantage of maximum performance and overcome limitations in the protocol. Create new data ​ No data yet? Just starting? No worries. We've got you covered. There are several quick scaffolding options: QuestDB demo instance : Hosted, fully loaded and ready to go. Quickly explore the Web Console and SQL syntax. Create my first data set guide : Create tables, use rnd_ functions and make your own data. Sample dataset repos : IoT, e-commerce, finance or git logs? Check them out! Quick start repos : Code-based quick starts that cover ingestion, querying and data visualization using common programming languages and use cas"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Introduction", "text": ": IoT, e-commerce, finance or git logs? Check them out! Quick start repos : Code-based quick starts that cover ingestion, querying and data visualization using common programming languages and use cases. Also, a cat in a tracksuit. Time series streaming analytics template : A handy template for near real-time analytics using open source technologies. Next step - queries ​ Depending on your infrastructure, it should now be apparent which ingestion method is worth pursuing. Of course, ingestion (data-in) is only half the battle. Your next best step? Learn how to query and explore data-out from the Query & SQL Overview . It might also be a solid bet to review timestamp basics . Edit this page Previous Schema Design Next Configuration string\n\nIngestion Reference Overview On this page Ingestion overview QuestDB makes top performance \"data-in\" easy. This guide will prepare you to get the most out of (and into!) QuestDB. Choose from first-party clients, apply message brokers, event streaming platforms, queues, and more. First-party clients ​ Recommended! Our first party clients are the fastest way to insert data, and they excel with high volume, high cardinality data streaming. To start q"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Introduction", "text": "platforms, queues, and more. First-party clients ​ Recommended! Our first party clients are the fastest way to insert data, and they excel with high volume, high cardinality data streaming. To start quickly, select your language: C & C++ High-performance client for systems programming and embedded applications. Read more .NET Cross-platform client for building applications with .NET technologies. Read more Go An open-source programming language supported by Google with built-in concurrency. Read more Java Platform-independent client for enterprise applications and Android development. Read more Node.js Node.js® is an open-source, cross-platform JavaScript runtime environment. Read more Python Python is a programming language that lets you work quickly and integrate systems more effectively. Read more Rust Systems programming language focused on safety, speed, and concurrency. Read more Our clients utitilize the InfluxDB Line Protocol (ILP) which is an insert-only protocol that bypasses SQL INSERT statements, thus achieving significantly higher throughput. It also provides some key benefits: Automatic table creation : No need to define your schema upfront. Concurrent schema changes"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Introduction", "text": "SQL INSERT statements, thus achieving significantly higher throughput. It also provides some key benefits: Automatic table creation : No need to define your schema upfront. Concurrent schema changes : Seamlessly handle multiple data streams with on-the-fly schema modifications Optimized batching : Use strong defaults or curate the size of your batches Health checks and feedback : Ensure your system's integrity with built-in health monitoring Automatic write retries : Reuse connections and retry after interruptions An example of \"data-in\" - via the line - appears as: trades,symbol=ETH-USD,side=sell price=2615.54,amount=0.00044 1646762637609765000\\n trades,symbol=BTC-USD,side=sell price=39269.98,amount=0.001 1646762637710419000\\n trades,symbol=ETH-USD,side=buy price=2615.4,amount=0.002 1646762637764098000\\n Once inside of QuestDB, it's yours to manipulate and query via extended SQL. Please note that table and column names must follow the QuestDB naming rules . Message brokers and queues ​ Recommended! QuestDB supports several excellent message brokers, event streaming platforms and/or queues. Checkout our quick start guides for the following: Flink Kafka Redpanda Telegraf Easy CSV u"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Introduction", "text": "​ Recommended! QuestDB supports several excellent message brokers, event streaming platforms and/or queues. Checkout our quick start guides for the following: Flink Kafka Redpanda Telegraf Easy CSV upload ​ Recommended! For GUI-driven CSV upload which leverages the built-in REST HTTP API , use the Import CSV tab in the Web Console : For all CSV import methods, including using the APIs directly, see the CSV Import Guide . PostgreSQL Wire Protocol ​ QuestDB also supports the PostgreSQL Wire Protocol (PGWire) . It offers most PostgreSQL keywords and functions, including parameterized queries and psql on the command line. While PGWire is supported, we recommend applying the first-party clients or other tools if possible. This is to take advantage of maximum performance and overcome limitations in the protocol. Create new data ​ No data yet? Just starting? No worries. We've got you covered. There are several quick scaffolding options: QuestDB demo instance : Hosted, fully loaded and ready to go. Quickly explore the Web Console and SQL syntax. Create my first data set guide : Create tables, use rnd_ functions and make your own data. Sample dataset repos : IoT, e-commerce, finance or git"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Introduction", "text": "to go. Quickly explore the Web Console and SQL syntax. Create my first data set guide : Create tables, use rnd_ functions and make your own data. Sample dataset repos : IoT, e-commerce, finance or git logs? Check them out! Quick start repos : Code-based quick starts that cover ingestion, querying and data visualization using common programming languages and use cases. Also, a cat in a tracksuit. Time series streaming analytics template : A handy template for near real-time analytics using open source technologies. Next step - queries ​ Depending on your infrastructure, it should now be apparent which ingestion method is worth pursuing. Of course, ingestion (data-in) is only half the battle. Your next best step? Learn how to query and explore data-out from the Query & SQL Overview . It might also be a solid bet to review timestamp basics . Edit this page Previous Schema Design Next Configuration string\n\nIngestion Reference\n\nOverview\n\nOn this page\n\nIngestion overview QuestDB makes top performance \"data-in\" easy. This guide will prepare you to get the most out of (and into!) QuestDB. Choose from first-party clients, apply message brokers, event streaming platforms, queues, and more. F"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Introduction", "text": "rformance \"data-in\" easy. This guide will prepare you to get the most out of (and into!) QuestDB. Choose from first-party clients, apply message brokers, event streaming platforms, queues, and more. First-party clients ​ Recommended! Our first party clients are the fastest way to insert data, and they excel with high volume, high cardinality data streaming. To start quickly, select your language: C & C++ High-performance client for systems programming and embedded applications. Read more .NET Cross-platform client for building applications with .NET technologies. Read more Go An open-source programming language supported by Google with built-in concurrency. Read more Java Platform-independent client for enterprise applications and Android development. Read more Node.js Node.js® is an open-source, cross-platform JavaScript runtime environment. Read more Python Python is a programming language that lets you work quickly and integrate systems more effectively. Read more Rust Systems programming language focused on safety, speed, and concurrency. Read more Our clients utitilize the InfluxDB Line Protocol (ILP) which is an insert-only protocol that bypasses SQL INSERT statements, thus a"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Introduction", "text": "amming language focused on safety, speed, and concurrency. Read more Our clients utitilize the InfluxDB Line Protocol (ILP) which is an insert-only protocol that bypasses SQL INSERT statements, thus achieving significantly higher throughput. It also provides some key benefits: Automatic table creation : No need to define your schema upfront. Concurrent schema changes : Seamlessly handle multiple data streams with on-the-fly schema modifications Optimized batching : Use strong defaults or curate the size of your batches Health checks and feedback : Ensure your system's integrity with built-in health monitoring Automatic write retries : Reuse connections and retry after interruptions An example of \"data-in\" - via the line - appears as: trades,symbol=ETH-USD,side=sell price=2615.54,amount=0.00044 1646762637609765000\\n trades,symbol=BTC-USD,side=sell price=39269.98,amount=0.001 1646762637710419000\\n trades,symbol=ETH-USD,side=buy price=2615.4,amount=0.002 1646762637764098000\\n Once inside of QuestDB, it's yours to manipulate and query via extended SQL. Please note that table and column names must follow the QuestDB naming rules . Message brokers and queues ​ Recommended! QuestDB suppor"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Introduction", "text": "of QuestDB, it's yours to manipulate and query via extended SQL. Please note that table and column names must follow the QuestDB naming rules . Message brokers and queues ​ Recommended! QuestDB supports several excellent message brokers, event streaming platforms and/or queues. Checkout our quick start guides for the following: Flink Kafka Redpanda Telegraf Easy CSV upload ​ Recommended! For GUI-driven CSV upload which leverages the built-in REST HTTP API , use the Import CSV tab in the Web Console : For all CSV import methods, including using the APIs directly, see the CSV Import Guide . PostgreSQL Wire Protocol ​ QuestDB also supports the PostgreSQL Wire Protocol (PGWire) . It offers most PostgreSQL keywords and functions, including parameterized queries and psql on the command line. While PGWire is supported, we recommend applying the first-party clients or other tools if possible. This is to take advantage of maximum performance and overcome limitations in the protocol. Create new data ​ No data yet? Just starting? No worries. We've got you covered. There are several quick scaffolding options: QuestDB demo instance : Hosted, fully loaded and ready to go. Quickly explore the Web"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Introduction", "text": "data ​ No data yet? Just starting? No worries. We've got you covered. There are several quick scaffolding options: QuestDB demo instance : Hosted, fully loaded and ready to go. Quickly explore the Web Console and SQL syntax. Create my first data set guide : Create tables, use rnd_ functions and make your own data. Sample dataset repos : IoT, e-commerce, finance or git logs? Check them out! Quick start repos : Code-based quick starts that cover ingestion, querying and data visualization using common programming languages and use cases. Also, a cat in a tracksuit. Time series streaming analytics template : A handy template for near real-time analytics using open source technologies. Next step - queries ​ Depending on your infrastructure, it should now be apparent which ingestion method is worth pursuing. Of course, ingestion (data-in) is only half the battle. Your next best step? Learn how to query and explore data-out from the Query & SQL Overview . It might also be a solid bet to review timestamp basics ."}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Ingestion overview", "text": "QuestDB makes top performance \"data-in\" easy.\n\nThis guide will prepare you to get the most out of (and into!) QuestDB.\n\nChoose from first-party clients, apply message brokers, event streaming platforms, queues, and more."}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "First-party clients ​", "text": "Recommended!\n\nOur first party clients are the fastest way to insert data, and they excel with high volume, high cardinality data streaming.\n\nTo start quickly, select your language:\n\nC & C++ High-performance client for systems programming and embedded applications. Read more .NET Cross-platform client for building applications with .NET technologies. Read more Go An open-source programming language supported by Google with built-in concurrency. Read more Java Platform-independent client for enterprise applications and Android development. Read more Node.js Node.js® is an open-source, cross-platform JavaScript runtime environment. Read more Python Python is a programming language that lets you work quickly and integrate systems more effectively. Read more Rust Systems programming language focused on safety, speed, and concurrency. Read more\n\nC & C++ High-performance client for systems programming and embedded applications. Read more .NET Cross-platform client for building applications with .NET technologies. Read more Go An open-source programming language supported by Google with built-in concurrency. Read more Java Platform-independent client for enterprise applications and Android"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "First-party clients ​", "text": "th .NET technologies. Read more Go An open-source programming language supported by Google with built-in concurrency. Read more Java Platform-independent client for enterprise applications and Android development. Read more Node.js Node.js® is an open-source, cross-platform JavaScript runtime environment. Read more Python Python is a programming language that lets you work quickly and integrate systems more effectively. Read more Rust Systems programming language focused on safety, speed, and concurrency. Read more\n\nC & C++ High-performance client for systems programming and embedded applications. Read more\n\nC & C++ High-performance client for systems programming and embedded applications. Read more"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "C & C++", "text": "High-performance client for systems programming and embedded applications.\n\nRead more\n\n.NET Cross-platform client for building applications with .NET technologies. Read more\n\n.NET Cross-platform client for building applications with .NET technologies. Read more"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": ".NET", "text": "Cross-platform client for building applications with .NET technologies.\n\nRead more\n\nGo An open-source programming language supported by Google with built-in concurrency. Read more\n\nGo An open-source programming language supported by Google with built-in concurrency. Read more"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Go", "text": "An open-source programming language supported by Google with built-in concurrency.\n\nRead more\n\nJava Platform-independent client for enterprise applications and Android development. Read more\n\nJava Platform-independent client for enterprise applications and Android development. Read more"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Java", "text": "Platform-independent client for enterprise applications and Android development.\n\nRead more\n\nNode.js Node.js® is an open-source, cross-platform JavaScript runtime environment. Read more\n\nNode.js Node.js® is an open-source, cross-platform JavaScript runtime environment. Read more"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Node.js", "text": "Node.js® is an open-source, cross-platform JavaScript runtime environment.\n\nRead more\n\nPython Python is a programming language that lets you work quickly and integrate systems more effectively. Read more\n\nPython Python is a programming language that lets you work quickly and integrate systems more effectively. Read more"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Python", "text": "Python is a programming language that lets you work quickly and integrate systems more effectively.\n\nRead more\n\nRust Systems programming language focused on safety, speed, and concurrency. Read more\n\nRust Systems programming language focused on safety, speed, and concurrency. Read more"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Rust", "text": "Systems programming language focused on safety, speed, and concurrency.\n\nRead more\n\nOur clients utitilize the InfluxDB Line Protocol (ILP) which is an insert-only protocol that bypasses SQL INSERT statements, thus achieving significantly higher throughput. It also provides some key benefits:\n\nINSERT\n\nAutomatic table creation : No need to define your schema upfront.\n\nConcurrent schema changes : Seamlessly handle multiple data streams with on-the-fly schema modifications\n\nOptimized batching : Use strong defaults or curate the size of your batches\n\nHealth checks and feedback : Ensure your system's integrity with built-in health monitoring\n\nAutomatic write retries : Reuse connections and retry after interruptions\n\nAn example of \"data-in\" - via the line - appears as:\n\ntrades,symbol=ETH-USD,side=sell price=2615.54,amount=0.00044 1646762637609765000\\n trades,symbol=BTC-USD,side=sell price=39269.98,amount=0.001 1646762637710419000\\n trades,symbol=ETH-USD,side=buy price=2615.4,amount=0.002 1646762637764098000\\n\n\ntrades,symbol=ETH-USD,side=sell price=2615.54,amount=0.00044 1646762637609765000\\n trades,symbol=BTC-USD,side=sell price=39269.98,amount=0.001 1646762637710419000\\n trades,symbol=ET"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Rust", "text": "762637764098000\\n\n\ntrades,symbol=ETH-USD,side=sell price=2615.54,amount=0.00044 1646762637609765000\\n trades,symbol=BTC-USD,side=sell price=39269.98,amount=0.001 1646762637710419000\\n trades,symbol=ETH-USD,side=buy price=2615.4,amount=0.002 1646762637764098000\\n\n\ntrades,symbol=ETH-USD,side=sell price=2615.54,amount=0.00044 1646762637609765000\\n trades,symbol=BTC-USD,side=sell price=39269.98,amount=0.001 1646762637710419000\\n trades,symbol=ETH-USD,side=buy price=2615.4,amount=0.002 1646762637764098000\\n\n\ntrades,symbol=ETH-USD,side=sell price=2615.54,amount=0.00044 1646762637609765000\\n trades,symbol=BTC-USD,side=sell price=39269.98,amount=0.001 1646762637710419000\\n trades,symbol=ETH-USD,side=buy price=2615.4,amount=0.002 1646762637764098000\\n\n\nOnce inside of QuestDB, it's yours to manipulate and query via extended SQL. Please note that table and column names must follow the QuestDB naming rules ."}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Message brokers and queues ​", "text": "Recommended!\n\nQuestDB supports several excellent message brokers, event streaming platforms and/or queues.\n\nCheckout our quick start guides for the following:\n\nFlink\n\nKafka\n\nRedpanda\n\nTelegraf"}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Easy CSV upload ​", "text": "Recommended!\n\nFor GUI-driven CSV upload which leverages the built-in REST HTTP API , use the Import CSV tab in the Web Console :\n\nFor all CSV import methods, including using the APIs directly, see the CSV Import Guide ."}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "PostgreSQL Wire Protocol ​", "text": "QuestDB also supports the PostgreSQL Wire Protocol (PGWire) .\n\nIt offers most PostgreSQL keywords and functions, including parameterized queries and psql on the command line.\n\npsql\n\nWhile PGWire is supported, we recommend applying the first-party clients or other tools if possible.\n\nThis is to take advantage of maximum performance and overcome limitations in the protocol."}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Create new data ​", "text": "No data yet? Just starting? No worries. We've got you covered.\n\nThere are several quick scaffolding options:\n\nQuestDB demo instance : Hosted, fully loaded and ready to go. Quickly explore the Web Console and SQL syntax.\n\nCreate my first data set guide : Create tables, use rnd_ functions and make your own data.\n\nrnd_\n\nSample dataset repos : IoT, e-commerce, finance or git logs? Check them out!\n\nQuick start repos : Code-based quick starts that cover ingestion, querying and data visualization using common programming languages and use cases. Also, a cat in a tracksuit.\n\nTime series streaming analytics template : A handy template for near real-time analytics using open source technologies."}
{"source_url": "https://questdb.com/docs/ingestion-overview", "title": "Ingestion overview", "section": "Next step - queries ​", "text": "Depending on your infrastructure, it should now be apparent which ingestion method is worth pursuing.\n\nOf course, ingestion (data-in) is only half the battle.\n\nYour next best step? Learn how to query and explore data-out from the Query & SQL Overview .\n\nIt might also be a solid bet to review timestamp basics .\n\nEdit this page\n\nEdit this page\n\nPrevious\n\nSchema Design\n\nNext\n\nConfiguration string\n\nFirst-party clients Message brokers and queues Easy CSV upload PostgreSQL Wire Protocol Create new data Next step - queries\n\nFirst-party clients Message brokers and queues Easy CSV upload PostgreSQL Wire Protocol Create new data Next step - queries\n\nFirst-party clients\n\nMessage brokers and queues\n\nEasy CSV upload\n\nPostgreSQL Wire Protocol\n\nCreate new data\n\nNext step - queries"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "Query & SQL Reference Query & SQL Overview On this page Query & SQL Overview Querying - as a base action - is performed in three primary ways: Query via the QuestDB Web Console Query via PostgreSQL Query via REST HTTP API Query via Apache Parquet For efficient and clear querying, QuestDB provides SQL with enhanced time series extensions. This makes analyzing, downsampling, processing and reading time series data an intuitive and flexible experience. Queries can be written into many applications using existing drivers and clients of the PostgreSQL or REST-ful ecosystems. However, querying is also leveraged heavily by third-party tools to provide visualizations, such as within Grafana , or for connectivity into broad data infrastructure and application environments such as with a tool like Cube . Need to ingest data first? Checkout our Ingestion overview . QuestDB Web Console ​ The Web Console is available by default at localhost:9000 . The GUI makes it easy to write, return and chart queries. There is autocomplete, syntax highlighting, errors, and more. If you want to test a query or interact direclty with your data in the cleanest and simplest way, apply queries via the Web Console"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "eries. There is autocomplete, syntax highlighting, errors, and more. If you want to test a query or interact direclty with your data in the cleanest and simplest way, apply queries via the Web Console . Click to zoom For an example, click Demo this query in the below snippet. This will run a query within our public demo instance and Web Console : Navigate time with SQL Demo this query SELECT timestamp, symbol, first(price) AS open, last(price) AS close, min(price), max(price), sum(amount) AS volume FROM trades WHERE timestamp > dateadd('d', -1, now()) SAMPLE BY 15m; If you see Demo this query on other snippets in this docs, they can be run against the demo instance. PostgreSQL ​ Query QuestDB using the PostgreSQL endpoint via the default port 8812 . See PGWire Client overview for details on how to connect to QuestDB using PostgreSQL clients. Brief examples in multiple languages are shown below. Python Java NodeJS Go C# C Ruby PHP import psycopg as pg import time # Connect to an existing QuestDB instance conn_str = 'user=admin password=quest host=127.0.0.1 port=8812 dbname=qdb' with pg . connect ( conn_str , autocommit = True ) as connection : # Open a cursor to perform database ope"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "g QuestDB instance conn_str = 'user=admin password=quest host=127.0.0.1 port=8812 dbname=qdb' with pg . connect ( conn_str , autocommit = True ) as connection : # Open a cursor to perform database operations with connection . cursor ( ) as cur : #Query the database and obtain data as Python objects. cur . execute ( 'SELECT * FROM trades_pg;' ) records = cur . fetchall ( ) for row in records : print ( row ) # the connection is now closed package com.myco; import java.sql.*; import java.util.Properties; public class App { public static void main(String[] args) throws SQLException { Properties properties = new Properties(); properties.setProperty(\"user\", \"admin\"); properties.setProperty(\"password\", \"quest\"); properties.setProperty(\"sslmode\", \"disable\"); final Connection connection = DriverManager.getConnection( \"jdbc:postgresql://localhost:8812/qdb\", properties); try (PreparedStatement preparedStatement = connection.prepareStatement( \"SELECT x FROM long_sequence(5);\")) { try (ResultSet rs = preparedStatement.executeQuery()) { while (rs.next()) { System.out.println(rs.getLong(1)); } } } connection.close(); } } \"use strict\" const { Client } = require ( \"pg\" ) const start = async ( ) =>"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "= preparedStatement.executeQuery()) { while (rs.next()) { System.out.println(rs.getLong(1)); } } } connection.close(); } } \"use strict\" const { Client } = require ( \"pg\" ) const start = async ( ) => { const client = new Client ( { database : \"qdb\" , host : \"127.0.0.1\" , password : \"quest\" , port : 8812 , user : \"admin\" , } ) await client . connect ( ) const res = await client . query ( \"SELECT x FROM long_sequence(5);\" ) console . log ( res . rows ) await client . end ( ) } start ( ) . catch ( console . error ) package main import ( \"database/sql\" \"fmt\" _ \"github.com/lib/pq\" ) const ( host = \"localhost\" port = 8812 user = \"admin\" password = \"quest\" dbname = \"qdb\" ) func main ( ) { connStr := fmt . Sprintf ( \"host=%s port=%d user=%s password=%s dbname=%s sslmode=disable\" , host , port , user , password , dbname ) db , err := sql . Open ( \"postgres\" , connStr ) checkErr ( err ) defer db . Close ( ) stmt , err := db . Prepare ( \"SELECT x FROM long_sequence(5);\" ) checkErr ( err ) defer stmt . Close ( ) rows , err := stmt . Query ( ) checkErr ( err ) defer rows . Close ( ) var num string for rows . Next ( ) { err = rows . Scan ( & num ) checkErr ( err ) fmt . Println ( num ) } err = r"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "tmt . Close ( ) rows , err := stmt . Query ( ) checkErr ( err ) defer rows . Close ( ) var num string for rows . Next ( ) { err = rows . Scan ( & num ) checkErr ( err ) fmt . Println ( num ) } err = rows . Err ( ) checkErr ( err ) } func checkErr ( err error ) { if err != nil { panic ( err ) } } // compile with // g++ libpq_example.c -o libpq_example.exe -I pgsql\\include -L dev\\pgsql\\lib // -std=c++17 -lpthread -lpq # include <libpq-fe.h> # include <stdio.h> # include <stdlib.h> void do_exit ( PGconn * conn ) { PQfinish ( conn ) ; exit ( 1 ) ; } int main ( ) { PGconn * conn = PQconnectdb ( \"host=localhost user=admin password=quest port=8812 dbname=testdb\" ) ; if ( PQstatus ( conn ) == CONNECTION_BAD ) { fprintf ( stderr , \"Connection to database failed: %s\\n\" , PQerrorMessage ( conn ) ) ; do_exit ( conn ) ; } PGresult * res = PQexec ( conn , \"SELECT x FROM long_sequence(5);\" ) ; if ( PQresultStatus ( res ) != PGRES_TUPLES_OK ) { printf ( \"No data retrieved\\n\" ) ; PQclear ( res ) ; do_exit ( conn ) ; } int rows = PQntuples ( res ) ; for ( int i = 0 ; i < rows ; i ++ ) { printf ( \"%s\\n\" , PQgetvalue ( res , i , 0 ) ) ; } PQclear ( res ) ; PQfinish ( conn ) ; return 0 ; } using Npgsql"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "xit ( conn ) ; } int rows = PQntuples ( res ) ; for ( int i = 0 ; i < rows ; i ++ ) { printf ( \"%s\\n\" , PQgetvalue ( res , i , 0 ) ) ; } PQclear ( res ) ; PQfinish ( conn ) ; return 0 ; } using Npgsql; string username = \"admin\"; string password = \"quest\"; string database = \"qdb\"; int port = 8812; var connectionString = $@\"host=localhost;port={port};username={username};password={password}; database={database};ServerCompatibilityMode=NoTypeLoading;\"; await using NpgsqlConnection connection = new NpgsqlConnection(connectionString); await connection.OpenAsync(); var sql = \"SELECT x FROM long_sequence(5);\"; await using NpgsqlCommand command = new NpgsqlCommand(sql, connection); await using (var reader = await command.ExecuteReaderAsync()) { while (await reader.ReadAsync()) { var x = reader.GetInt64(0); } } require 'pg' begin conn =PG.connect( host: \"127.0.0.1\", port: 8812, dbname: 'qdb', user: 'admin', password: 'quest' ) rows = conn.exec 'SELECT x FROM long_sequence(5);' rows.each do |row| puts row end rescue PG::Error => e puts e.message ensure conn.close if conn end <?php function exceptions_error_handler($severity, $message, $filename, $lineno) { throw new ErrorException($message, 0"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "puts row end rescue PG::Error => e puts e.message ensure conn.close if conn end <?php function exceptions_error_handler($severity, $message, $filename, $lineno) { throw new ErrorException($message, 0, $severity, $filename, $lineno); } set_error_handler('exceptions_error_handler'); $db_conn = null; try { $db_conn = pg_connect(\" host = 'localhost' port=8812 dbname = 'qdb' user = 'admin' password = 'quest' \"); $result = pg_query($db_conn, 'SELECT x FROM long_sequence(5);' ); while ($row = pg_fetch_assoc($result) ){ print_r($row); } pg_free_result($result); } catch (Exception $e) { echo 'Caught exception: ', $e->getMessage(), \"\\n\"; } finally { if (!is_null($db_conn)) { pg_close($db_conn); } } ?> Further Reading ​ See the PGWire Client overview for more details on how to use PostgreSQL clients to connect to QuestDB. REST HTTP API ​ QuestDB exposes a REST API for compatibility with a wide range of libraries and tools. The REST API is accessible on port 9000 and has the following query-capable entrypoints: For details such as content type, query parameters and more, refer to the REST HTTP API reference. Entrypoint HTTP Method Description REST HTTP API Reference /exp?query=.. GET Export S"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "le entrypoints: For details such as content type, query parameters and more, refer to the REST HTTP API reference. Entrypoint HTTP Method Description REST HTTP API Reference /exp?query=.. GET Export SQL Query as CSV Reference /exec?query=.. GET Run SQL Query returning JSON result set Reference /exp : SQL Query to CSV ​ The /exp entrypoint allows querying the database with a SQL select query and obtaining the results as CSV. For obtaining results in JSON, use /exec instead, documented next. cURL Python curl -G --data-urlencode \\ \"query=SELECT * FROM example_table2 LIMIT 3\" \\ http://localhost:9000/exp \"col1\",\"col2\",\"col3\" \"a\",10.5,true \"b\",100.0,false \"c\",,true import requests resp = requests . get ( 'http://localhost:9000/exp' , { 'query' : 'SELECT * FROM example_table2' , 'limit' : '3,6' # Rows 3, 4, 5 } ) print ( resp . text ) \"col1\",\"col2\",\"col3\" \"d\",20.5,true \"e\",200.0,false \"f\",,true /exec : SQL Query to JSON ​ The /exec entrypoint takes a SQL query and returns results as JSON. This is similar to the /exp entry point which returns results as CSV. Querying Data ​ cURL Python NodeJS Go curl -G \\ --data-urlencode \"query=SELECT x FROM long_sequence(5);\" \\ http://localhost:9000/exec"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "s is similar to the /exp entry point which returns results as CSV. Querying Data ​ cURL Python NodeJS Go curl -G \\ --data-urlencode \"query=SELECT x FROM long_sequence(5);\" \\ http://localhost:9000/exec The JSON response contains the original query, a \"columns\" key with the schema of the results, a \"count\" number of rows and a \"dataset\" with the results. { \"query\" : \"SELECT x FROM long_sequence(5);\" , \"columns\" : [ { \"name\" : \"x\" , \"type\" : \"LONG\" } ] , \"dataset\" : [ [ 1 ] , [ 2 ] , [ 3 ] , [ 4 ] , [ 5 ] ] , \"count\" : 5 } import sys import requests host = 'http://localhost:9000' sql_query = \"select * from long_sequence(10)\" try : response = requests . get ( host + '/exec' , params = { 'query' : sql_query } ) . json ( ) for row in response [ 'dataset' ] : print ( row [ 0 ] ) except requests . exceptions . RequestException as e : print ( f'Error: { e } ' , file = sys . stderr ) const fetch = require ( \"node-fetch\" ) const HOST = \"http://localhost:9000\" async function run ( ) { try { const query = \"SELECT x FROM long_sequence(5);\" const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) const json = await response . json ( ) console . log ( json )"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "query = \"SELECT x FROM long_sequence(5);\" const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) const json = await response . json ( ) console . log ( json ) } catch ( error ) { console . log ( error ) } } run ( ) package main import ( \"fmt\" \"io/ioutil\" \"log\" \"net/http\" \"net/url\" ) func main ( ) { u , err := url . Parse ( \"http://localhost:9000\" ) checkErr ( err ) u . Path += \"exec\" params := url . Values { } params . Add ( \"query\" , \"SELECT x FROM long_sequence(5);\" ) u . RawQuery = params . Encode ( ) url := fmt . Sprintf ( \"%v\" , u ) res , err := http . Get ( url ) checkErr ( err ) defer res . Body . Close ( ) body , err := ioutil . ReadAll ( res . Body ) checkErr ( err ) log . Println ( string ( body ) ) } func checkErr ( err error ) { if err != nil { panic ( err ) } } Alternatively, the /exec endpoint can be used to create a table and the INSERT statement can be used to populate it with values: cURL NodeJS Python # Create Table curl -G \\ --data-urlencode \"query=CREATE TABLE IF NOT EXISTS trades(name VARCHAR, value INT)\" \\ http://localhost:9000/exec # Insert a row curl -G \\ --data-urlencode \"query=INSERT INTO trades VALUES('abc', 123456"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "-data-urlencode \"query=CREATE TABLE IF NOT EXISTS trades(name VARCHAR, value INT)\" \\ http://localhost:9000/exec # Insert a row curl -G \\ --data-urlencode \"query=INSERT INTO trades VALUES('abc', 123456)\" \\ http://localhost:9000/exec # Update a row curl -G \\ --data-urlencode \"query=UPDATE trades SET value = 9876 WHERE name = 'abc'\" \\ http://localhost:9000/exec The node-fetch package can be installed using npm i node-fetch . const fetch = require ( \"node-fetch\" ) ; const HOST = \"http://localhost:9000\" ; async function createTable ( ) { try { const query = \"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value INT)\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } async function insertData ( ) { try { const query = \"INSERT INTO trades VALUES('abc', 123456)\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } async function updateData ( ) { try { const query ="}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "RIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } async function updateData ( ) { try { const query = \"UPDATE trades SET value = 9876 WHERE name = 'abc'\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } createTable ( ) . then ( insertData ) . then ( updateData ) ; import requests import json host = 'http://localhost:9000' def run_query ( sql_query ) : query_params = { 'query' : sql_query , 'fmt' : 'json' } try : response = requests . get ( host + '/exec' , params = query_params ) json_response = json . loads ( response . text ) print ( json_response ) except requests . exceptions . RequestException as e : print ( \"Error: %s\" % ( e ) ) # create table run_query ( \"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value INT)\" ) # insert row run_query ( \"INSERT INTO trades VALUES('abc', 123456)\" ) # update row run_query ( \"UPDATE trades SET value = 9876 WHERE name = 'abc'\" ) Apache Parquet ​ info Apache Parquet support is in b"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "insert row run_query ( \"INSERT INTO trades VALUES('abc', 123456)\" ) # update row run_query ( \"UPDATE trades SET value = 9876 WHERE name = 'abc'\" ) Apache Parquet ​ info Apache Parquet support is in beta . It may not be fit for production use. Please let us know if you run into issues. Either: Email us at support@questdb.io Join our public Slack Post on our Discourse community Parquet files can be read and thus queried by QuestDB. QuestDB is shipped with a demo Parquet file, trades.parquet , which can be queried using the read_parquet function. Example: read_parquet example SELECT * FROM read_parquet('trades.parquet') WHERE side = 'buy'; The trades.parquet file is located in the import subdirectory inside the QuestDB root directory. Drop your own Parquet files to the import directory and query them using the read_parquet() function. You can change the allowed directory by setting the cairo.sql.copy.root configuration key. For more information, see the Parquet documentation . What's next? ​ Now... SQL! It's query time. Whether you want to use the Web Console , PostgreSQL or REST HTTP (or both), query construction is rich. To brush up and learn what's unique in QuestDB, consider the"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "t? ​ Now... SQL! It's query time. Whether you want to use the Web Console , PostgreSQL or REST HTTP (or both), query construction is rich. To brush up and learn what's unique in QuestDB, consider the following: Data types SQL execution order And to learn about some of our favourite, most powerful syntax: Window functions are a powerful analysis tool Aggregate functions - aggregations are key! Date & time operators to learn about date and time SAMPLE BY to summarize data into chunks based on a specified time interval, from a year to a microsecond WHERE IN to compress time ranges into concise intervals LATEST ON for latest values within multiple series within a table ASOF JOIN to associate timestamps between a series based on proximity; no extra indices required Materialized Views to pre-compute complex queries for optimal performance Looking for visuals? Explore Grafana Jump quickly into the Web Console Edit this page Previous REST HTTP API Next Overview QuestDB Web Console PostgreSQL REST HTTP API Apache Parquet What's next?\n\nQuery & SQL Reference Query & SQL Overview On this page Query & SQL Overview Querying - as a base action - is performed in three primary ways: Query via the Q"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "EST HTTP API Apache Parquet What's next?\n\nQuery & SQL Reference Query & SQL Overview On this page Query & SQL Overview Querying - as a base action - is performed in three primary ways: Query via the QuestDB Web Console Query via PostgreSQL Query via REST HTTP API Query via Apache Parquet For efficient and clear querying, QuestDB provides SQL with enhanced time series extensions. This makes analyzing, downsampling, processing and reading time series data an intuitive and flexible experience. Queries can be written into many applications using existing drivers and clients of the PostgreSQL or REST-ful ecosystems. However, querying is also leveraged heavily by third-party tools to provide visualizations, such as within Grafana , or for connectivity into broad data infrastructure and application environments such as with a tool like Cube . Need to ingest data first? Checkout our Ingestion overview . QuestDB Web Console ​ The Web Console is available by default at localhost:9000 . The GUI makes it easy to write, return and chart queries. There is autocomplete, syntax highlighting, errors, and more. If you want to test a query or interact direclty with your data in the cleanest and simpl"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "akes it easy to write, return and chart queries. There is autocomplete, syntax highlighting, errors, and more. If you want to test a query or interact direclty with your data in the cleanest and simplest way, apply queries via the Web Console . Click to zoom For an example, click Demo this query in the below snippet. This will run a query within our public demo instance and Web Console : Navigate time with SQL Demo this query SELECT timestamp, symbol, first(price) AS open, last(price) AS close, min(price), max(price), sum(amount) AS volume FROM trades WHERE timestamp > dateadd('d', -1, now()) SAMPLE BY 15m; If you see Demo this query on other snippets in this docs, they can be run against the demo instance. PostgreSQL ​ Query QuestDB using the PostgreSQL endpoint via the default port 8812 . See PGWire Client overview for details on how to connect to QuestDB using PostgreSQL clients. Brief examples in multiple languages are shown below. Python Java NodeJS Go C# C Ruby PHP import psycopg as pg import time # Connect to an existing QuestDB instance conn_str = 'user=admin password=quest host=127.0.0.1 port=8812 dbname=qdb' with pg . connect ( conn_str , autocommit = True ) as connection"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "as pg import time # Connect to an existing QuestDB instance conn_str = 'user=admin password=quest host=127.0.0.1 port=8812 dbname=qdb' with pg . connect ( conn_str , autocommit = True ) as connection : # Open a cursor to perform database operations with connection . cursor ( ) as cur : #Query the database and obtain data as Python objects. cur . execute ( 'SELECT * FROM trades_pg;' ) records = cur . fetchall ( ) for row in records : print ( row ) # the connection is now closed package com.myco; import java.sql.*; import java.util.Properties; public class App { public static void main(String[] args) throws SQLException { Properties properties = new Properties(); properties.setProperty(\"user\", \"admin\"); properties.setProperty(\"password\", \"quest\"); properties.setProperty(\"sslmode\", \"disable\"); final Connection connection = DriverManager.getConnection( \"jdbc:postgresql://localhost:8812/qdb\", properties); try (PreparedStatement preparedStatement = connection.prepareStatement( \"SELECT x FROM long_sequence(5);\")) { try (ResultSet rs = preparedStatement.executeQuery()) { while (rs.next()) { System.out.println(rs.getLong(1)); } } } connection.close(); } } \"use strict\" const { Client } = re"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "M long_sequence(5);\")) { try (ResultSet rs = preparedStatement.executeQuery()) { while (rs.next()) { System.out.println(rs.getLong(1)); } } } connection.close(); } } \"use strict\" const { Client } = require ( \"pg\" ) const start = async ( ) => { const client = new Client ( { database : \"qdb\" , host : \"127.0.0.1\" , password : \"quest\" , port : 8812 , user : \"admin\" , } ) await client . connect ( ) const res = await client . query ( \"SELECT x FROM long_sequence(5);\" ) console . log ( res . rows ) await client . end ( ) } start ( ) . catch ( console . error ) package main import ( \"database/sql\" \"fmt\" _ \"github.com/lib/pq\" ) const ( host = \"localhost\" port = 8812 user = \"admin\" password = \"quest\" dbname = \"qdb\" ) func main ( ) { connStr := fmt . Sprintf ( \"host=%s port=%d user=%s password=%s dbname=%s sslmode=disable\" , host , port , user , password , dbname ) db , err := sql . Open ( \"postgres\" , connStr ) checkErr ( err ) defer db . Close ( ) stmt , err := db . Prepare ( \"SELECT x FROM long_sequence(5);\" ) checkErr ( err ) defer stmt . Close ( ) rows , err := stmt . Query ( ) checkErr ( err ) defer rows . Close ( ) var num string for rows . Next ( ) { err = rows . Scan ( & num ) checkE"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "g_sequence(5);\" ) checkErr ( err ) defer stmt . Close ( ) rows , err := stmt . Query ( ) checkErr ( err ) defer rows . Close ( ) var num string for rows . Next ( ) { err = rows . Scan ( & num ) checkErr ( err ) fmt . Println ( num ) } err = rows . Err ( ) checkErr ( err ) } func checkErr ( err error ) { if err != nil { panic ( err ) } } // compile with // g++ libpq_example.c -o libpq_example.exe -I pgsql\\include -L dev\\pgsql\\lib // -std=c++17 -lpthread -lpq # include <libpq-fe.h> # include <stdio.h> # include <stdlib.h> void do_exit ( PGconn * conn ) { PQfinish ( conn ) ; exit ( 1 ) ; } int main ( ) { PGconn * conn = PQconnectdb ( \"host=localhost user=admin password=quest port=8812 dbname=testdb\" ) ; if ( PQstatus ( conn ) == CONNECTION_BAD ) { fprintf ( stderr , \"Connection to database failed: %s\\n\" , PQerrorMessage ( conn ) ) ; do_exit ( conn ) ; } PGresult * res = PQexec ( conn , \"SELECT x FROM long_sequence(5);\" ) ; if ( PQresultStatus ( res ) != PGRES_TUPLES_OK ) { printf ( \"No data retrieved\\n\" ) ; PQclear ( res ) ; do_exit ( conn ) ; } int rows = PQntuples ( res ) ; for ( int i = 0 ; i < rows ; i ++ ) { printf ( \"%s\\n\" , PQgetvalue ( res , i , 0 ) ) ; } PQclear ( res ) ; PQf"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "ta retrieved\\n\" ) ; PQclear ( res ) ; do_exit ( conn ) ; } int rows = PQntuples ( res ) ; for ( int i = 0 ; i < rows ; i ++ ) { printf ( \"%s\\n\" , PQgetvalue ( res , i , 0 ) ) ; } PQclear ( res ) ; PQfinish ( conn ) ; return 0 ; } using Npgsql; string username = \"admin\"; string password = \"quest\"; string database = \"qdb\"; int port = 8812; var connectionString = $@\"host=localhost;port={port};username={username};password={password}; database={database};ServerCompatibilityMode=NoTypeLoading;\"; await using NpgsqlConnection connection = new NpgsqlConnection(connectionString); await connection.OpenAsync(); var sql = \"SELECT x FROM long_sequence(5);\"; await using NpgsqlCommand command = new NpgsqlCommand(sql, connection); await using (var reader = await command.ExecuteReaderAsync()) { while (await reader.ReadAsync()) { var x = reader.GetInt64(0); } } require 'pg' begin conn =PG.connect( host: \"127.0.0.1\", port: 8812, dbname: 'qdb', user: 'admin', password: 'quest' ) rows = conn.exec 'SELECT x FROM long_sequence(5);' rows.each do |row| puts row end rescue PG::Error => e puts e.message ensure conn.close if conn end <?php function exceptions_error_handler($severity, $message, $filename, $line"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "FROM long_sequence(5);' rows.each do |row| puts row end rescue PG::Error => e puts e.message ensure conn.close if conn end <?php function exceptions_error_handler($severity, $message, $filename, $lineno) { throw new ErrorException($message, 0, $severity, $filename, $lineno); } set_error_handler('exceptions_error_handler'); $db_conn = null; try { $db_conn = pg_connect(\" host = 'localhost' port=8812 dbname = 'qdb' user = 'admin' password = 'quest' \"); $result = pg_query($db_conn, 'SELECT x FROM long_sequence(5);' ); while ($row = pg_fetch_assoc($result) ){ print_r($row); } pg_free_result($result); } catch (Exception $e) { echo 'Caught exception: ', $e->getMessage(), \"\\n\"; } finally { if (!is_null($db_conn)) { pg_close($db_conn); } } ?> Further Reading ​ See the PGWire Client overview for more details on how to use PostgreSQL clients to connect to QuestDB. REST HTTP API ​ QuestDB exposes a REST API for compatibility with a wide range of libraries and tools. The REST API is accessible on port 9000 and has the following query-capable entrypoints: For details such as content type, query parameters and more, refer to the REST HTTP API reference. Entrypoint HTTP Method Description REST HTT"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "ort 9000 and has the following query-capable entrypoints: For details such as content type, query parameters and more, refer to the REST HTTP API reference. Entrypoint HTTP Method Description REST HTTP API Reference /exp?query=.. GET Export SQL Query as CSV Reference /exec?query=.. GET Run SQL Query returning JSON result set Reference /exp : SQL Query to CSV ​ The /exp entrypoint allows querying the database with a SQL select query and obtaining the results as CSV. For obtaining results in JSON, use /exec instead, documented next. cURL Python curl -G --data-urlencode \\ \"query=SELECT * FROM example_table2 LIMIT 3\" \\ http://localhost:9000/exp \"col1\",\"col2\",\"col3\" \"a\",10.5,true \"b\",100.0,false \"c\",,true import requests resp = requests . get ( 'http://localhost:9000/exp' , { 'query' : 'SELECT * FROM example_table2' , 'limit' : '3,6' # Rows 3, 4, 5 } ) print ( resp . text ) \"col1\",\"col2\",\"col3\" \"d\",20.5,true \"e\",200.0,false \"f\",,true /exec : SQL Query to JSON ​ The /exec entrypoint takes a SQL query and returns results as JSON. This is similar to the /exp entry point which returns results as CSV. Querying Data ​ cURL Python NodeJS Go curl -G \\ --data-urlencode \"query=SELECT x FROM long_"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "SQL query and returns results as JSON. This is similar to the /exp entry point which returns results as CSV. Querying Data ​ cURL Python NodeJS Go curl -G \\ --data-urlencode \"query=SELECT x FROM long_sequence(5);\" \\ http://localhost:9000/exec The JSON response contains the original query, a \"columns\" key with the schema of the results, a \"count\" number of rows and a \"dataset\" with the results. { \"query\" : \"SELECT x FROM long_sequence(5);\" , \"columns\" : [ { \"name\" : \"x\" , \"type\" : \"LONG\" } ] , \"dataset\" : [ [ 1 ] , [ 2 ] , [ 3 ] , [ 4 ] , [ 5 ] ] , \"count\" : 5 } import sys import requests host = 'http://localhost:9000' sql_query = \"select * from long_sequence(10)\" try : response = requests . get ( host + '/exec' , params = { 'query' : sql_query } ) . json ( ) for row in response [ 'dataset' ] : print ( row [ 0 ] ) except requests . exceptions . RequestException as e : print ( f'Error: { e } ' , file = sys . stderr ) const fetch = require ( \"node-fetch\" ) const HOST = \"http://localhost:9000\" async function run ( ) { try { const query = \"SELECT x FROM long_sequence(5);\" const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) const json = await r"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "000\" async function run ( ) { try { const query = \"SELECT x FROM long_sequence(5);\" const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) const json = await response . json ( ) console . log ( json ) } catch ( error ) { console . log ( error ) } } run ( ) package main import ( \"fmt\" \"io/ioutil\" \"log\" \"net/http\" \"net/url\" ) func main ( ) { u , err := url . Parse ( \"http://localhost:9000\" ) checkErr ( err ) u . Path += \"exec\" params := url . Values { } params . Add ( \"query\" , \"SELECT x FROM long_sequence(5);\" ) u . RawQuery = params . Encode ( ) url := fmt . Sprintf ( \"%v\" , u ) res , err := http . Get ( url ) checkErr ( err ) defer res . Body . Close ( ) body , err := ioutil . ReadAll ( res . Body ) checkErr ( err ) log . Println ( string ( body ) ) } func checkErr ( err error ) { if err != nil { panic ( err ) } } Alternatively, the /exec endpoint can be used to create a table and the INSERT statement can be used to populate it with values: cURL NodeJS Python # Create Table curl -G \\ --data-urlencode \"query=CREATE TABLE IF NOT EXISTS trades(name VARCHAR, value INT)\" \\ http://localhost:9000/exec # Insert a row curl -G \\ --data-urlencode \"que"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "L NodeJS Python # Create Table curl -G \\ --data-urlencode \"query=CREATE TABLE IF NOT EXISTS trades(name VARCHAR, value INT)\" \\ http://localhost:9000/exec # Insert a row curl -G \\ --data-urlencode \"query=INSERT INTO trades VALUES('abc', 123456)\" \\ http://localhost:9000/exec # Update a row curl -G \\ --data-urlencode \"query=UPDATE trades SET value = 9876 WHERE name = 'abc'\" \\ http://localhost:9000/exec The node-fetch package can be installed using npm i node-fetch . const fetch = require ( \"node-fetch\" ) ; const HOST = \"http://localhost:9000\" ; async function createTable ( ) { try { const query = \"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value INT)\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } async function insertData ( ) { try { const query = \"INSERT INTO trades VALUES('abc', 123456)\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } async func"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "etch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } async function updateData ( ) { try { const query = \"UPDATE trades SET value = 9876 WHERE name = 'abc'\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } createTable ( ) . then ( insertData ) . then ( updateData ) ; import requests import json host = 'http://localhost:9000' def run_query ( sql_query ) : query_params = { 'query' : sql_query , 'fmt' : 'json' } try : response = requests . get ( host + '/exec' , params = query_params ) json_response = json . loads ( response . text ) print ( json_response ) except requests . exceptions . RequestException as e : print ( \"Error: %s\" % ( e ) ) # create table run_query ( \"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value INT)\" ) # insert row run_query ( \"INSERT INTO trades VALUES('abc', 123456)\" ) # update row run_query ( \"UPDATE trades SET value = 9876 WHERE name = 'abc'\" ) Apache Par"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "ISTS trades (name VARCHAR, value INT)\" ) # insert row run_query ( \"INSERT INTO trades VALUES('abc', 123456)\" ) # update row run_query ( \"UPDATE trades SET value = 9876 WHERE name = 'abc'\" ) Apache Parquet ​ info Apache Parquet support is in beta . It may not be fit for production use. Please let us know if you run into issues. Either: Email us at support@questdb.io Join our public Slack Post on our Discourse community Parquet files can be read and thus queried by QuestDB. QuestDB is shipped with a demo Parquet file, trades.parquet , which can be queried using the read_parquet function. Example: read_parquet example SELECT * FROM read_parquet('trades.parquet') WHERE side = 'buy'; The trades.parquet file is located in the import subdirectory inside the QuestDB root directory. Drop your own Parquet files to the import directory and query them using the read_parquet() function. You can change the allowed directory by setting the cairo.sql.copy.root configuration key. For more information, see the Parquet documentation . What's next? ​ Now... SQL! It's query time. Whether you want to use the Web Console , PostgreSQL or REST HTTP (or both), query construction is rich. To brush up and lea"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "see the Parquet documentation . What's next? ​ Now... SQL! It's query time. Whether you want to use the Web Console , PostgreSQL or REST HTTP (or both), query construction is rich. To brush up and learn what's unique in QuestDB, consider the following: Data types SQL execution order And to learn about some of our favourite, most powerful syntax: Window functions are a powerful analysis tool Aggregate functions - aggregations are key! Date & time operators to learn about date and time SAMPLE BY to summarize data into chunks based on a specified time interval, from a year to a microsecond WHERE IN to compress time ranges into concise intervals LATEST ON for latest values within multiple series within a table ASOF JOIN to associate timestamps between a series based on proximity; no extra indices required Materialized Views to pre-compute complex queries for optimal performance Looking for visuals? Explore Grafana Jump quickly into the Web Console Edit this page Previous REST HTTP API Next Overview QuestDB Web Console PostgreSQL REST HTTP API Apache Parquet What's next?\n\nQuery & SQL Reference Query & SQL Overview On this page Query & SQL Overview Querying - as a base action - is perfor"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "Overview QuestDB Web Console PostgreSQL REST HTTP API Apache Parquet What's next?\n\nQuery & SQL Reference Query & SQL Overview On this page Query & SQL Overview Querying - as a base action - is performed in three primary ways: Query via the QuestDB Web Console Query via PostgreSQL Query via REST HTTP API Query via Apache Parquet For efficient and clear querying, QuestDB provides SQL with enhanced time series extensions. This makes analyzing, downsampling, processing and reading time series data an intuitive and flexible experience. Queries can be written into many applications using existing drivers and clients of the PostgreSQL or REST-ful ecosystems. However, querying is also leveraged heavily by third-party tools to provide visualizations, such as within Grafana , or for connectivity into broad data infrastructure and application environments such as with a tool like Cube . Need to ingest data first? Checkout our Ingestion overview . QuestDB Web Console ​ The Web Console is available by default at localhost:9000 . The GUI makes it easy to write, return and chart queries. There is autocomplete, syntax highlighting, errors, and more. If you want to test a query or interact direclt"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "e by default at localhost:9000 . The GUI makes it easy to write, return and chart queries. There is autocomplete, syntax highlighting, errors, and more. If you want to test a query or interact direclty with your data in the cleanest and simplest way, apply queries via the Web Console . Click to zoom For an example, click Demo this query in the below snippet. This will run a query within our public demo instance and Web Console : Navigate time with SQL Demo this query SELECT timestamp, symbol, first(price) AS open, last(price) AS close, min(price), max(price), sum(amount) AS volume FROM trades WHERE timestamp > dateadd('d', -1, now()) SAMPLE BY 15m; If you see Demo this query on other snippets in this docs, they can be run against the demo instance. PostgreSQL ​ Query QuestDB using the PostgreSQL endpoint via the default port 8812 . See PGWire Client overview for details on how to connect to QuestDB using PostgreSQL clients. Brief examples in multiple languages are shown below. Python Java NodeJS Go C# C Ruby PHP import psycopg as pg import time # Connect to an existing QuestDB instance conn_str = 'user=admin password=quest host=127.0.0.1 port=8812 dbname=qdb' with pg . connect ( co"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "ava NodeJS Go C# C Ruby PHP import psycopg as pg import time # Connect to an existing QuestDB instance conn_str = 'user=admin password=quest host=127.0.0.1 port=8812 dbname=qdb' with pg . connect ( conn_str , autocommit = True ) as connection : # Open a cursor to perform database operations with connection . cursor ( ) as cur : #Query the database and obtain data as Python objects. cur . execute ( 'SELECT * FROM trades_pg;' ) records = cur . fetchall ( ) for row in records : print ( row ) # the connection is now closed package com.myco; import java.sql.*; import java.util.Properties; public class App { public static void main(String[] args) throws SQLException { Properties properties = new Properties(); properties.setProperty(\"user\", \"admin\"); properties.setProperty(\"password\", \"quest\"); properties.setProperty(\"sslmode\", \"disable\"); final Connection connection = DriverManager.getConnection( \"jdbc:postgresql://localhost:8812/qdb\", properties); try (PreparedStatement preparedStatement = connection.prepareStatement( \"SELECT x FROM long_sequence(5);\")) { try (ResultSet rs = preparedStatement.executeQuery()) { while (rs.next()) { System.out.println(rs.getLong(1)); } } } connection.close"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "connection.prepareStatement( \"SELECT x FROM long_sequence(5);\")) { try (ResultSet rs = preparedStatement.executeQuery()) { while (rs.next()) { System.out.println(rs.getLong(1)); } } } connection.close(); } } \"use strict\" const { Client } = require ( \"pg\" ) const start = async ( ) => { const client = new Client ( { database : \"qdb\" , host : \"127.0.0.1\" , password : \"quest\" , port : 8812 , user : \"admin\" , } ) await client . connect ( ) const res = await client . query ( \"SELECT x FROM long_sequence(5);\" ) console . log ( res . rows ) await client . end ( ) } start ( ) . catch ( console . error ) package main import ( \"database/sql\" \"fmt\" _ \"github.com/lib/pq\" ) const ( host = \"localhost\" port = 8812 user = \"admin\" password = \"quest\" dbname = \"qdb\" ) func main ( ) { connStr := fmt . Sprintf ( \"host=%s port=%d user=%s password=%s dbname=%s sslmode=disable\" , host , port , user , password , dbname ) db , err := sql . Open ( \"postgres\" , connStr ) checkErr ( err ) defer db . Close ( ) stmt , err := db . Prepare ( \"SELECT x FROM long_sequence(5);\" ) checkErr ( err ) defer stmt . Close ( ) rows , err := stmt . Query ( ) checkErr ( err ) defer rows . Close ( ) var num string for rows . Nex"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": ", err := db . Prepare ( \"SELECT x FROM long_sequence(5);\" ) checkErr ( err ) defer stmt . Close ( ) rows , err := stmt . Query ( ) checkErr ( err ) defer rows . Close ( ) var num string for rows . Next ( ) { err = rows . Scan ( & num ) checkErr ( err ) fmt . Println ( num ) } err = rows . Err ( ) checkErr ( err ) } func checkErr ( err error ) { if err != nil { panic ( err ) } } // compile with // g++ libpq_example.c -o libpq_example.exe -I pgsql\\include -L dev\\pgsql\\lib // -std=c++17 -lpthread -lpq # include <libpq-fe.h> # include <stdio.h> # include <stdlib.h> void do_exit ( PGconn * conn ) { PQfinish ( conn ) ; exit ( 1 ) ; } int main ( ) { PGconn * conn = PQconnectdb ( \"host=localhost user=admin password=quest port=8812 dbname=testdb\" ) ; if ( PQstatus ( conn ) == CONNECTION_BAD ) { fprintf ( stderr , \"Connection to database failed: %s\\n\" , PQerrorMessage ( conn ) ) ; do_exit ( conn ) ; } PGresult * res = PQexec ( conn , \"SELECT x FROM long_sequence(5);\" ) ; if ( PQresultStatus ( res ) != PGRES_TUPLES_OK ) { printf ( \"No data retrieved\\n\" ) ; PQclear ( res ) ; do_exit ( conn ) ; } int rows = PQntuples ( res ) ; for ( int i = 0 ; i < rows ; i ++ ) { printf ( \"%s\\n\" , PQgetvalue ("}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "s ) != PGRES_TUPLES_OK ) { printf ( \"No data retrieved\\n\" ) ; PQclear ( res ) ; do_exit ( conn ) ; } int rows = PQntuples ( res ) ; for ( int i = 0 ; i < rows ; i ++ ) { printf ( \"%s\\n\" , PQgetvalue ( res , i , 0 ) ) ; } PQclear ( res ) ; PQfinish ( conn ) ; return 0 ; } using Npgsql; string username = \"admin\"; string password = \"quest\"; string database = \"qdb\"; int port = 8812; var connectionString = $@\"host=localhost;port={port};username={username};password={password}; database={database};ServerCompatibilityMode=NoTypeLoading;\"; await using NpgsqlConnection connection = new NpgsqlConnection(connectionString); await connection.OpenAsync(); var sql = \"SELECT x FROM long_sequence(5);\"; await using NpgsqlCommand command = new NpgsqlCommand(sql, connection); await using (var reader = await command.ExecuteReaderAsync()) { while (await reader.ReadAsync()) { var x = reader.GetInt64(0); } } require 'pg' begin conn =PG.connect( host: \"127.0.0.1\", port: 8812, dbname: 'qdb', user: 'admin', password: 'quest' ) rows = conn.exec 'SELECT x FROM long_sequence(5);' rows.each do |row| puts row end rescue PG::Error => e puts e.message ensure conn.close if conn end <?php function exceptions_error_han"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "ord: 'quest' ) rows = conn.exec 'SELECT x FROM long_sequence(5);' rows.each do |row| puts row end rescue PG::Error => e puts e.message ensure conn.close if conn end <?php function exceptions_error_handler($severity, $message, $filename, $lineno) { throw new ErrorException($message, 0, $severity, $filename, $lineno); } set_error_handler('exceptions_error_handler'); $db_conn = null; try { $db_conn = pg_connect(\" host = 'localhost' port=8812 dbname = 'qdb' user = 'admin' password = 'quest' \"); $result = pg_query($db_conn, 'SELECT x FROM long_sequence(5);' ); while ($row = pg_fetch_assoc($result) ){ print_r($row); } pg_free_result($result); } catch (Exception $e) { echo 'Caught exception: ', $e->getMessage(), \"\\n\"; } finally { if (!is_null($db_conn)) { pg_close($db_conn); } } ?> Further Reading ​ See the PGWire Client overview for more details on how to use PostgreSQL clients to connect to QuestDB. REST HTTP API ​ QuestDB exposes a REST API for compatibility with a wide range of libraries and tools. The REST API is accessible on port 9000 and has the following query-capable entrypoints: For details such as content type, query parameters and more, refer to the REST HTTP API reference. E"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "and tools. The REST API is accessible on port 9000 and has the following query-capable entrypoints: For details such as content type, query parameters and more, refer to the REST HTTP API reference. Entrypoint HTTP Method Description REST HTTP API Reference /exp?query=.. GET Export SQL Query as CSV Reference /exec?query=.. GET Run SQL Query returning JSON result set Reference /exp : SQL Query to CSV ​ The /exp entrypoint allows querying the database with a SQL select query and obtaining the results as CSV. For obtaining results in JSON, use /exec instead, documented next. cURL Python curl -G --data-urlencode \\ \"query=SELECT * FROM example_table2 LIMIT 3\" \\ http://localhost:9000/exp \"col1\",\"col2\",\"col3\" \"a\",10.5,true \"b\",100.0,false \"c\",,true import requests resp = requests . get ( 'http://localhost:9000/exp' , { 'query' : 'SELECT * FROM example_table2' , 'limit' : '3,6' # Rows 3, 4, 5 } ) print ( resp . text ) \"col1\",\"col2\",\"col3\" \"d\",20.5,true \"e\",200.0,false \"f\",,true /exec : SQL Query to JSON ​ The /exec entrypoint takes a SQL query and returns results as JSON. This is similar to the /exp entry point which returns results as CSV. Querying Data ​ cURL Python NodeJS Go curl -G \\ -"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "ry to JSON ​ The /exec entrypoint takes a SQL query and returns results as JSON. This is similar to the /exp entry point which returns results as CSV. Querying Data ​ cURL Python NodeJS Go curl -G \\ --data-urlencode \"query=SELECT x FROM long_sequence(5);\" \\ http://localhost:9000/exec The JSON response contains the original query, a \"columns\" key with the schema of the results, a \"count\" number of rows and a \"dataset\" with the results. { \"query\" : \"SELECT x FROM long_sequence(5);\" , \"columns\" : [ { \"name\" : \"x\" , \"type\" : \"LONG\" } ] , \"dataset\" : [ [ 1 ] , [ 2 ] , [ 3 ] , [ 4 ] , [ 5 ] ] , \"count\" : 5 } import sys import requests host = 'http://localhost:9000' sql_query = \"select * from long_sequence(10)\" try : response = requests . get ( host + '/exec' , params = { 'query' : sql_query } ) . json ( ) for row in response [ 'dataset' ] : print ( row [ 0 ] ) except requests . exceptions . RequestException as e : print ( f'Error: { e } ' , file = sys . stderr ) const fetch = require ( \"node-fetch\" ) const HOST = \"http://localhost:9000\" async function run ( ) { try { const query = \"SELECT x FROM long_sequence(5);\" const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURICompon"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "-fetch\" ) const HOST = \"http://localhost:9000\" async function run ( ) { try { const query = \"SELECT x FROM long_sequence(5);\" const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) const json = await response . json ( ) console . log ( json ) } catch ( error ) { console . log ( error ) } } run ( ) package main import ( \"fmt\" \"io/ioutil\" \"log\" \"net/http\" \"net/url\" ) func main ( ) { u , err := url . Parse ( \"http://localhost:9000\" ) checkErr ( err ) u . Path += \"exec\" params := url . Values { } params . Add ( \"query\" , \"SELECT x FROM long_sequence(5);\" ) u . RawQuery = params . Encode ( ) url := fmt . Sprintf ( \"%v\" , u ) res , err := http . Get ( url ) checkErr ( err ) defer res . Body . Close ( ) body , err := ioutil . ReadAll ( res . Body ) checkErr ( err ) log . Println ( string ( body ) ) } func checkErr ( err error ) { if err != nil { panic ( err ) } } Alternatively, the /exec endpoint can be used to create a table and the INSERT statement can be used to populate it with values: cURL NodeJS Python # Create Table curl -G \\ --data-urlencode \"query=CREATE TABLE IF NOT EXISTS trades(name VARCHAR, value INT)\" \\ http://localhost:9000/exec # In"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "an be used to populate it with values: cURL NodeJS Python # Create Table curl -G \\ --data-urlencode \"query=CREATE TABLE IF NOT EXISTS trades(name VARCHAR, value INT)\" \\ http://localhost:9000/exec # Insert a row curl -G \\ --data-urlencode \"query=INSERT INTO trades VALUES('abc', 123456)\" \\ http://localhost:9000/exec # Update a row curl -G \\ --data-urlencode \"query=UPDATE trades SET value = 9876 WHERE name = 'abc'\" \\ http://localhost:9000/exec The node-fetch package can be installed using npm i node-fetch . const fetch = require ( \"node-fetch\" ) ; const HOST = \"http://localhost:9000\" ; async function createTable ( ) { try { const query = \"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value INT)\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } async function insertData ( ) { try { const query = \"INSERT INTO trades VALUES('abc', 123456)\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error )"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "'abc', 123456)\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } async function updateData ( ) { try { const query = \"UPDATE trades SET value = 9876 WHERE name = 'abc'\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } createTable ( ) . then ( insertData ) . then ( updateData ) ; import requests import json host = 'http://localhost:9000' def run_query ( sql_query ) : query_params = { 'query' : sql_query , 'fmt' : 'json' } try : response = requests . get ( host + '/exec' , params = query_params ) json_response = json . loads ( response . text ) print ( json_response ) except requests . exceptions . RequestException as e : print ( \"Error: %s\" % ( e ) ) # create table run_query ( \"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value INT)\" ) # insert row run_query ( \"INSERT INTO trades VALUES('abc', 123456)\" ) # update row run_query ( \"UPDATE trades SET val"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "table run_query ( \"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value INT)\" ) # insert row run_query ( \"INSERT INTO trades VALUES('abc', 123456)\" ) # update row run_query ( \"UPDATE trades SET value = 9876 WHERE name = 'abc'\" ) Apache Parquet ​ info Apache Parquet support is in beta . It may not be fit for production use. Please let us know if you run into issues. Either: Email us at support@questdb.io Join our public Slack Post on our Discourse community Parquet files can be read and thus queried by QuestDB. QuestDB is shipped with a demo Parquet file, trades.parquet , which can be queried using the read_parquet function. Example: read_parquet example SELECT * FROM read_parquet('trades.parquet') WHERE side = 'buy'; The trades.parquet file is located in the import subdirectory inside the QuestDB root directory. Drop your own Parquet files to the import directory and query them using the read_parquet() function. You can change the allowed directory by setting the cairo.sql.copy.root configuration key. For more information, see the Parquet documentation . What's next? ​ Now... SQL! It's query time. Whether you want to use the Web Console , PostgreSQL or REST HTTP (or both), query"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "configuration key. For more information, see the Parquet documentation . What's next? ​ Now... SQL! It's query time. Whether you want to use the Web Console , PostgreSQL or REST HTTP (or both), query construction is rich. To brush up and learn what's unique in QuestDB, consider the following: Data types SQL execution order And to learn about some of our favourite, most powerful syntax: Window functions are a powerful analysis tool Aggregate functions - aggregations are key! Date & time operators to learn about date and time SAMPLE BY to summarize data into chunks based on a specified time interval, from a year to a microsecond WHERE IN to compress time ranges into concise intervals LATEST ON for latest values within multiple series within a table ASOF JOIN to associate timestamps between a series based on proximity; no extra indices required Materialized Views to pre-compute complex queries for optimal performance Looking for visuals? Explore Grafana Jump quickly into the Web Console Edit this page Previous REST HTTP API Next Overview\n\nQuery & SQL Reference Query & SQL Overview On this page Query & SQL Overview Querying - as a base action - is performed in three primary ways: Quer"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "Edit this page Previous REST HTTP API Next Overview\n\nQuery & SQL Reference Query & SQL Overview On this page Query & SQL Overview Querying - as a base action - is performed in three primary ways: Query via the QuestDB Web Console Query via PostgreSQL Query via REST HTTP API Query via Apache Parquet For efficient and clear querying, QuestDB provides SQL with enhanced time series extensions. This makes analyzing, downsampling, processing and reading time series data an intuitive and flexible experience. Queries can be written into many applications using existing drivers and clients of the PostgreSQL or REST-ful ecosystems. However, querying is also leveraged heavily by third-party tools to provide visualizations, such as within Grafana , or for connectivity into broad data infrastructure and application environments such as with a tool like Cube . Need to ingest data first? Checkout our Ingestion overview . QuestDB Web Console ​ The Web Console is available by default at localhost:9000 . The GUI makes it easy to write, return and chart queries. There is autocomplete, syntax highlighting, errors, and more. If you want to test a query or interact direclty with your data in the cleanes"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": ". The GUI makes it easy to write, return and chart queries. There is autocomplete, syntax highlighting, errors, and more. If you want to test a query or interact direclty with your data in the cleanest and simplest way, apply queries via the Web Console . Click to zoom For an example, click Demo this query in the below snippet. This will run a query within our public demo instance and Web Console : Navigate time with SQL Demo this query SELECT timestamp, symbol, first(price) AS open, last(price) AS close, min(price), max(price), sum(amount) AS volume FROM trades WHERE timestamp > dateadd('d', -1, now()) SAMPLE BY 15m; If you see Demo this query on other snippets in this docs, they can be run against the demo instance. PostgreSQL ​ Query QuestDB using the PostgreSQL endpoint via the default port 8812 . See PGWire Client overview for details on how to connect to QuestDB using PostgreSQL clients. Brief examples in multiple languages are shown below. Python Java NodeJS Go C# C Ruby PHP import psycopg as pg import time # Connect to an existing QuestDB instance conn_str = 'user=admin password=quest host=127.0.0.1 port=8812 dbname=qdb' with pg . connect ( conn_str , autocommit = True ) as"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "ort psycopg as pg import time # Connect to an existing QuestDB instance conn_str = 'user=admin password=quest host=127.0.0.1 port=8812 dbname=qdb' with pg . connect ( conn_str , autocommit = True ) as connection : # Open a cursor to perform database operations with connection . cursor ( ) as cur : #Query the database and obtain data as Python objects. cur . execute ( 'SELECT * FROM trades_pg;' ) records = cur . fetchall ( ) for row in records : print ( row ) # the connection is now closed package com.myco; import java.sql.*; import java.util.Properties; public class App { public static void main(String[] args) throws SQLException { Properties properties = new Properties(); properties.setProperty(\"user\", \"admin\"); properties.setProperty(\"password\", \"quest\"); properties.setProperty(\"sslmode\", \"disable\"); final Connection connection = DriverManager.getConnection( \"jdbc:postgresql://localhost:8812/qdb\", properties); try (PreparedStatement preparedStatement = connection.prepareStatement( \"SELECT x FROM long_sequence(5);\")) { try (ResultSet rs = preparedStatement.executeQuery()) { while (rs.next()) { System.out.println(rs.getLong(1)); } } } connection.close(); } } \"use strict\" const { Cl"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "ELECT x FROM long_sequence(5);\")) { try (ResultSet rs = preparedStatement.executeQuery()) { while (rs.next()) { System.out.println(rs.getLong(1)); } } } connection.close(); } } \"use strict\" const { Client } = require ( \"pg\" ) const start = async ( ) => { const client = new Client ( { database : \"qdb\" , host : \"127.0.0.1\" , password : \"quest\" , port : 8812 , user : \"admin\" , } ) await client . connect ( ) const res = await client . query ( \"SELECT x FROM long_sequence(5);\" ) console . log ( res . rows ) await client . end ( ) } start ( ) . catch ( console . error ) package main import ( \"database/sql\" \"fmt\" _ \"github.com/lib/pq\" ) const ( host = \"localhost\" port = 8812 user = \"admin\" password = \"quest\" dbname = \"qdb\" ) func main ( ) { connStr := fmt . Sprintf ( \"host=%s port=%d user=%s password=%s dbname=%s sslmode=disable\" , host , port , user , password , dbname ) db , err := sql . Open ( \"postgres\" , connStr ) checkErr ( err ) defer db . Close ( ) stmt , err := db . Prepare ( \"SELECT x FROM long_sequence(5);\" ) checkErr ( err ) defer stmt . Close ( ) rows , err := stmt . Query ( ) checkErr ( err ) defer rows . Close ( ) var num string for rows . Next ( ) { err = rows . Scan ( & n"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "x FROM long_sequence(5);\" ) checkErr ( err ) defer stmt . Close ( ) rows , err := stmt . Query ( ) checkErr ( err ) defer rows . Close ( ) var num string for rows . Next ( ) { err = rows . Scan ( & num ) checkErr ( err ) fmt . Println ( num ) } err = rows . Err ( ) checkErr ( err ) } func checkErr ( err error ) { if err != nil { panic ( err ) } } // compile with // g++ libpq_example.c -o libpq_example.exe -I pgsql\\include -L dev\\pgsql\\lib // -std=c++17 -lpthread -lpq # include <libpq-fe.h> # include <stdio.h> # include <stdlib.h> void do_exit ( PGconn * conn ) { PQfinish ( conn ) ; exit ( 1 ) ; } int main ( ) { PGconn * conn = PQconnectdb ( \"host=localhost user=admin password=quest port=8812 dbname=testdb\" ) ; if ( PQstatus ( conn ) == CONNECTION_BAD ) { fprintf ( stderr , \"Connection to database failed: %s\\n\" , PQerrorMessage ( conn ) ) ; do_exit ( conn ) ; } PGresult * res = PQexec ( conn , \"SELECT x FROM long_sequence(5);\" ) ; if ( PQresultStatus ( res ) != PGRES_TUPLES_OK ) { printf ( \"No data retrieved\\n\" ) ; PQclear ( res ) ; do_exit ( conn ) ; } int rows = PQntuples ( res ) ; for ( int i = 0 ; i < rows ; i ++ ) { printf ( \"%s\\n\" , PQgetvalue ( res , i , 0 ) ) ; } PQclear ("}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "tf ( \"No data retrieved\\n\" ) ; PQclear ( res ) ; do_exit ( conn ) ; } int rows = PQntuples ( res ) ; for ( int i = 0 ; i < rows ; i ++ ) { printf ( \"%s\\n\" , PQgetvalue ( res , i , 0 ) ) ; } PQclear ( res ) ; PQfinish ( conn ) ; return 0 ; } using Npgsql; string username = \"admin\"; string password = \"quest\"; string database = \"qdb\"; int port = 8812; var connectionString = $@\"host=localhost;port={port};username={username};password={password}; database={database};ServerCompatibilityMode=NoTypeLoading;\"; await using NpgsqlConnection connection = new NpgsqlConnection(connectionString); await connection.OpenAsync(); var sql = \"SELECT x FROM long_sequence(5);\"; await using NpgsqlCommand command = new NpgsqlCommand(sql, connection); await using (var reader = await command.ExecuteReaderAsync()) { while (await reader.ReadAsync()) { var x = reader.GetInt64(0); } } require 'pg' begin conn =PG.connect( host: \"127.0.0.1\", port: 8812, dbname: 'qdb', user: 'admin', password: 'quest' ) rows = conn.exec 'SELECT x FROM long_sequence(5);' rows.each do |row| puts row end rescue PG::Error => e puts e.message ensure conn.close if conn end <?php function exceptions_error_handler($severity, $message, $file"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "'SELECT x FROM long_sequence(5);' rows.each do |row| puts row end rescue PG::Error => e puts e.message ensure conn.close if conn end <?php function exceptions_error_handler($severity, $message, $filename, $lineno) { throw new ErrorException($message, 0, $severity, $filename, $lineno); } set_error_handler('exceptions_error_handler'); $db_conn = null; try { $db_conn = pg_connect(\" host = 'localhost' port=8812 dbname = 'qdb' user = 'admin' password = 'quest' \"); $result = pg_query($db_conn, 'SELECT x FROM long_sequence(5);' ); while ($row = pg_fetch_assoc($result) ){ print_r($row); } pg_free_result($result); } catch (Exception $e) { echo 'Caught exception: ', $e->getMessage(), \"\\n\"; } finally { if (!is_null($db_conn)) { pg_close($db_conn); } } ?> Further Reading ​ See the PGWire Client overview for more details on how to use PostgreSQL clients to connect to QuestDB. REST HTTP API ​ QuestDB exposes a REST API for compatibility with a wide range of libraries and tools. The REST API is accessible on port 9000 and has the following query-capable entrypoints: For details such as content type, query parameters and more, refer to the REST HTTP API reference. Entrypoint HTTP Method Descripti"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "ssible on port 9000 and has the following query-capable entrypoints: For details such as content type, query parameters and more, refer to the REST HTTP API reference. Entrypoint HTTP Method Description REST HTTP API Reference /exp?query=.. GET Export SQL Query as CSV Reference /exec?query=.. GET Run SQL Query returning JSON result set Reference /exp : SQL Query to CSV ​ The /exp entrypoint allows querying the database with a SQL select query and obtaining the results as CSV. For obtaining results in JSON, use /exec instead, documented next. cURL Python curl -G --data-urlencode \\ \"query=SELECT * FROM example_table2 LIMIT 3\" \\ http://localhost:9000/exp \"col1\",\"col2\",\"col3\" \"a\",10.5,true \"b\",100.0,false \"c\",,true import requests resp = requests . get ( 'http://localhost:9000/exp' , { 'query' : 'SELECT * FROM example_table2' , 'limit' : '3,6' # Rows 3, 4, 5 } ) print ( resp . text ) \"col1\",\"col2\",\"col3\" \"d\",20.5,true \"e\",200.0,false \"f\",,true /exec : SQL Query to JSON ​ The /exec entrypoint takes a SQL query and returns results as JSON. This is similar to the /exp entry point which returns results as CSV. Querying Data ​ cURL Python NodeJS Go curl -G \\ --data-urlencode \"query=SELECT x"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "nt takes a SQL query and returns results as JSON. This is similar to the /exp entry point which returns results as CSV. Querying Data ​ cURL Python NodeJS Go curl -G \\ --data-urlencode \"query=SELECT x FROM long_sequence(5);\" \\ http://localhost:9000/exec The JSON response contains the original query, a \"columns\" key with the schema of the results, a \"count\" number of rows and a \"dataset\" with the results. { \"query\" : \"SELECT x FROM long_sequence(5);\" , \"columns\" : [ { \"name\" : \"x\" , \"type\" : \"LONG\" } ] , \"dataset\" : [ [ 1 ] , [ 2 ] , [ 3 ] , [ 4 ] , [ 5 ] ] , \"count\" : 5 } import sys import requests host = 'http://localhost:9000' sql_query = \"select * from long_sequence(10)\" try : response = requests . get ( host + '/exec' , params = { 'query' : sql_query } ) . json ( ) for row in response [ 'dataset' ] : print ( row [ 0 ] ) except requests . exceptions . RequestException as e : print ( f'Error: { e } ' , file = sys . stderr ) const fetch = require ( \"node-fetch\" ) const HOST = \"http://localhost:9000\" async function run ( ) { try { const query = \"SELECT x FROM long_sequence(5);\" const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) const jso"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "localhost:9000\" async function run ( ) { try { const query = \"SELECT x FROM long_sequence(5);\" const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) const json = await response . json ( ) console . log ( json ) } catch ( error ) { console . log ( error ) } } run ( ) package main import ( \"fmt\" \"io/ioutil\" \"log\" \"net/http\" \"net/url\" ) func main ( ) { u , err := url . Parse ( \"http://localhost:9000\" ) checkErr ( err ) u . Path += \"exec\" params := url . Values { } params . Add ( \"query\" , \"SELECT x FROM long_sequence(5);\" ) u . RawQuery = params . Encode ( ) url := fmt . Sprintf ( \"%v\" , u ) res , err := http . Get ( url ) checkErr ( err ) defer res . Body . Close ( ) body , err := ioutil . ReadAll ( res . Body ) checkErr ( err ) log . Println ( string ( body ) ) } func checkErr ( err error ) { if err != nil { panic ( err ) } } Alternatively, the /exec endpoint can be used to create a table and the INSERT statement can be used to populate it with values: cURL NodeJS Python # Create Table curl -G \\ --data-urlencode \"query=CREATE TABLE IF NOT EXISTS trades(name VARCHAR, value INT)\" \\ http://localhost:9000/exec # Insert a row curl -G \\ --data-url"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "values: cURL NodeJS Python # Create Table curl -G \\ --data-urlencode \"query=CREATE TABLE IF NOT EXISTS trades(name VARCHAR, value INT)\" \\ http://localhost:9000/exec # Insert a row curl -G \\ --data-urlencode \"query=INSERT INTO trades VALUES('abc', 123456)\" \\ http://localhost:9000/exec # Update a row curl -G \\ --data-urlencode \"query=UPDATE trades SET value = 9876 WHERE name = 'abc'\" \\ http://localhost:9000/exec The node-fetch package can be installed using npm i node-fetch . const fetch = require ( \"node-fetch\" ) ; const HOST = \"http://localhost:9000\" ; async function createTable ( ) { try { const query = \"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value INT)\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } async function insertData ( ) { try { const query = \"INSERT INTO trades VALUES('abc', 123456)\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } }"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "e = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } async function updateData ( ) { try { const query = \"UPDATE trades SET value = 9876 WHERE name = 'abc'\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } createTable ( ) . then ( insertData ) . then ( updateData ) ; import requests import json host = 'http://localhost:9000' def run_query ( sql_query ) : query_params = { 'query' : sql_query , 'fmt' : 'json' } try : response = requests . get ( host + '/exec' , params = query_params ) json_response = json . loads ( response . text ) print ( json_response ) except requests . exceptions . RequestException as e : print ( \"Error: %s\" % ( e ) ) # create table run_query ( \"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value INT)\" ) # insert row run_query ( \"INSERT INTO trades VALUES('abc', 123456)\" ) # update row run_query ( \"UPDATE trades SET value = 9876 WHERE name = 'abc'\" )"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "E IF NOT EXISTS trades (name VARCHAR, value INT)\" ) # insert row run_query ( \"INSERT INTO trades VALUES('abc', 123456)\" ) # update row run_query ( \"UPDATE trades SET value = 9876 WHERE name = 'abc'\" ) Apache Parquet ​ info Apache Parquet support is in beta . It may not be fit for production use. Please let us know if you run into issues. Either: Email us at support@questdb.io Join our public Slack Post on our Discourse community Parquet files can be read and thus queried by QuestDB. QuestDB is shipped with a demo Parquet file, trades.parquet , which can be queried using the read_parquet function. Example: read_parquet example SELECT * FROM read_parquet('trades.parquet') WHERE side = 'buy'; The trades.parquet file is located in the import subdirectory inside the QuestDB root directory. Drop your own Parquet files to the import directory and query them using the read_parquet() function. You can change the allowed directory by setting the cairo.sql.copy.root configuration key. For more information, see the Parquet documentation . What's next? ​ Now... SQL! It's query time. Whether you want to use the Web Console , PostgreSQL or REST HTTP (or both), query construction is rich. To brush"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "formation, see the Parquet documentation . What's next? ​ Now... SQL! It's query time. Whether you want to use the Web Console , PostgreSQL or REST HTTP (or both), query construction is rich. To brush up and learn what's unique in QuestDB, consider the following: Data types SQL execution order And to learn about some of our favourite, most powerful syntax: Window functions are a powerful analysis tool Aggregate functions - aggregations are key! Date & time operators to learn about date and time SAMPLE BY to summarize data into chunks based on a specified time interval, from a year to a microsecond WHERE IN to compress time ranges into concise intervals LATEST ON for latest values within multiple series within a table ASOF JOIN to associate timestamps between a series based on proximity; no extra indices required Materialized Views to pre-compute complex queries for optimal performance Looking for visuals? Explore Grafana Jump quickly into the Web Console Edit this page Previous REST HTTP API Next Overview\n\nQuery & SQL Reference\n\nQuery & SQL Overview\n\nOn this page\n\nQuery & SQL Overview Querying - as a base action - is performed in three primary ways: Query via the QuestDB Web Consol"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "TP API Next Overview\n\nQuery & SQL Reference\n\nQuery & SQL Overview\n\nOn this page\n\nQuery & SQL Overview Querying - as a base action - is performed in three primary ways: Query via the QuestDB Web Console Query via PostgreSQL Query via REST HTTP API Query via Apache Parquet For efficient and clear querying, QuestDB provides SQL with enhanced time series extensions. This makes analyzing, downsampling, processing and reading time series data an intuitive and flexible experience. Queries can be written into many applications using existing drivers and clients of the PostgreSQL or REST-ful ecosystems. However, querying is also leveraged heavily by third-party tools to provide visualizations, such as within Grafana , or for connectivity into broad data infrastructure and application environments such as with a tool like Cube . Need to ingest data first? Checkout our Ingestion overview . QuestDB Web Console ​ The Web Console is available by default at localhost:9000 . The GUI makes it easy to write, return and chart queries. There is autocomplete, syntax highlighting, errors, and more. If you want to test a query or interact direclty with your data in the cleanest and simplest way, apply qu"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "rite, return and chart queries. There is autocomplete, syntax highlighting, errors, and more. If you want to test a query or interact direclty with your data in the cleanest and simplest way, apply queries via the Web Console . Click to zoom For an example, click Demo this query in the below snippet. This will run a query within our public demo instance and Web Console : Navigate time with SQL Demo this query SELECT timestamp, symbol, first(price) AS open, last(price) AS close, min(price), max(price), sum(amount) AS volume FROM trades WHERE timestamp > dateadd('d', -1, now()) SAMPLE BY 15m; If you see Demo this query on other snippets in this docs, they can be run against the demo instance. PostgreSQL ​ Query QuestDB using the PostgreSQL endpoint via the default port 8812 . See PGWire Client overview for details on how to connect to QuestDB using PostgreSQL clients. Brief examples in multiple languages are shown below. Python Java NodeJS Go C# C Ruby PHP import psycopg as pg import time # Connect to an existing QuestDB instance conn_str = 'user=admin password=quest host=127.0.0.1 port=8812 dbname=qdb' with pg . connect ( conn_str , autocommit = True ) as connection : # Open a curso"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "e # Connect to an existing QuestDB instance conn_str = 'user=admin password=quest host=127.0.0.1 port=8812 dbname=qdb' with pg . connect ( conn_str , autocommit = True ) as connection : # Open a cursor to perform database operations with connection . cursor ( ) as cur : #Query the database and obtain data as Python objects. cur . execute ( 'SELECT * FROM trades_pg;' ) records = cur . fetchall ( ) for row in records : print ( row ) # the connection is now closed package com.myco; import java.sql.*; import java.util.Properties; public class App { public static void main(String[] args) throws SQLException { Properties properties = new Properties(); properties.setProperty(\"user\", \"admin\"); properties.setProperty(\"password\", \"quest\"); properties.setProperty(\"sslmode\", \"disable\"); final Connection connection = DriverManager.getConnection( \"jdbc:postgresql://localhost:8812/qdb\", properties); try (PreparedStatement preparedStatement = connection.prepareStatement( \"SELECT x FROM long_sequence(5);\")) { try (ResultSet rs = preparedStatement.executeQuery()) { while (rs.next()) { System.out.println(rs.getLong(1)); } } } connection.close(); } } \"use strict\" const { Client } = require ( \"pg\" ) co"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": ");\")) { try (ResultSet rs = preparedStatement.executeQuery()) { while (rs.next()) { System.out.println(rs.getLong(1)); } } } connection.close(); } } \"use strict\" const { Client } = require ( \"pg\" ) const start = async ( ) => { const client = new Client ( { database : \"qdb\" , host : \"127.0.0.1\" , password : \"quest\" , port : 8812 , user : \"admin\" , } ) await client . connect ( ) const res = await client . query ( \"SELECT x FROM long_sequence(5);\" ) console . log ( res . rows ) await client . end ( ) } start ( ) . catch ( console . error ) package main import ( \"database/sql\" \"fmt\" _ \"github.com/lib/pq\" ) const ( host = \"localhost\" port = 8812 user = \"admin\" password = \"quest\" dbname = \"qdb\" ) func main ( ) { connStr := fmt . Sprintf ( \"host=%s port=%d user=%s password=%s dbname=%s sslmode=disable\" , host , port , user , password , dbname ) db , err := sql . Open ( \"postgres\" , connStr ) checkErr ( err ) defer db . Close ( ) stmt , err := db . Prepare ( \"SELECT x FROM long_sequence(5);\" ) checkErr ( err ) defer stmt . Close ( ) rows , err := stmt . Query ( ) checkErr ( err ) defer rows . Close ( ) var num string for rows . Next ( ) { err = rows . Scan ( & num ) checkErr ( err ) fmt ."}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "checkErr ( err ) defer stmt . Close ( ) rows , err := stmt . Query ( ) checkErr ( err ) defer rows . Close ( ) var num string for rows . Next ( ) { err = rows . Scan ( & num ) checkErr ( err ) fmt . Println ( num ) } err = rows . Err ( ) checkErr ( err ) } func checkErr ( err error ) { if err != nil { panic ( err ) } } // compile with // g++ libpq_example.c -o libpq_example.exe -I pgsql\\include -L dev\\pgsql\\lib // -std=c++17 -lpthread -lpq # include <libpq-fe.h> # include <stdio.h> # include <stdlib.h> void do_exit ( PGconn * conn ) { PQfinish ( conn ) ; exit ( 1 ) ; } int main ( ) { PGconn * conn = PQconnectdb ( \"host=localhost user=admin password=quest port=8812 dbname=testdb\" ) ; if ( PQstatus ( conn ) == CONNECTION_BAD ) { fprintf ( stderr , \"Connection to database failed: %s\\n\" , PQerrorMessage ( conn ) ) ; do_exit ( conn ) ; } PGresult * res = PQexec ( conn , \"SELECT x FROM long_sequence(5);\" ) ; if ( PQresultStatus ( res ) != PGRES_TUPLES_OK ) { printf ( \"No data retrieved\\n\" ) ; PQclear ( res ) ; do_exit ( conn ) ; } int rows = PQntuples ( res ) ; for ( int i = 0 ; i < rows ; i ++ ) { printf ( \"%s\\n\" , PQgetvalue ( res , i , 0 ) ) ; } PQclear ( res ) ; PQfinish ( conn ) ;"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "; PQclear ( res ) ; do_exit ( conn ) ; } int rows = PQntuples ( res ) ; for ( int i = 0 ; i < rows ; i ++ ) { printf ( \"%s\\n\" , PQgetvalue ( res , i , 0 ) ) ; } PQclear ( res ) ; PQfinish ( conn ) ; return 0 ; } using Npgsql; string username = \"admin\"; string password = \"quest\"; string database = \"qdb\"; int port = 8812; var connectionString = $@\"host=localhost;port={port};username={username};password={password}; database={database};ServerCompatibilityMode=NoTypeLoading;\"; await using NpgsqlConnection connection = new NpgsqlConnection(connectionString); await connection.OpenAsync(); var sql = \"SELECT x FROM long_sequence(5);\"; await using NpgsqlCommand command = new NpgsqlCommand(sql, connection); await using (var reader = await command.ExecuteReaderAsync()) { while (await reader.ReadAsync()) { var x = reader.GetInt64(0); } } require 'pg' begin conn =PG.connect( host: \"127.0.0.1\", port: 8812, dbname: 'qdb', user: 'admin', password: 'quest' ) rows = conn.exec 'SELECT x FROM long_sequence(5);' rows.each do |row| puts row end rescue PG::Error => e puts e.message ensure conn.close if conn end <?php function exceptions_error_handler($severity, $message, $filename, $lineno) { throw new E"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "e(5);' rows.each do |row| puts row end rescue PG::Error => e puts e.message ensure conn.close if conn end <?php function exceptions_error_handler($severity, $message, $filename, $lineno) { throw new ErrorException($message, 0, $severity, $filename, $lineno); } set_error_handler('exceptions_error_handler'); $db_conn = null; try { $db_conn = pg_connect(\" host = 'localhost' port=8812 dbname = 'qdb' user = 'admin' password = 'quest' \"); $result = pg_query($db_conn, 'SELECT x FROM long_sequence(5);' ); while ($row = pg_fetch_assoc($result) ){ print_r($row); } pg_free_result($result); } catch (Exception $e) { echo 'Caught exception: ', $e->getMessage(), \"\\n\"; } finally { if (!is_null($db_conn)) { pg_close($db_conn); } } ?> Further Reading ​ See the PGWire Client overview for more details on how to use PostgreSQL clients to connect to QuestDB. REST HTTP API ​ QuestDB exposes a REST API for compatibility with a wide range of libraries and tools. The REST API is accessible on port 9000 and has the following query-capable entrypoints: For details such as content type, query parameters and more, refer to the REST HTTP API reference. Entrypoint HTTP Method Description REST HTTP API Reference /"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "the following query-capable entrypoints: For details such as content type, query parameters and more, refer to the REST HTTP API reference. Entrypoint HTTP Method Description REST HTTP API Reference /exp?query=.. GET Export SQL Query as CSV Reference /exec?query=.. GET Run SQL Query returning JSON result set Reference /exp : SQL Query to CSV ​ The /exp entrypoint allows querying the database with a SQL select query and obtaining the results as CSV. For obtaining results in JSON, use /exec instead, documented next. cURL Python curl -G --data-urlencode \\ \"query=SELECT * FROM example_table2 LIMIT 3\" \\ http://localhost:9000/exp \"col1\",\"col2\",\"col3\" \"a\",10.5,true \"b\",100.0,false \"c\",,true import requests resp = requests . get ( 'http://localhost:9000/exp' , { 'query' : 'SELECT * FROM example_table2' , 'limit' : '3,6' # Rows 3, 4, 5 } ) print ( resp . text ) \"col1\",\"col2\",\"col3\" \"d\",20.5,true \"e\",200.0,false \"f\",,true /exec : SQL Query to JSON ​ The /exec entrypoint takes a SQL query and returns results as JSON. This is similar to the /exp entry point which returns results as CSV. Querying Data ​ cURL Python NodeJS Go curl -G \\ --data-urlencode \"query=SELECT x FROM long_sequence(5);\" \\ h"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "urns results as JSON. This is similar to the /exp entry point which returns results as CSV. Querying Data ​ cURL Python NodeJS Go curl -G \\ --data-urlencode \"query=SELECT x FROM long_sequence(5);\" \\ http://localhost:9000/exec The JSON response contains the original query, a \"columns\" key with the schema of the results, a \"count\" number of rows and a \"dataset\" with the results. { \"query\" : \"SELECT x FROM long_sequence(5);\" , \"columns\" : [ { \"name\" : \"x\" , \"type\" : \"LONG\" } ] , \"dataset\" : [ [ 1 ] , [ 2 ] , [ 3 ] , [ 4 ] , [ 5 ] ] , \"count\" : 5 } import sys import requests host = 'http://localhost:9000' sql_query = \"select * from long_sequence(10)\" try : response = requests . get ( host + '/exec' , params = { 'query' : sql_query } ) . json ( ) for row in response [ 'dataset' ] : print ( row [ 0 ] ) except requests . exceptions . RequestException as e : print ( f'Error: { e } ' , file = sys . stderr ) const fetch = require ( \"node-fetch\" ) const HOST = \"http://localhost:9000\" async function run ( ) { try { const query = \"SELECT x FROM long_sequence(5);\" const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) const json = await response . json ("}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "on run ( ) { try { const query = \"SELECT x FROM long_sequence(5);\" const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) const json = await response . json ( ) console . log ( json ) } catch ( error ) { console . log ( error ) } } run ( ) package main import ( \"fmt\" \"io/ioutil\" \"log\" \"net/http\" \"net/url\" ) func main ( ) { u , err := url . Parse ( \"http://localhost:9000\" ) checkErr ( err ) u . Path += \"exec\" params := url . Values { } params . Add ( \"query\" , \"SELECT x FROM long_sequence(5);\" ) u . RawQuery = params . Encode ( ) url := fmt . Sprintf ( \"%v\" , u ) res , err := http . Get ( url ) checkErr ( err ) defer res . Body . Close ( ) body , err := ioutil . ReadAll ( res . Body ) checkErr ( err ) log . Println ( string ( body ) ) } func checkErr ( err error ) { if err != nil { panic ( err ) } } Alternatively, the /exec endpoint can be used to create a table and the INSERT statement can be used to populate it with values: cURL NodeJS Python # Create Table curl -G \\ --data-urlencode \"query=CREATE TABLE IF NOT EXISTS trades(name VARCHAR, value INT)\" \\ http://localhost:9000/exec # Insert a row curl -G \\ --data-urlencode \"query=INSERT INTO tr"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "Create Table curl -G \\ --data-urlencode \"query=CREATE TABLE IF NOT EXISTS trades(name VARCHAR, value INT)\" \\ http://localhost:9000/exec # Insert a row curl -G \\ --data-urlencode \"query=INSERT INTO trades VALUES('abc', 123456)\" \\ http://localhost:9000/exec # Update a row curl -G \\ --data-urlencode \"query=UPDATE trades SET value = 9876 WHERE name = 'abc'\" \\ http://localhost:9000/exec The node-fetch package can be installed using npm i node-fetch . const fetch = require ( \"node-fetch\" ) ; const HOST = \"http://localhost:9000\" ; async function createTable ( ) { try { const query = \"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value INT)\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } async function insertData ( ) { try { const query = \"INSERT INTO trades VALUES('abc', 123456)\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } async function updateData ("}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "} /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } async function updateData ( ) { try { const query = \"UPDATE trades SET value = 9876 WHERE name = 'abc'\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } createTable ( ) . then ( insertData ) . then ( updateData ) ; import requests import json host = 'http://localhost:9000' def run_query ( sql_query ) : query_params = { 'query' : sql_query , 'fmt' : 'json' } try : response = requests . get ( host + '/exec' , params = query_params ) json_response = json . loads ( response . text ) print ( json_response ) except requests . exceptions . RequestException as e : print ( \"Error: %s\" % ( e ) ) # create table run_query ( \"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value INT)\" ) # insert row run_query ( \"INSERT INTO trades VALUES('abc', 123456)\" ) # update row run_query ( \"UPDATE trades SET value = 9876 WHERE name = 'abc'\" ) Apache Parquet ​ info Apach"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "VARCHAR, value INT)\" ) # insert row run_query ( \"INSERT INTO trades VALUES('abc', 123456)\" ) # update row run_query ( \"UPDATE trades SET value = 9876 WHERE name = 'abc'\" ) Apache Parquet ​ info Apache Parquet support is in beta . It may not be fit for production use. Please let us know if you run into issues. Either: Email us at support@questdb.io Join our public Slack Post on our Discourse community Parquet files can be read and thus queried by QuestDB. QuestDB is shipped with a demo Parquet file, trades.parquet , which can be queried using the read_parquet function. Example: read_parquet example SELECT * FROM read_parquet('trades.parquet') WHERE side = 'buy'; The trades.parquet file is located in the import subdirectory inside the QuestDB root directory. Drop your own Parquet files to the import directory and query them using the read_parquet() function. You can change the allowed directory by setting the cairo.sql.copy.root configuration key. For more information, see the Parquet documentation . What's next? ​ Now... SQL! It's query time. Whether you want to use the Web Console , PostgreSQL or REST HTTP (or both), query construction is rich. To brush up and learn what's unique"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Introduction", "text": "ocumentation . What's next? ​ Now... SQL! It's query time. Whether you want to use the Web Console , PostgreSQL or REST HTTP (or both), query construction is rich. To brush up and learn what's unique in QuestDB, consider the following: Data types SQL execution order And to learn about some of our favourite, most powerful syntax: Window functions are a powerful analysis tool Aggregate functions - aggregations are key! Date & time operators to learn about date and time SAMPLE BY to summarize data into chunks based on a specified time interval, from a year to a microsecond WHERE IN to compress time ranges into concise intervals LATEST ON for latest values within multiple series within a table ASOF JOIN to associate timestamps between a series based on proximity; no extra indices required Materialized Views to pre-compute complex queries for optimal performance Looking for visuals? Explore Grafana Jump quickly into the Web Console"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Query & SQL Overview", "text": "Querying - as a base action - is performed in three primary ways:\n\nQuery via the QuestDB Web Console\n\nQuery via PostgreSQL\n\nQuery via REST HTTP API\n\nQuery via Apache Parquet\n\nFor efficient and clear querying, QuestDB provides SQL with enhanced time series extensions. This makes analyzing, downsampling, processing and reading time series data an intuitive and flexible experience.\n\nQueries can be written into many applications using existing drivers and clients of the PostgreSQL or REST-ful ecosystems. However, querying is also leveraged heavily by third-party tools to provide visualizations, such as within Grafana , or for connectivity into broad data infrastructure and application environments such as with a tool like Cube .\n\nNeed to ingest data first? Checkout our Ingestion overview ."}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "QuestDB Web Console ​", "text": "The Web Console is available by default at localhost:9000 . The GUI makes it easy to write, return and chart queries. There is autocomplete, syntax highlighting, errors, and more. If you want to test a query or interact direclty with your data in the cleanest and simplest way, apply queries via the Web Console .\n\nClick to zoom\n\nFor an example, click Demo this query in the below snippet. This will run a query within our public demo instance and Web Console :\n\nNavigate time with SQL Demo this query SELECT timestamp, symbol, first(price) AS open, last(price) AS close, min(price), max(price), sum(amount) AS volume FROM trades WHERE timestamp > dateadd('d', -1, now()) SAMPLE BY 15m;\n\nNavigate time with SQL Demo this query\n\nSELECT timestamp, symbol, first(price) AS open, last(price) AS close, min(price), max(price), sum(amount) AS volume FROM trades WHERE timestamp > dateadd('d', -1, now()) SAMPLE BY 15m;\n\nSELECT timestamp, symbol, first(price) AS open, last(price) AS close, min(price), max(price), sum(amount) AS volume FROM trades WHERE timestamp > dateadd('d', -1, now()) SAMPLE BY 15m;\n\nSELECT timestamp, symbol, first(price) AS open, last(price) AS close, min(price), max(price), sum(am"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "QuestDB Web Console ​", "text": "price), sum(amount) AS volume FROM trades WHERE timestamp > dateadd('d', -1, now()) SAMPLE BY 15m;\n\nSELECT timestamp, symbol, first(price) AS open, last(price) AS close, min(price), max(price), sum(amount) AS volume FROM trades WHERE timestamp > dateadd('d', -1, now()) SAMPLE BY 15m;\n\nIf you see Demo this query on other snippets in this docs, they can be run against the demo instance."}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "Query QuestDB using the PostgreSQL endpoint via the default port 8812 .\n\n8812\n\nSee PGWire Client overview for details on how to connect to QuestDB using PostgreSQL clients.\n\nBrief examples in multiple languages are shown below.\n\nPython Java NodeJS Go C# C Ruby PHP import psycopg as pg import time # Connect to an existing QuestDB instance conn_str = 'user=admin password=quest host=127.0.0.1 port=8812 dbname=qdb' with pg . connect ( conn_str , autocommit = True ) as connection : # Open a cursor to perform database operations with connection . cursor ( ) as cur : #Query the database and obtain data as Python objects. cur . execute ( 'SELECT * FROM trades_pg;' ) records = cur . fetchall ( ) for row in records : print ( row ) # the connection is now closed package com.myco; import java.sql.*; import java.util.Properties; public class App { public static void main(String[] args) throws SQLException { Properties properties = new Properties(); properties.setProperty(\"user\", \"admin\"); properties.setProperty(\"password\", \"quest\"); properties.setProperty(\"sslmode\", \"disable\"); final Connection connection = DriverManager.getConnection( \"jdbc:postgresql://localhost:8812/qdb\", properties); try (P"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "s.setProperty(\"password\", \"quest\"); properties.setProperty(\"sslmode\", \"disable\"); final Connection connection = DriverManager.getConnection( \"jdbc:postgresql://localhost:8812/qdb\", properties); try (PreparedStatement preparedStatement = connection.prepareStatement( \"SELECT x FROM long_sequence(5);\")) { try (ResultSet rs = preparedStatement.executeQuery()) { while (rs.next()) { System.out.println(rs.getLong(1)); } } } connection.close(); } } \"use strict\" const { Client } = require ( \"pg\" ) const start = async ( ) => { const client = new Client ( { database : \"qdb\" , host : \"127.0.0.1\" , password : \"quest\" , port : 8812 , user : \"admin\" , } ) await client . connect ( ) const res = await client . query ( \"SELECT x FROM long_sequence(5);\" ) console . log ( res . rows ) await client . end ( ) } start ( ) . catch ( console . error ) package main import ( \"database/sql\" \"fmt\" _ \"github.com/lib/pq\" ) const ( host = \"localhost\" port = 8812 user = \"admin\" password = \"quest\" dbname = \"qdb\" ) func main ( ) { connStr := fmt . Sprintf ( \"host=%s port=%d user=%s password=%s dbname=%s sslmode=disable\" , host , port , user , password , dbname ) db , err := sql . Open ( \"postgres\" , connStr ) checkE"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "main ( ) { connStr := fmt . Sprintf ( \"host=%s port=%d user=%s password=%s dbname=%s sslmode=disable\" , host , port , user , password , dbname ) db , err := sql . Open ( \"postgres\" , connStr ) checkErr ( err ) defer db . Close ( ) stmt , err := db . Prepare ( \"SELECT x FROM long_sequence(5);\" ) checkErr ( err ) defer stmt . Close ( ) rows , err := stmt . Query ( ) checkErr ( err ) defer rows . Close ( ) var num string for rows . Next ( ) { err = rows . Scan ( & num ) checkErr ( err ) fmt . Println ( num ) } err = rows . Err ( ) checkErr ( err ) } func checkErr ( err error ) { if err != nil { panic ( err ) } } // compile with // g++ libpq_example.c -o libpq_example.exe -I pgsql\\include -L dev\\pgsql\\lib // -std=c++17 -lpthread -lpq # include <libpq-fe.h> # include <stdio.h> # include <stdlib.h> void do_exit ( PGconn * conn ) { PQfinish ( conn ) ; exit ( 1 ) ; } int main ( ) { PGconn * conn = PQconnectdb ( \"host=localhost user=admin password=quest port=8812 dbname=testdb\" ) ; if ( PQstatus ( conn ) == CONNECTION_BAD ) { fprintf ( stderr , \"Connection to database failed: %s\\n\" , PQerrorMessage ( conn ) ) ; do_exit ( conn ) ; } PGresult * res = PQexec ( conn , \"SELECT x FROM long_seque"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "us ( conn ) == CONNECTION_BAD ) { fprintf ( stderr , \"Connection to database failed: %s\\n\" , PQerrorMessage ( conn ) ) ; do_exit ( conn ) ; } PGresult * res = PQexec ( conn , \"SELECT x FROM long_sequence(5);\" ) ; if ( PQresultStatus ( res ) != PGRES_TUPLES_OK ) { printf ( \"No data retrieved\\n\" ) ; PQclear ( res ) ; do_exit ( conn ) ; } int rows = PQntuples ( res ) ; for ( int i = 0 ; i < rows ; i ++ ) { printf ( \"%s\\n\" , PQgetvalue ( res , i , 0 ) ) ; } PQclear ( res ) ; PQfinish ( conn ) ; return 0 ; } using Npgsql; string username = \"admin\"; string password = \"quest\"; string database = \"qdb\"; int port = 8812; var connectionString = $@\"host=localhost;port={port};username={username};password={password}; database={database};ServerCompatibilityMode=NoTypeLoading;\"; await using NpgsqlConnection connection = new NpgsqlConnection(connectionString); await connection.OpenAsync(); var sql = \"SELECT x FROM long_sequence(5);\"; await using NpgsqlCommand command = new NpgsqlCommand(sql, connection); await using (var reader = await command.ExecuteReaderAsync()) { while (await reader.ReadAsync()) { var x = reader.GetInt64(0); } } require 'pg' begin conn =PG.connect( host: \"127.0.0.1\", port: 8812"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "); await using (var reader = await command.ExecuteReaderAsync()) { while (await reader.ReadAsync()) { var x = reader.GetInt64(0); } } require 'pg' begin conn =PG.connect( host: \"127.0.0.1\", port: 8812, dbname: 'qdb', user: 'admin', password: 'quest' ) rows = conn.exec 'SELECT x FROM long_sequence(5);' rows.each do |row| puts row end rescue PG::Error => e puts e.message ensure conn.close if conn end <?php function exceptions_error_handler($severity, $message, $filename, $lineno) { throw new ErrorException($message, 0, $severity, $filename, $lineno); } set_error_handler('exceptions_error_handler'); $db_conn = null; try { $db_conn = pg_connect(\" host = 'localhost' port=8812 dbname = 'qdb' user = 'admin' password = 'quest' \"); $result = pg_query($db_conn, 'SELECT x FROM long_sequence(5);' ); while ($row = pg_fetch_assoc($result) ){ print_r($row); } pg_free_result($result); } catch (Exception $e) { echo 'Caught exception: ', $e->getMessage(), \"\\n\"; } finally { if (!is_null($db_conn)) { pg_close($db_conn); } } ?>\n\nPython\n\nJava\n\nNodeJS\n\nGo\n\nC#\n\nC\n\nRuby\n\nPHP\n\nimport psycopg as pg import time # Connect to an existing QuestDB instance conn_str = 'user=admin password=quest host=127.0.0.1 port"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "lose($db_conn); } } ?>\n\nPython\n\nJava\n\nNodeJS\n\nGo\n\nC#\n\nC\n\nRuby\n\nPHP\n\nimport psycopg as pg import time # Connect to an existing QuestDB instance conn_str = 'user=admin password=quest host=127.0.0.1 port=8812 dbname=qdb' with pg . connect ( conn_str , autocommit = True ) as connection : # Open a cursor to perform database operations with connection . cursor ( ) as cur : #Query the database and obtain data as Python objects. cur . execute ( 'SELECT * FROM trades_pg;' ) records = cur . fetchall ( ) for row in records : print ( row ) # the connection is now closed package com.myco; import java.sql.*; import java.util.Properties; public class App { public static void main(String[] args) throws SQLException { Properties properties = new Properties(); properties.setProperty(\"user\", \"admin\"); properties.setProperty(\"password\", \"quest\"); properties.setProperty(\"sslmode\", \"disable\"); final Connection connection = DriverManager.getConnection( \"jdbc:postgresql://localhost:8812/qdb\", properties); try (PreparedStatement preparedStatement = connection.prepareStatement( \"SELECT x FROM long_sequence(5);\")) { try (ResultSet rs = preparedStatement.executeQuery()) { while (rs.next()) { System.out.printl"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "(PreparedStatement preparedStatement = connection.prepareStatement( \"SELECT x FROM long_sequence(5);\")) { try (ResultSet rs = preparedStatement.executeQuery()) { while (rs.next()) { System.out.println(rs.getLong(1)); } } } connection.close(); } } \"use strict\" const { Client } = require ( \"pg\" ) const start = async ( ) => { const client = new Client ( { database : \"qdb\" , host : \"127.0.0.1\" , password : \"quest\" , port : 8812 , user : \"admin\" , } ) await client . connect ( ) const res = await client . query ( \"SELECT x FROM long_sequence(5);\" ) console . log ( res . rows ) await client . end ( ) } start ( ) . catch ( console . error ) package main import ( \"database/sql\" \"fmt\" _ \"github.com/lib/pq\" ) const ( host = \"localhost\" port = 8812 user = \"admin\" password = \"quest\" dbname = \"qdb\" ) func main ( ) { connStr := fmt . Sprintf ( \"host=%s port=%d user=%s password=%s dbname=%s sslmode=disable\" , host , port , user , password , dbname ) db , err := sql . Open ( \"postgres\" , connStr ) checkErr ( err ) defer db . Close ( ) stmt , err := db . Prepare ( \"SELECT x FROM long_sequence(5);\" ) checkErr ( err ) defer stmt . Close ( ) rows , err := stmt . Query ( ) checkErr ( err ) defer rows ."}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "ckErr ( err ) defer db . Close ( ) stmt , err := db . Prepare ( \"SELECT x FROM long_sequence(5);\" ) checkErr ( err ) defer stmt . Close ( ) rows , err := stmt . Query ( ) checkErr ( err ) defer rows . Close ( ) var num string for rows . Next ( ) { err = rows . Scan ( & num ) checkErr ( err ) fmt . Println ( num ) } err = rows . Err ( ) checkErr ( err ) } func checkErr ( err error ) { if err != nil { panic ( err ) } } // compile with // g++ libpq_example.c -o libpq_example.exe -I pgsql\\include -L dev\\pgsql\\lib // -std=c++17 -lpthread -lpq # include <libpq-fe.h> # include <stdio.h> # include <stdlib.h> void do_exit ( PGconn * conn ) { PQfinish ( conn ) ; exit ( 1 ) ; } int main ( ) { PGconn * conn = PQconnectdb ( \"host=localhost user=admin password=quest port=8812 dbname=testdb\" ) ; if ( PQstatus ( conn ) == CONNECTION_BAD ) { fprintf ( stderr , \"Connection to database failed: %s\\n\" , PQerrorMessage ( conn ) ) ; do_exit ( conn ) ; } PGresult * res = PQexec ( conn , \"SELECT x FROM long_sequence(5);\" ) ; if ( PQresultStatus ( res ) != PGRES_TUPLES_OK ) { printf ( \"No data retrieved\\n\" ) ; PQclear ( res ) ; do_exit ( conn ) ; } int rows = PQntuples ( res ) ; for ( int i = 0 ; i < rows ;"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "quence(5);\" ) ; if ( PQresultStatus ( res ) != PGRES_TUPLES_OK ) { printf ( \"No data retrieved\\n\" ) ; PQclear ( res ) ; do_exit ( conn ) ; } int rows = PQntuples ( res ) ; for ( int i = 0 ; i < rows ; i ++ ) { printf ( \"%s\\n\" , PQgetvalue ( res , i , 0 ) ) ; } PQclear ( res ) ; PQfinish ( conn ) ; return 0 ; } using Npgsql; string username = \"admin\"; string password = \"quest\"; string database = \"qdb\"; int port = 8812; var connectionString = $@\"host=localhost;port={port};username={username};password={password}; database={database};ServerCompatibilityMode=NoTypeLoading;\"; await using NpgsqlConnection connection = new NpgsqlConnection(connectionString); await connection.OpenAsync(); var sql = \"SELECT x FROM long_sequence(5);\"; await using NpgsqlCommand command = new NpgsqlCommand(sql, connection); await using (var reader = await command.ExecuteReaderAsync()) { while (await reader.ReadAsync()) { var x = reader.GetInt64(0); } } require 'pg' begin conn =PG.connect( host: \"127.0.0.1\", port: 8812, dbname: 'qdb', user: 'admin', password: 'quest' ) rows = conn.exec 'SELECT x FROM long_sequence(5);' rows.each do |row| puts row end rescue PG::Error => e puts e.message ensure conn.close if conn"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "812, dbname: 'qdb', user: 'admin', password: 'quest' ) rows = conn.exec 'SELECT x FROM long_sequence(5);' rows.each do |row| puts row end rescue PG::Error => e puts e.message ensure conn.close if conn end <?php function exceptions_error_handler($severity, $message, $filename, $lineno) { throw new ErrorException($message, 0, $severity, $filename, $lineno); } set_error_handler('exceptions_error_handler'); $db_conn = null; try { $db_conn = pg_connect(\" host = 'localhost' port=8812 dbname = 'qdb' user = 'admin' password = 'quest' \"); $result = pg_query($db_conn, 'SELECT x FROM long_sequence(5);' ); while ($row = pg_fetch_assoc($result) ){ print_r($row); } pg_free_result($result); } catch (Exception $e) { echo 'Caught exception: ', $e->getMessage(), \"\\n\"; } finally { if (!is_null($db_conn)) { pg_close($db_conn); } } ?>\n\nimport psycopg as pg import time # Connect to an existing QuestDB instance conn_str = 'user=admin password=quest host=127.0.0.1 port=8812 dbname=qdb' with pg . connect ( conn_str , autocommit = True ) as connection : # Open a cursor to perform database operations with connection . cursor ( ) as cur : #Query the database and obtain data as Python objects. cur . execute ("}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "n_str , autocommit = True ) as connection : # Open a cursor to perform database operations with connection . cursor ( ) as cur : #Query the database and obtain data as Python objects. cur . execute ( 'SELECT * FROM trades_pg;' ) records = cur . fetchall ( ) for row in records : print ( row ) # the connection is now closed\n\nimport psycopg as pg import time # Connect to an existing QuestDB instance conn_str = 'user=admin password=quest host=127.0.0.1 port=8812 dbname=qdb' with pg . connect ( conn_str , autocommit = True ) as connection : # Open a cursor to perform database operations with connection . cursor ( ) as cur : #Query the database and obtain data as Python objects. cur . execute ( 'SELECT * FROM trades_pg;' ) records = cur . fetchall ( ) for row in records : print ( row ) # the connection is now closed\n\nimport psycopg as pg import time # Connect to an existing QuestDB instance conn_str = 'user=admin password=quest host=127.0.0.1 port=8812 dbname=qdb' with pg . connect ( conn_str , autocommit = True ) as connection : # Open a cursor to perform database operations with connection . cursor ( ) as cur : #Query the database and obtain data as Python objects. cur . execute ( 'SEL"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "r , autocommit = True ) as connection : # Open a cursor to perform database operations with connection . cursor ( ) as cur : #Query the database and obtain data as Python objects. cur . execute ( 'SELECT * FROM trades_pg;' ) records = cur . fetchall ( ) for row in records : print ( row ) # the connection is now closed\n\nimport psycopg as pg import time # Connect to an existing QuestDB instance conn_str = 'user=admin password=quest host=127.0.0.1 port=8812 dbname=qdb' with pg . connect ( conn_str , autocommit = True ) as connection : # Open a cursor to perform database operations with connection . cursor ( ) as cur : #Query the database and obtain data as Python objects. cur . execute ( 'SELECT * FROM trades_pg;' ) records = cur . fetchall ( ) for row in records : print ( row ) # the connection is now closed\n\nimport psycopg as pg import time # Connect to an existing QuestDB instance conn_str = 'user=admin password=quest host=127.0.0.1 port=8812 dbname=qdb' with pg . connect ( conn_str , autocommit = True ) as connection : # Open a cursor to perform database operations with connection . cursor ( ) as cur : #Query the database and obtain data as Python objects. cur . execute ( 'SELECT"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "autocommit = True ) as connection : # Open a cursor to perform database operations with connection . cursor ( ) as cur : #Query the database and obtain data as Python objects. cur . execute ( 'SELECT * FROM trades_pg;' ) records = cur . fetchall ( ) for row in records : print ( row ) # the connection is now closed\n\npackage com.myco; import java.sql.*; import java.util.Properties; public class App { public static void main(String[] args) throws SQLException { Properties properties = new Properties(); properties.setProperty(\"user\", \"admin\"); properties.setProperty(\"password\", \"quest\"); properties.setProperty(\"sslmode\", \"disable\"); final Connection connection = DriverManager.getConnection( \"jdbc:postgresql://localhost:8812/qdb\", properties); try (PreparedStatement preparedStatement = connection.prepareStatement( \"SELECT x FROM long_sequence(5);\")) { try (ResultSet rs = preparedStatement.executeQuery()) { while (rs.next()) { System.out.println(rs.getLong(1)); } } } connection.close(); } }\n\npackage com.myco; import java.sql.*; import java.util.Properties; public class App { public static void main(String[] args) throws SQLException { Properties properties = new Properties(); properties."}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "package com.myco; import java.sql.*; import java.util.Properties; public class App { public static void main(String[] args) throws SQLException { Properties properties = new Properties(); properties.setProperty(\"user\", \"admin\"); properties.setProperty(\"password\", \"quest\"); properties.setProperty(\"sslmode\", \"disable\"); final Connection connection = DriverManager.getConnection( \"jdbc:postgresql://localhost:8812/qdb\", properties); try (PreparedStatement preparedStatement = connection.prepareStatement( \"SELECT x FROM long_sequence(5);\")) { try (ResultSet rs = preparedStatement.executeQuery()) { while (rs.next()) { System.out.println(rs.getLong(1)); } } } connection.close(); } }\n\npackage com.myco; import java.sql.*; import java.util.Properties; public class App { public static void main(String[] args) throws SQLException { Properties properties = new Properties(); properties.setProperty(\"user\", \"admin\"); properties.setProperty(\"password\", \"quest\"); properties.setProperty(\"sslmode\", \"disable\"); final Connection connection = DriverManager.getConnection( \"jdbc:postgresql://localhost:8812/qdb\", properties); try (PreparedStatement preparedStatement = connection.prepareStatement( \"SELECT x F"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "e\"); final Connection connection = DriverManager.getConnection( \"jdbc:postgresql://localhost:8812/qdb\", properties); try (PreparedStatement preparedStatement = connection.prepareStatement( \"SELECT x FROM long_sequence(5);\")) { try (ResultSet rs = preparedStatement.executeQuery()) { while (rs.next()) { System.out.println(rs.getLong(1)); } } } connection.close(); } }\n\npackage com.myco; import java.sql.*; import java.util.Properties; public class App { public static void main(String[] args) throws SQLException { Properties properties = new Properties(); properties.setProperty(\"user\", \"admin\"); properties.setProperty(\"password\", \"quest\"); properties.setProperty(\"sslmode\", \"disable\"); final Connection connection = DriverManager.getConnection( \"jdbc:postgresql://localhost:8812/qdb\", properties); try (PreparedStatement preparedStatement = connection.prepareStatement( \"SELECT x FROM long_sequence(5);\")) { try (ResultSet rs = preparedStatement.executeQuery()) { while (rs.next()) { System.out.println(rs.getLong(1)); } } } connection.close(); } }\n\npackage com.myco; import java.sql.*; import java.util.Properties; public class App { public static void main(String[] args) throws SQLException { P"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "intln(rs.getLong(1)); } } } connection.close(); } }\n\npackage com.myco; import java.sql.*; import java.util.Properties; public class App { public static void main(String[] args) throws SQLException { Properties properties = new Properties(); properties.setProperty(\"user\", \"admin\"); properties.setProperty(\"password\", \"quest\"); properties.setProperty(\"sslmode\", \"disable\"); final Connection connection = DriverManager.getConnection( \"jdbc:postgresql://localhost:8812/qdb\", properties); try (PreparedStatement preparedStatement = connection.prepareStatement( \"SELECT x FROM long_sequence(5);\")) { try (ResultSet rs = preparedStatement.executeQuery()) { while (rs.next()) { System.out.println(rs.getLong(1)); } } } connection.close(); } }\n\n\"use strict\" const { Client } = require ( \"pg\" ) const start = async ( ) => { const client = new Client ( { database : \"qdb\" , host : \"127.0.0.1\" , password : \"quest\" , port : 8812 , user : \"admin\" , } ) await client . connect ( ) const res = await client . query ( \"SELECT x FROM long_sequence(5);\" ) console . log ( res . rows ) await client . end ( ) } start ( ) . catch ( console . error )\n\n\"use strict\" const { Client } = require ( \"pg\" ) const start = async"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "( \"SELECT x FROM long_sequence(5);\" ) console . log ( res . rows ) await client . end ( ) } start ( ) . catch ( console . error )\n\n\"use strict\" const { Client } = require ( \"pg\" ) const start = async ( ) => { const client = new Client ( { database : \"qdb\" , host : \"127.0.0.1\" , password : \"quest\" , port : 8812 , user : \"admin\" , } ) await client . connect ( ) const res = await client . query ( \"SELECT x FROM long_sequence(5);\" ) console . log ( res . rows ) await client . end ( ) } start ( ) . catch ( console . error )\n\n\"use strict\" const { Client } = require ( \"pg\" ) const start = async ( ) => { const client = new Client ( { database : \"qdb\" , host : \"127.0.0.1\" , password : \"quest\" , port : 8812 , user : \"admin\" , } ) await client . connect ( ) const res = await client . query ( \"SELECT x FROM long_sequence(5);\" ) console . log ( res . rows ) await client . end ( ) } start ( ) . catch ( console . error )\n\n\"use strict\" const { Client } = require ( \"pg\" ) const start = async ( ) => { const client = new Client ( { database : \"qdb\" , host : \"127.0.0.1\" , password : \"quest\" , port : 8812 , user : \"admin\" , } ) await client . connect ( ) const res = await client . query ( \"SELECT x FR"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "const client = new Client ( { database : \"qdb\" , host : \"127.0.0.1\" , password : \"quest\" , port : 8812 , user : \"admin\" , } ) await client . connect ( ) const res = await client . query ( \"SELECT x FROM long_sequence(5);\" ) console . log ( res . rows ) await client . end ( ) } start ( ) . catch ( console . error )\n\n\"use strict\" const { Client } = require ( \"pg\" ) const start = async ( ) => { const client = new Client ( { database : \"qdb\" , host : \"127.0.0.1\" , password : \"quest\" , port : 8812 , user : \"admin\" , } ) await client . connect ( ) const res = await client . query ( \"SELECT x FROM long_sequence(5);\" ) console . log ( res . rows ) await client . end ( ) } start ( ) . catch ( console . error )\n\npackage main import ( \"database/sql\" \"fmt\" _ \"github.com/lib/pq\" ) const ( host = \"localhost\" port = 8812 user = \"admin\" password = \"quest\" dbname = \"qdb\" ) func main ( ) { connStr := fmt . Sprintf ( \"host=%s port=%d user=%s password=%s dbname=%s sslmode=disable\" , host , port , user , password , dbname ) db , err := sql . Open ( \"postgres\" , connStr ) checkErr ( err ) defer db . Close ( ) stmt , err := db . Prepare ( \"SELECT x FROM long_sequence(5);\" ) checkErr ( err ) defer stmt ."}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "assword , dbname ) db , err := sql . Open ( \"postgres\" , connStr ) checkErr ( err ) defer db . Close ( ) stmt , err := db . Prepare ( \"SELECT x FROM long_sequence(5);\" ) checkErr ( err ) defer stmt . Close ( ) rows , err := stmt . Query ( ) checkErr ( err ) defer rows . Close ( ) var num string for rows . Next ( ) { err = rows . Scan ( & num ) checkErr ( err ) fmt . Println ( num ) } err = rows . Err ( ) checkErr ( err ) } func checkErr ( err error ) { if err != nil { panic ( err ) } }\n\npackage main import ( \"database/sql\" \"fmt\" _ \"github.com/lib/pq\" ) const ( host = \"localhost\" port = 8812 user = \"admin\" password = \"quest\" dbname = \"qdb\" ) func main ( ) { connStr := fmt . Sprintf ( \"host=%s port=%d user=%s password=%s dbname=%s sslmode=disable\" , host , port , user , password , dbname ) db , err := sql . Open ( \"postgres\" , connStr ) checkErr ( err ) defer db . Close ( ) stmt , err := db . Prepare ( \"SELECT x FROM long_sequence(5);\" ) checkErr ( err ) defer stmt . Close ( ) rows , err := stmt . Query ( ) checkErr ( err ) defer rows . Close ( ) var num string for rows . Next ( ) { err = rows . Scan ( & num ) checkErr ( err ) fmt . Println ( num ) } err = rows . Err ( ) checkErr ( e"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": ":= stmt . Query ( ) checkErr ( err ) defer rows . Close ( ) var num string for rows . Next ( ) { err = rows . Scan ( & num ) checkErr ( err ) fmt . Println ( num ) } err = rows . Err ( ) checkErr ( err ) } func checkErr ( err error ) { if err != nil { panic ( err ) } }\n\npackage main import ( \"database/sql\" \"fmt\" _ \"github.com/lib/pq\" ) const ( host = \"localhost\" port = 8812 user = \"admin\" password = \"quest\" dbname = \"qdb\" ) func main ( ) { connStr := fmt . Sprintf ( \"host=%s port=%d user=%s password=%s dbname=%s sslmode=disable\" , host , port , user , password , dbname ) db , err := sql . Open ( \"postgres\" , connStr ) checkErr ( err ) defer db . Close ( ) stmt , err := db . Prepare ( \"SELECT x FROM long_sequence(5);\" ) checkErr ( err ) defer stmt . Close ( ) rows , err := stmt . Query ( ) checkErr ( err ) defer rows . Close ( ) var num string for rows . Next ( ) { err = rows . Scan ( & num ) checkErr ( err ) fmt . Println ( num ) } err = rows . Err ( ) checkErr ( err ) } func checkErr ( err error ) { if err != nil { panic ( err ) } }\n\npackage main import ( \"database/sql\" \"fmt\" _ \"github.com/lib/pq\" ) const ( host = \"localhost\" port = 8812 user = \"admin\" password = \"quest\" dbname ="}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "( err error ) { if err != nil { panic ( err ) } }\n\npackage main import ( \"database/sql\" \"fmt\" _ \"github.com/lib/pq\" ) const ( host = \"localhost\" port = 8812 user = \"admin\" password = \"quest\" dbname = \"qdb\" ) func main ( ) { connStr := fmt . Sprintf ( \"host=%s port=%d user=%s password=%s dbname=%s sslmode=disable\" , host , port , user , password , dbname ) db , err := sql . Open ( \"postgres\" , connStr ) checkErr ( err ) defer db . Close ( ) stmt , err := db . Prepare ( \"SELECT x FROM long_sequence(5);\" ) checkErr ( err ) defer stmt . Close ( ) rows , err := stmt . Query ( ) checkErr ( err ) defer rows . Close ( ) var num string for rows . Next ( ) { err = rows . Scan ( & num ) checkErr ( err ) fmt . Println ( num ) } err = rows . Err ( ) checkErr ( err ) } func checkErr ( err error ) { if err != nil { panic ( err ) } }\n\npackage main import ( \"database/sql\" \"fmt\" _ \"github.com/lib/pq\" ) const ( host = \"localhost\" port = 8812 user = \"admin\" password = \"quest\" dbname = \"qdb\" ) func main ( ) { connStr := fmt . Sprintf ( \"host=%s port=%d user=%s password=%s dbname=%s sslmode=disable\" , host , port , user , password , dbname ) db , err := sql . Open ( \"postgres\" , connStr ) checkErr ( er"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": ") { connStr := fmt . Sprintf ( \"host=%s port=%d user=%s password=%s dbname=%s sslmode=disable\" , host , port , user , password , dbname ) db , err := sql . Open ( \"postgres\" , connStr ) checkErr ( err ) defer db . Close ( ) stmt , err := db . Prepare ( \"SELECT x FROM long_sequence(5);\" ) checkErr ( err ) defer stmt . Close ( ) rows , err := stmt . Query ( ) checkErr ( err ) defer rows . Close ( ) var num string for rows . Next ( ) { err = rows . Scan ( & num ) checkErr ( err ) fmt . Println ( num ) } err = rows . Err ( ) checkErr ( err ) } func checkErr ( err error ) { if err != nil { panic ( err ) } }\n\n// compile with // g++ libpq_example.c -o libpq_example.exe -I pgsql\\include -L dev\\pgsql\\lib // -std=c++17 -lpthread -lpq # include <libpq-fe.h> # include <stdio.h> # include <stdlib.h> void do_exit ( PGconn * conn ) { PQfinish ( conn ) ; exit ( 1 ) ; } int main ( ) { PGconn * conn = PQconnectdb ( \"host=localhost user=admin password=quest port=8812 dbname=testdb\" ) ; if ( PQstatus ( conn ) == CONNECTION_BAD ) { fprintf ( stderr , \"Connection to database failed: %s\\n\" , PQerrorMessage ( conn ) ) ; do_exit ( conn ) ; } PGresult * res = PQexec ( conn , \"SELECT x FROM long_sequence(5)"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "onn ) == CONNECTION_BAD ) { fprintf ( stderr , \"Connection to database failed: %s\\n\" , PQerrorMessage ( conn ) ) ; do_exit ( conn ) ; } PGresult * res = PQexec ( conn , \"SELECT x FROM long_sequence(5);\" ) ; if ( PQresultStatus ( res ) != PGRES_TUPLES_OK ) { printf ( \"No data retrieved\\n\" ) ; PQclear ( res ) ; do_exit ( conn ) ; } int rows = PQntuples ( res ) ; for ( int i = 0 ; i < rows ; i ++ ) { printf ( \"%s\\n\" , PQgetvalue ( res , i , 0 ) ) ; } PQclear ( res ) ; PQfinish ( conn ) ; return 0 ; }\n\n// compile with // g++ libpq_example.c -o libpq_example.exe -I pgsql\\include -L dev\\pgsql\\lib // -std=c++17 -lpthread -lpq # include <libpq-fe.h> # include <stdio.h> # include <stdlib.h> void do_exit ( PGconn * conn ) { PQfinish ( conn ) ; exit ( 1 ) ; } int main ( ) { PGconn * conn = PQconnectdb ( \"host=localhost user=admin password=quest port=8812 dbname=testdb\" ) ; if ( PQstatus ( conn ) == CONNECTION_BAD ) { fprintf ( stderr , \"Connection to database failed: %s\\n\" , PQerrorMessage ( conn ) ) ; do_exit ( conn ) ; } PGresult * res = PQexec ( conn , \"SELECT x FROM long_sequence(5);\" ) ; if ( PQresultStatus ( res ) != PGRES_TUPLES_OK ) { printf ( \"No data retrieved\\n\" ) ; PQclear ( res )"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": ") ) ; do_exit ( conn ) ; } PGresult * res = PQexec ( conn , \"SELECT x FROM long_sequence(5);\" ) ; if ( PQresultStatus ( res ) != PGRES_TUPLES_OK ) { printf ( \"No data retrieved\\n\" ) ; PQclear ( res ) ; do_exit ( conn ) ; } int rows = PQntuples ( res ) ; for ( int i = 0 ; i < rows ; i ++ ) { printf ( \"%s\\n\" , PQgetvalue ( res , i , 0 ) ) ; } PQclear ( res ) ; PQfinish ( conn ) ; return 0 ; }\n\n// compile with // g++ libpq_example.c -o libpq_example.exe -I pgsql\\include -L dev\\pgsql\\lib // -std=c++17 -lpthread -lpq # include <libpq-fe.h> # include <stdio.h> # include <stdlib.h> void do_exit ( PGconn * conn ) { PQfinish ( conn ) ; exit ( 1 ) ; } int main ( ) { PGconn * conn = PQconnectdb ( \"host=localhost user=admin password=quest port=8812 dbname=testdb\" ) ; if ( PQstatus ( conn ) == CONNECTION_BAD ) { fprintf ( stderr , \"Connection to database failed: %s\\n\" , PQerrorMessage ( conn ) ) ; do_exit ( conn ) ; } PGresult * res = PQexec ( conn , \"SELECT x FROM long_sequence(5);\" ) ; if ( PQresultStatus ( res ) != PGRES_TUPLES_OK ) { printf ( \"No data retrieved\\n\" ) ; PQclear ( res ) ; do_exit ( conn ) ; } int rows = PQntuples ( res ) ; for ( int i = 0 ; i < rows ; i ++ ) { printf ( \"%s\\n\""}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "sultStatus ( res ) != PGRES_TUPLES_OK ) { printf ( \"No data retrieved\\n\" ) ; PQclear ( res ) ; do_exit ( conn ) ; } int rows = PQntuples ( res ) ; for ( int i = 0 ; i < rows ; i ++ ) { printf ( \"%s\\n\" , PQgetvalue ( res , i , 0 ) ) ; } PQclear ( res ) ; PQfinish ( conn ) ; return 0 ; }\n\n// compile with // g++ libpq_example.c -o libpq_example.exe -I pgsql\\include -L dev\\pgsql\\lib // -std=c++17 -lpthread -lpq # include <libpq-fe.h> # include <stdio.h> # include <stdlib.h> void do_exit ( PGconn * conn ) { PQfinish ( conn ) ; exit ( 1 ) ; } int main ( ) { PGconn * conn = PQconnectdb ( \"host=localhost user=admin password=quest port=8812 dbname=testdb\" ) ; if ( PQstatus ( conn ) == CONNECTION_BAD ) { fprintf ( stderr , \"Connection to database failed: %s\\n\" , PQerrorMessage ( conn ) ) ; do_exit ( conn ) ; } PGresult * res = PQexec ( conn , \"SELECT x FROM long_sequence(5);\" ) ; if ( PQresultStatus ( res ) != PGRES_TUPLES_OK ) { printf ( \"No data retrieved\\n\" ) ; PQclear ( res ) ; do_exit ( conn ) ; } int rows = PQntuples ( res ) ; for ( int i = 0 ; i < rows ; i ++ ) { printf ( \"%s\\n\" , PQgetvalue ( res , i , 0 ) ) ; } PQclear ( res ) ; PQfinish ( conn ) ; return 0 ; }\n\n// compile with // g"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "n ) ; } int rows = PQntuples ( res ) ; for ( int i = 0 ; i < rows ; i ++ ) { printf ( \"%s\\n\" , PQgetvalue ( res , i , 0 ) ) ; } PQclear ( res ) ; PQfinish ( conn ) ; return 0 ; }\n\n// compile with // g++ libpq_example.c -o libpq_example.exe -I pgsql\\include -L dev\\pgsql\\lib // -std=c++17 -lpthread -lpq # include <libpq-fe.h> # include <stdio.h> # include <stdlib.h> void do_exit ( PGconn * conn ) { PQfinish ( conn ) ; exit ( 1 ) ; } int main ( ) { PGconn * conn = PQconnectdb ( \"host=localhost user=admin password=quest port=8812 dbname=testdb\" ) ; if ( PQstatus ( conn ) == CONNECTION_BAD ) { fprintf ( stderr , \"Connection to database failed: %s\\n\" , PQerrorMessage ( conn ) ) ; do_exit ( conn ) ; } PGresult * res = PQexec ( conn , \"SELECT x FROM long_sequence(5);\" ) ; if ( PQresultStatus ( res ) != PGRES_TUPLES_OK ) { printf ( \"No data retrieved\\n\" ) ; PQclear ( res ) ; do_exit ( conn ) ; } int rows = PQntuples ( res ) ; for ( int i = 0 ; i < rows ; i ++ ) { printf ( \"%s\\n\" , PQgetvalue ( res , i , 0 ) ) ; } PQclear ( res ) ; PQfinish ( conn ) ; return 0 ; }\n\nusing Npgsql; string username = \"admin\"; string password = \"quest\"; string database = \"qdb\"; int port = 8812; var connectionStri"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "res , i , 0 ) ) ; } PQclear ( res ) ; PQfinish ( conn ) ; return 0 ; }\n\nusing Npgsql; string username = \"admin\"; string password = \"quest\"; string database = \"qdb\"; int port = 8812; var connectionString = $@\"host=localhost;port={port};username={username};password={password}; database={database};ServerCompatibilityMode=NoTypeLoading;\"; await using NpgsqlConnection connection = new NpgsqlConnection(connectionString); await connection.OpenAsync(); var sql = \"SELECT x FROM long_sequence(5);\"; await using NpgsqlCommand command = new NpgsqlCommand(sql, connection); await using (var reader = await command.ExecuteReaderAsync()) { while (await reader.ReadAsync()) { var x = reader.GetInt64(0); } }\n\nusing Npgsql; string username = \"admin\"; string password = \"quest\"; string database = \"qdb\"; int port = 8812; var connectionString = $@\"host=localhost;port={port};username={username};password={password}; database={database};ServerCompatibilityMode=NoTypeLoading;\"; await using NpgsqlConnection connection = new NpgsqlConnection(connectionString); await connection.OpenAsync(); var sql = \"SELECT x FROM long_sequence(5);\"; await using NpgsqlCommand command = new NpgsqlCommand(sql, connection); await us"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "on = new NpgsqlConnection(connectionString); await connection.OpenAsync(); var sql = \"SELECT x FROM long_sequence(5);\"; await using NpgsqlCommand command = new NpgsqlCommand(sql, connection); await using (var reader = await command.ExecuteReaderAsync()) { while (await reader.ReadAsync()) { var x = reader.GetInt64(0); } }\n\nusing Npgsql; string username = \"admin\"; string password = \"quest\"; string database = \"qdb\"; int port = 8812; var connectionString = $@\"host=localhost;port={port};username={username};password={password}; database={database};ServerCompatibilityMode=NoTypeLoading;\"; await using NpgsqlConnection connection = new NpgsqlConnection(connectionString); await connection.OpenAsync(); var sql = \"SELECT x FROM long_sequence(5);\"; await using NpgsqlCommand command = new NpgsqlCommand(sql, connection); await using (var reader = await command.ExecuteReaderAsync()) { while (await reader.ReadAsync()) { var x = reader.GetInt64(0); } }\n\nusing Npgsql; string username = \"admin\"; string password = \"quest\"; string database = \"qdb\"; int port = 8812; var connectionString = $@\"host=localhost;port={port};username={username};password={password}; database={database};ServerCompatibilityMode=No"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "ssword = \"quest\"; string database = \"qdb\"; int port = 8812; var connectionString = $@\"host=localhost;port={port};username={username};password={password}; database={database};ServerCompatibilityMode=NoTypeLoading;\"; await using NpgsqlConnection connection = new NpgsqlConnection(connectionString); await connection.OpenAsync(); var sql = \"SELECT x FROM long_sequence(5);\"; await using NpgsqlCommand command = new NpgsqlCommand(sql, connection); await using (var reader = await command.ExecuteReaderAsync()) { while (await reader.ReadAsync()) { var x = reader.GetInt64(0); } }\n\nusing Npgsql; string username = \"admin\"; string password = \"quest\"; string database = \"qdb\"; int port = 8812; var connectionString = $@\"host=localhost;port={port};username={username};password={password}; database={database};ServerCompatibilityMode=NoTypeLoading;\"; await using NpgsqlConnection connection = new NpgsqlConnection(connectionString); await connection.OpenAsync(); var sql = \"SELECT x FROM long_sequence(5);\"; await using NpgsqlCommand command = new NpgsqlCommand(sql, connection); await using (var reader = await command.ExecuteReaderAsync()) { while (await reader.ReadAsync()) { var x = reader.GetInt64(0); } }"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "ait using NpgsqlCommand command = new NpgsqlCommand(sql, connection); await using (var reader = await command.ExecuteReaderAsync()) { while (await reader.ReadAsync()) { var x = reader.GetInt64(0); } }\n\nrequire 'pg' begin conn =PG.connect( host: \"127.0.0.1\", port: 8812, dbname: 'qdb', user: 'admin', password: 'quest' ) rows = conn.exec 'SELECT x FROM long_sequence(5);' rows.each do |row| puts row end rescue PG::Error => e puts e.message ensure conn.close if conn end\n\nrequire 'pg' begin conn =PG.connect( host: \"127.0.0.1\", port: 8812, dbname: 'qdb', user: 'admin', password: 'quest' ) rows = conn.exec 'SELECT x FROM long_sequence(5);' rows.each do |row| puts row end rescue PG::Error => e puts e.message ensure conn.close if conn end\n\nrequire 'pg' begin conn =PG.connect( host: \"127.0.0.1\", port: 8812, dbname: 'qdb', user: 'admin', password: 'quest' ) rows = conn.exec 'SELECT x FROM long_sequence(5);' rows.each do |row| puts row end rescue PG::Error => e puts e.message ensure conn.close if conn end\n\nrequire 'pg' begin conn =PG.connect( host: \"127.0.0.1\", port: 8812, dbname: 'qdb', user: 'admin', password: 'quest' ) rows = conn.exec 'SELECT x FROM long_sequence(5);' rows.each do |row| put"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "onn end\n\nrequire 'pg' begin conn =PG.connect( host: \"127.0.0.1\", port: 8812, dbname: 'qdb', user: 'admin', password: 'quest' ) rows = conn.exec 'SELECT x FROM long_sequence(5);' rows.each do |row| puts row end rescue PG::Error => e puts e.message ensure conn.close if conn end\n\nrequire 'pg' begin conn =PG.connect( host: \"127.0.0.1\", port: 8812, dbname: 'qdb', user: 'admin', password: 'quest' ) rows = conn.exec 'SELECT x FROM long_sequence(5);' rows.each do |row| puts row end rescue PG::Error => e puts e.message ensure conn.close if conn end\n\n<?php function exceptions_error_handler($severity, $message, $filename, $lineno) { throw new ErrorException($message, 0, $severity, $filename, $lineno); } set_error_handler('exceptions_error_handler'); $db_conn = null; try { $db_conn = pg_connect(\" host = 'localhost' port=8812 dbname = 'qdb' user = 'admin' password = 'quest' \"); $result = pg_query($db_conn, 'SELECT x FROM long_sequence(5);' ); while ($row = pg_fetch_assoc($result) ){ print_r($row); } pg_free_result($result); } catch (Exception $e) { echo 'Caught exception: ', $e->getMessage(), \"\\n\"; } finally { if (!is_null($db_conn)) { pg_close($db_conn); } } ?>\n\n<?php function exceptions_error"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "} pg_free_result($result); } catch (Exception $e) { echo 'Caught exception: ', $e->getMessage(), \"\\n\"; } finally { if (!is_null($db_conn)) { pg_close($db_conn); } } ?>\n\n<?php function exceptions_error_handler($severity, $message, $filename, $lineno) { throw new ErrorException($message, 0, $severity, $filename, $lineno); } set_error_handler('exceptions_error_handler'); $db_conn = null; try { $db_conn = pg_connect(\" host = 'localhost' port=8812 dbname = 'qdb' user = 'admin' password = 'quest' \"); $result = pg_query($db_conn, 'SELECT x FROM long_sequence(5);' ); while ($row = pg_fetch_assoc($result) ){ print_r($row); } pg_free_result($result); } catch (Exception $e) { echo 'Caught exception: ', $e->getMessage(), \"\\n\"; } finally { if (!is_null($db_conn)) { pg_close($db_conn); } } ?>\n\n<?php function exceptions_error_handler($severity, $message, $filename, $lineno) { throw new ErrorException($message, 0, $severity, $filename, $lineno); } set_error_handler('exceptions_error_handler'); $db_conn = null; try { $db_conn = pg_connect(\" host = 'localhost' port=8812 dbname = 'qdb' user = 'admin' password = 'quest' \"); $result = pg_query($db_conn, 'SELECT x FROM long_sequence(5);' ); while ($row"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "n = null; try { $db_conn = pg_connect(\" host = 'localhost' port=8812 dbname = 'qdb' user = 'admin' password = 'quest' \"); $result = pg_query($db_conn, 'SELECT x FROM long_sequence(5);' ); while ($row = pg_fetch_assoc($result) ){ print_r($row); } pg_free_result($result); } catch (Exception $e) { echo 'Caught exception: ', $e->getMessage(), \"\\n\"; } finally { if (!is_null($db_conn)) { pg_close($db_conn); } } ?>\n\n<?php function exceptions_error_handler($severity, $message, $filename, $lineno) { throw new ErrorException($message, 0, $severity, $filename, $lineno); } set_error_handler('exceptions_error_handler'); $db_conn = null; try { $db_conn = pg_connect(\" host = 'localhost' port=8812 dbname = 'qdb' user = 'admin' password = 'quest' \"); $result = pg_query($db_conn, 'SELECT x FROM long_sequence(5);' ); while ($row = pg_fetch_assoc($result) ){ print_r($row); } pg_free_result($result); } catch (Exception $e) { echo 'Caught exception: ', $e->getMessage(), \"\\n\"; } finally { if (!is_null($db_conn)) { pg_close($db_conn); } } ?>\n\n<?php function exceptions_error_handler($severity, $message, $filename, $lineno) { throw new ErrorException($message, 0, $severity, $filename, $lineno); } set_error_"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "PostgreSQL ​", "text": "nn)) { pg_close($db_conn); } } ?>\n\n<?php function exceptions_error_handler($severity, $message, $filename, $lineno) { throw new ErrorException($message, 0, $severity, $filename, $lineno); } set_error_handler('exceptions_error_handler'); $db_conn = null; try { $db_conn = pg_connect(\" host = 'localhost' port=8812 dbname = 'qdb' user = 'admin' password = 'quest' \"); $result = pg_query($db_conn, 'SELECT x FROM long_sequence(5);' ); while ($row = pg_fetch_assoc($result) ){ print_r($row); } pg_free_result($result); } catch (Exception $e) { echo 'Caught exception: ', $e->getMessage(), \"\\n\"; } finally { if (!is_null($db_conn)) { pg_close($db_conn); } } ?>\n\nSee the PGWire Client overview for more details on how to use PostgreSQL clients to connect to QuestDB."}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": "QuestDB exposes a REST API for compatibility with a wide range of libraries and tools.\n\nThe REST API is accessible on port 9000 and has the following query-capable entrypoints:\n\n9000\n\nFor details such as content type, query parameters and more, refer to the REST HTTP API reference.\n\n/exp?query=..\n\n/exec?query=..\n\n/exp\n\nThe /exp entrypoint allows querying the database with a SQL select query and obtaining the results as CSV.\n\n/exp\n\nFor obtaining results in JSON, use /exec instead, documented next.\n\n/exec\n\ncURL Python curl -G --data-urlencode \\ \"query=SELECT * FROM example_table2 LIMIT 3\" \\ http://localhost:9000/exp \"col1\",\"col2\",\"col3\" \"a\",10.5,true \"b\",100.0,false \"c\",,true import requests resp = requests . get ( 'http://localhost:9000/exp' , { 'query' : 'SELECT * FROM example_table2' , 'limit' : '3,6' # Rows 3, 4, 5 } ) print ( resp . text ) \"col1\",\"col2\",\"col3\" \"d\",20.5,true \"e\",200.0,false \"f\",,true\n\ncURL\n\nPython\n\ncurl -G --data-urlencode \\ \"query=SELECT * FROM example_table2 LIMIT 3\" \\ http://localhost:9000/exp \"col1\",\"col2\",\"col3\" \"a\",10.5,true \"b\",100.0,false \"c\",,true import requests resp = requests . get ( 'http://localhost:9000/exp' , { 'query' : 'SELECT * FROM example_tab"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": "3\" \\ http://localhost:9000/exp \"col1\",\"col2\",\"col3\" \"a\",10.5,true \"b\",100.0,false \"c\",,true import requests resp = requests . get ( 'http://localhost:9000/exp' , { 'query' : 'SELECT * FROM example_table2' , 'limit' : '3,6' # Rows 3, 4, 5 } ) print ( resp . text ) \"col1\",\"col2\",\"col3\" \"d\",20.5,true \"e\",200.0,false \"f\",,true\n\ncurl -G --data-urlencode \\ \"query=SELECT * FROM example_table2 LIMIT 3\" \\ http://localhost:9000/exp \"col1\",\"col2\",\"col3\" \"a\",10.5,true \"b\",100.0,false \"c\",,true\n\ncurl -G --data-urlencode \\ \"query=SELECT * FROM example_table2 LIMIT 3\" \\ http://localhost:9000/exp\n\ncurl -G --data-urlencode \\ \"query=SELECT * FROM example_table2 LIMIT 3\" \\ http://localhost:9000/exp\n\ncurl -G --data-urlencode \\ \"query=SELECT * FROM example_table2 LIMIT 3\" \\ http://localhost:9000/exp\n\ncurl -G --data-urlencode \\ \"query=SELECT * FROM example_table2 LIMIT 3\" \\ http://localhost:9000/exp\n\n\"col1\",\"col2\",\"col3\" \"a\",10.5,true \"b\",100.0,false \"c\",,true\n\n\"col1\",\"col2\",\"col3\" \"a\",10.5,true \"b\",100.0,false \"c\",,true\n\n\"col1\",\"col2\",\"col3\" \"a\",10.5,true \"b\",100.0,false \"c\",,true\n\n\"col1\",\"col2\",\"col3\" \"a\",10.5,true \"b\",100.0,false \"c\",,true\n\nimport requests resp = requests . get ( 'http://localhost:90"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": "alse \"c\",,true\n\n\"col1\",\"col2\",\"col3\" \"a\",10.5,true \"b\",100.0,false \"c\",,true\n\n\"col1\",\"col2\",\"col3\" \"a\",10.5,true \"b\",100.0,false \"c\",,true\n\nimport requests resp = requests . get ( 'http://localhost:9000/exp' , { 'query' : 'SELECT * FROM example_table2' , 'limit' : '3,6' # Rows 3, 4, 5 } ) print ( resp . text ) \"col1\",\"col2\",\"col3\" \"d\",20.5,true \"e\",200.0,false \"f\",,true\n\nimport requests resp = requests . get ( 'http://localhost:9000/exp' , { 'query' : 'SELECT * FROM example_table2' , 'limit' : '3,6' # Rows 3, 4, 5 } ) print ( resp . text )\n\nimport requests resp = requests . get ( 'http://localhost:9000/exp' , { 'query' : 'SELECT * FROM example_table2' , 'limit' : '3,6' # Rows 3, 4, 5 } ) print ( resp . text )\n\nimport requests resp = requests . get ( 'http://localhost:9000/exp' , { 'query' : 'SELECT * FROM example_table2' , 'limit' : '3,6' # Rows 3, 4, 5 } ) print ( resp . text )\n\nimport requests resp = requests . get ( 'http://localhost:9000/exp' , { 'query' : 'SELECT * FROM example_table2' , 'limit' : '3,6' # Rows 3, 4, 5 } ) print ( resp . text )\n\n\"col1\",\"col2\",\"col3\" \"d\",20.5,true \"e\",200.0,false \"f\",,true\n\n\"col1\",\"col2\",\"col3\" \"d\",20.5,true \"e\",200.0,false \"f\",,true\n\n\"col1\",\"co"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": "ble2' , 'limit' : '3,6' # Rows 3, 4, 5 } ) print ( resp . text )\n\n\"col1\",\"col2\",\"col3\" \"d\",20.5,true \"e\",200.0,false \"f\",,true\n\n\"col1\",\"col2\",\"col3\" \"d\",20.5,true \"e\",200.0,false \"f\",,true\n\n\"col1\",\"col2\",\"col3\" \"d\",20.5,true \"e\",200.0,false \"f\",,true\n\n\"col1\",\"col2\",\"col3\" \"d\",20.5,true \"e\",200.0,false \"f\",,true\n\n/exec\n\nThe /exec entrypoint takes a SQL query and returns results as JSON.\n\n/exec\n\nThis is similar to the /exp entry point which returns results as CSV.\n\n/exp\n\ncURL Python NodeJS Go curl -G \\ --data-urlencode \"query=SELECT x FROM long_sequence(5);\" \\ http://localhost:9000/exec The JSON response contains the original query, a \"columns\" key with the schema of the results, a \"count\" number of rows and a \"dataset\" with the results. { \"query\" : \"SELECT x FROM long_sequence(5);\" , \"columns\" : [ { \"name\" : \"x\" , \"type\" : \"LONG\" } ] , \"dataset\" : [ [ 1 ] , [ 2 ] , [ 3 ] , [ 4 ] , [ 5 ] ] , \"count\" : 5 } import sys import requests host = 'http://localhost:9000' sql_query = \"select * from long_sequence(10)\" try : response = requests . get ( host + '/exec' , params = { 'query' : sql_query } ) . json ( ) for row in response [ 'dataset' ] : print ( row [ 0 ] ) except requests . exceptio"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": "m long_sequence(10)\" try : response = requests . get ( host + '/exec' , params = { 'query' : sql_query } ) . json ( ) for row in response [ 'dataset' ] : print ( row [ 0 ] ) except requests . exceptions . RequestException as e : print ( f'Error: { e } ' , file = sys . stderr ) const fetch = require ( \"node-fetch\" ) const HOST = \"http://localhost:9000\" async function run ( ) { try { const query = \"SELECT x FROM long_sequence(5);\" const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) const json = await response . json ( ) console . log ( json ) } catch ( error ) { console . log ( error ) } } run ( ) package main import ( \"fmt\" \"io/ioutil\" \"log\" \"net/http\" \"net/url\" ) func main ( ) { u , err := url . Parse ( \"http://localhost:9000\" ) checkErr ( err ) u . Path += \"exec\" params := url . Values { } params . Add ( \"query\" , \"SELECT x FROM long_sequence(5);\" ) u . RawQuery = params . Encode ( ) url := fmt . Sprintf ( \"%v\" , u ) res , err := http . Get ( url ) checkErr ( err ) defer res . Body . Close ( ) body , err := ioutil . ReadAll ( res . Body ) checkErr ( err ) log . Println ( string ( body ) ) } func checkErr ( err error ) { if err != nil { p"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": "( url ) checkErr ( err ) defer res . Body . Close ( ) body , err := ioutil . ReadAll ( res . Body ) checkErr ( err ) log . Println ( string ( body ) ) } func checkErr ( err error ) { if err != nil { panic ( err ) } }\n\ncURL\n\nPython\n\nNodeJS\n\nGo\n\ncurl -G \\ --data-urlencode \"query=SELECT x FROM long_sequence(5);\" \\ http://localhost:9000/exec The JSON response contains the original query, a \"columns\" key with the schema of the results, a \"count\" number of rows and a \"dataset\" with the results. { \"query\" : \"SELECT x FROM long_sequence(5);\" , \"columns\" : [ { \"name\" : \"x\" , \"type\" : \"LONG\" } ] , \"dataset\" : [ [ 1 ] , [ 2 ] , [ 3 ] , [ 4 ] , [ 5 ] ] , \"count\" : 5 } import sys import requests host = 'http://localhost:9000' sql_query = \"select * from long_sequence(10)\" try : response = requests . get ( host + '/exec' , params = { 'query' : sql_query } ) . json ( ) for row in response [ 'dataset' ] : print ( row [ 0 ] ) except requests . exceptions . RequestException as e : print ( f'Error: { e } ' , file = sys . stderr ) const fetch = require ( \"node-fetch\" ) const HOST = \"http://localhost:9000\" async function run ( ) { try { const query = \"SELECT x FROM long_sequence(5);\" const response = aw"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": "' , file = sys . stderr ) const fetch = require ( \"node-fetch\" ) const HOST = \"http://localhost:9000\" async function run ( ) { try { const query = \"SELECT x FROM long_sequence(5);\" const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) const json = await response . json ( ) console . log ( json ) } catch ( error ) { console . log ( error ) } } run ( ) package main import ( \"fmt\" \"io/ioutil\" \"log\" \"net/http\" \"net/url\" ) func main ( ) { u , err := url . Parse ( \"http://localhost:9000\" ) checkErr ( err ) u . Path += \"exec\" params := url . Values { } params . Add ( \"query\" , \"SELECT x FROM long_sequence(5);\" ) u . RawQuery = params . Encode ( ) url := fmt . Sprintf ( \"%v\" , u ) res , err := http . Get ( url ) checkErr ( err ) defer res . Body . Close ( ) body , err := ioutil . ReadAll ( res . Body ) checkErr ( err ) log . Println ( string ( body ) ) } func checkErr ( err error ) { if err != nil { panic ( err ) } }\n\ncurl -G \\ --data-urlencode \"query=SELECT x FROM long_sequence(5);\" \\ http://localhost:9000/exec The JSON response contains the original query, a \"columns\" key with the schema of the results, a \"count\" number of rows and a \"dataset\" wi"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": "SELECT x FROM long_sequence(5);\" \\ http://localhost:9000/exec The JSON response contains the original query, a \"columns\" key with the schema of the results, a \"count\" number of rows and a \"dataset\" with the results. { \"query\" : \"SELECT x FROM long_sequence(5);\" , \"columns\" : [ { \"name\" : \"x\" , \"type\" : \"LONG\" } ] , \"dataset\" : [ [ 1 ] , [ 2 ] , [ 3 ] , [ 4 ] , [ 5 ] ] , \"count\" : 5 }\n\ncurl -G \\ --data-urlencode \"query=SELECT x FROM long_sequence(5);\" \\ http://localhost:9000/exec\n\ncurl -G \\ --data-urlencode \"query=SELECT x FROM long_sequence(5);\" \\ http://localhost:9000/exec\n\ncurl -G \\ --data-urlencode \"query=SELECT x FROM long_sequence(5);\" \\ http://localhost:9000/exec\n\ncurl -G \\ --data-urlencode \"query=SELECT x FROM long_sequence(5);\" \\ http://localhost:9000/exec\n\nThe JSON response contains the original query, a \"columns\" key with the schema of the results, a \"count\" number of rows and a \"dataset\" with the results.\n\n\"columns\"\n\n\"count\"\n\n\"dataset\"\n\n{ \"query\" : \"SELECT x FROM long_sequence(5);\" , \"columns\" : [ { \"name\" : \"x\" , \"type\" : \"LONG\" } ] , \"dataset\" : [ [ 1 ] , [ 2 ] , [ 3 ] , [ 4 ] , [ 5 ] ] , \"count\" : 5 }\n\n{ \"query\" : \"SELECT x FROM long_sequence(5);\" , \"columns\" : [ { \"n"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": "ce(5);\" , \"columns\" : [ { \"name\" : \"x\" , \"type\" : \"LONG\" } ] , \"dataset\" : [ [ 1 ] , [ 2 ] , [ 3 ] , [ 4 ] , [ 5 ] ] , \"count\" : 5 }\n\n{ \"query\" : \"SELECT x FROM long_sequence(5);\" , \"columns\" : [ { \"name\" : \"x\" , \"type\" : \"LONG\" } ] , \"dataset\" : [ [ 1 ] , [ 2 ] , [ 3 ] , [ 4 ] , [ 5 ] ] , \"count\" : 5 }\n\n{ \"query\" : \"SELECT x FROM long_sequence(5);\" , \"columns\" : [ { \"name\" : \"x\" , \"type\" : \"LONG\" } ] , \"dataset\" : [ [ 1 ] , [ 2 ] , [ 3 ] , [ 4 ] , [ 5 ] ] , \"count\" : 5 }\n\n{ \"query\" : \"SELECT x FROM long_sequence(5);\" , \"columns\" : [ { \"name\" : \"x\" , \"type\" : \"LONG\" } ] , \"dataset\" : [ [ 1 ] , [ 2 ] , [ 3 ] , [ 4 ] , [ 5 ] ] , \"count\" : 5 }\n\nimport sys import requests host = 'http://localhost:9000' sql_query = \"select * from long_sequence(10)\" try : response = requests . get ( host + '/exec' , params = { 'query' : sql_query } ) . json ( ) for row in response [ 'dataset' ] : print ( row [ 0 ] ) except requests . exceptions . RequestException as e : print ( f'Error: { e } ' , file = sys . stderr )\n\nimport sys import requests host = 'http://localhost:9000' sql_query = \"select * from long_sequence(10)\" try : response = requests . get ( host + '/exec' , params = { 'query' : sql_query }"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": ". stderr )\n\nimport sys import requests host = 'http://localhost:9000' sql_query = \"select * from long_sequence(10)\" try : response = requests . get ( host + '/exec' , params = { 'query' : sql_query } ) . json ( ) for row in response [ 'dataset' ] : print ( row [ 0 ] ) except requests . exceptions . RequestException as e : print ( f'Error: { e } ' , file = sys . stderr )\n\nimport sys import requests host = 'http://localhost:9000' sql_query = \"select * from long_sequence(10)\" try : response = requests . get ( host + '/exec' , params = { 'query' : sql_query } ) . json ( ) for row in response [ 'dataset' ] : print ( row [ 0 ] ) except requests . exceptions . RequestException as e : print ( f'Error: { e } ' , file = sys . stderr )\n\nimport sys import requests host = 'http://localhost:9000' sql_query = \"select * from long_sequence(10)\" try : response = requests . get ( host + '/exec' , params = { 'query' : sql_query } ) . json ( ) for row in response [ 'dataset' ] : print ( row [ 0 ] ) except requests . exceptions . RequestException as e : print ( f'Error: { e } ' , file = sys . stderr )\n\nimport sys import requests host = 'http://localhost:9000' sql_query = \"select * from long_sequence(10)"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": "requests . exceptions . RequestException as e : print ( f'Error: { e } ' , file = sys . stderr )\n\nimport sys import requests host = 'http://localhost:9000' sql_query = \"select * from long_sequence(10)\" try : response = requests . get ( host + '/exec' , params = { 'query' : sql_query } ) . json ( ) for row in response [ 'dataset' ] : print ( row [ 0 ] ) except requests . exceptions . RequestException as e : print ( f'Error: { e } ' , file = sys . stderr )\n\nconst fetch = require ( \"node-fetch\" ) const HOST = \"http://localhost:9000\" async function run ( ) { try { const query = \"SELECT x FROM long_sequence(5);\" const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) const json = await response . json ( ) console . log ( json ) } catch ( error ) { console . log ( error ) } } run ( )\n\nconst fetch = require ( \"node-fetch\" ) const HOST = \"http://localhost:9000\" async function run ( ) { try { const query = \"SELECT x FROM long_sequence(5);\" const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) const json = await response . json ( ) console . log ( json ) } catch ( error ) { console . log ( error ) } } run ( )"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": "= await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) const json = await response . json ( ) console . log ( json ) } catch ( error ) { console . log ( error ) } } run ( )\n\nconst fetch = require ( \"node-fetch\" ) const HOST = \"http://localhost:9000\" async function run ( ) { try { const query = \"SELECT x FROM long_sequence(5);\" const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) const json = await response . json ( ) console . log ( json ) } catch ( error ) { console . log ( error ) } } run ( )\n\nconst fetch = require ( \"node-fetch\" ) const HOST = \"http://localhost:9000\" async function run ( ) { try { const query = \"SELECT x FROM long_sequence(5);\" const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) const json = await response . json ( ) console . log ( json ) } catch ( error ) { console . log ( error ) } } run ( )\n\nconst fetch = require ( \"node-fetch\" ) const HOST = \"http://localhost:9000\" async function run ( ) { try { const query = \"SELECT x FROM long_sequence(5);\" const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) const jso"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": "localhost:9000\" async function run ( ) { try { const query = \"SELECT x FROM long_sequence(5);\" const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) const json = await response . json ( ) console . log ( json ) } catch ( error ) { console . log ( error ) } } run ( )\n\npackage main import ( \"fmt\" \"io/ioutil\" \"log\" \"net/http\" \"net/url\" ) func main ( ) { u , err := url . Parse ( \"http://localhost:9000\" ) checkErr ( err ) u . Path += \"exec\" params := url . Values { } params . Add ( \"query\" , \"SELECT x FROM long_sequence(5);\" ) u . RawQuery = params . Encode ( ) url := fmt . Sprintf ( \"%v\" , u ) res , err := http . Get ( url ) checkErr ( err ) defer res . Body . Close ( ) body , err := ioutil . ReadAll ( res . Body ) checkErr ( err ) log . Println ( string ( body ) ) } func checkErr ( err error ) { if err != nil { panic ( err ) } }\n\npackage main import ( \"fmt\" \"io/ioutil\" \"log\" \"net/http\" \"net/url\" ) func main ( ) { u , err := url . Parse ( \"http://localhost:9000\" ) checkErr ( err ) u . Path += \"exec\" params := url . Values { } params . Add ( \"query\" , \"SELECT x FROM long_sequence(5);\" ) u . RawQuery = params . Encode ( ) url := fmt . Sprintf ( \""}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": "ocalhost:9000\" ) checkErr ( err ) u . Path += \"exec\" params := url . Values { } params . Add ( \"query\" , \"SELECT x FROM long_sequence(5);\" ) u . RawQuery = params . Encode ( ) url := fmt . Sprintf ( \"%v\" , u ) res , err := http . Get ( url ) checkErr ( err ) defer res . Body . Close ( ) body , err := ioutil . ReadAll ( res . Body ) checkErr ( err ) log . Println ( string ( body ) ) } func checkErr ( err error ) { if err != nil { panic ( err ) } }\n\npackage main import ( \"fmt\" \"io/ioutil\" \"log\" \"net/http\" \"net/url\" ) func main ( ) { u , err := url . Parse ( \"http://localhost:9000\" ) checkErr ( err ) u . Path += \"exec\" params := url . Values { } params . Add ( \"query\" , \"SELECT x FROM long_sequence(5);\" ) u . RawQuery = params . Encode ( ) url := fmt . Sprintf ( \"%v\" , u ) res , err := http . Get ( url ) checkErr ( err ) defer res . Body . Close ( ) body , err := ioutil . ReadAll ( res . Body ) checkErr ( err ) log . Println ( string ( body ) ) } func checkErr ( err error ) { if err != nil { panic ( err ) } }\n\npackage main import ( \"fmt\" \"io/ioutil\" \"log\" \"net/http\" \"net/url\" ) func main ( ) { u , err := url . Parse ( \"http://localhost:9000\" ) checkErr ( err ) u . Path += \"exec\" param"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": "l { panic ( err ) } }\n\npackage main import ( \"fmt\" \"io/ioutil\" \"log\" \"net/http\" \"net/url\" ) func main ( ) { u , err := url . Parse ( \"http://localhost:9000\" ) checkErr ( err ) u . Path += \"exec\" params := url . Values { } params . Add ( \"query\" , \"SELECT x FROM long_sequence(5);\" ) u . RawQuery = params . Encode ( ) url := fmt . Sprintf ( \"%v\" , u ) res , err := http . Get ( url ) checkErr ( err ) defer res . Body . Close ( ) body , err := ioutil . ReadAll ( res . Body ) checkErr ( err ) log . Println ( string ( body ) ) } func checkErr ( err error ) { if err != nil { panic ( err ) } }\n\npackage main import ( \"fmt\" \"io/ioutil\" \"log\" \"net/http\" \"net/url\" ) func main ( ) { u , err := url . Parse ( \"http://localhost:9000\" ) checkErr ( err ) u . Path += \"exec\" params := url . Values { } params . Add ( \"query\" , \"SELECT x FROM long_sequence(5);\" ) u . RawQuery = params . Encode ( ) url := fmt . Sprintf ( \"%v\" , u ) res , err := http . Get ( url ) checkErr ( err ) defer res . Body . Close ( ) body , err := ioutil . ReadAll ( res . Body ) checkErr ( err ) log . Println ( string ( body ) ) } func checkErr ( err error ) { if err != nil { panic ( err ) } }\n\nAlternatively, the /exec endpoint c"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": "body , err := ioutil . ReadAll ( res . Body ) checkErr ( err ) log . Println ( string ( body ) ) } func checkErr ( err error ) { if err != nil { panic ( err ) } }\n\nAlternatively, the /exec endpoint can be used to create a table and the INSERT statement can be used to populate it with values:\n\n/exec\n\nINSERT\n\ncURL NodeJS Python # Create Table curl -G \\ --data-urlencode \"query=CREATE TABLE IF NOT EXISTS trades(name VARCHAR, value INT)\" \\ http://localhost:9000/exec # Insert a row curl -G \\ --data-urlencode \"query=INSERT INTO trades VALUES('abc', 123456)\" \\ http://localhost:9000/exec # Update a row curl -G \\ --data-urlencode \"query=UPDATE trades SET value = 9876 WHERE name = 'abc'\" \\ http://localhost:9000/exec The node-fetch package can be installed using npm i node-fetch . const fetch = require ( \"node-fetch\" ) ; const HOST = \"http://localhost:9000\" ; async function createTable ( ) { try { const query = \"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value INT)\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } async fu"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": "fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } async function insertData ( ) { try { const query = \"INSERT INTO trades VALUES('abc', 123456)\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } async function updateData ( ) { try { const query = \"UPDATE trades SET value = 9876 WHERE name = 'abc'\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } createTable ( ) . then ( insertData ) . then ( updateData ) ; import requests import json host = 'http://localhost:9000' def run_query ( sql_query ) : query_params = { 'query' : sql_query , 'fmt' : 'json' } try : response = requests . get ( host + '/exec' , params = query_params ) json_response = json . loads ( response . text ) print ( json_response ) except requests . exce"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": "sql_query , 'fmt' : 'json' } try : response = requests . get ( host + '/exec' , params = query_params ) json_response = json . loads ( response . text ) print ( json_response ) except requests . exceptions . RequestException as e : print ( \"Error: %s\" % ( e ) ) # create table run_query ( \"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value INT)\" ) # insert row run_query ( \"INSERT INTO trades VALUES('abc', 123456)\" ) # update row run_query ( \"UPDATE trades SET value = 9876 WHERE name = 'abc'\" )\n\ncURL\n\nNodeJS\n\nPython\n\n# Create Table curl -G \\ --data-urlencode \"query=CREATE TABLE IF NOT EXISTS trades(name VARCHAR, value INT)\" \\ http://localhost:9000/exec # Insert a row curl -G \\ --data-urlencode \"query=INSERT INTO trades VALUES('abc', 123456)\" \\ http://localhost:9000/exec # Update a row curl -G \\ --data-urlencode \"query=UPDATE trades SET value = 9876 WHERE name = 'abc'\" \\ http://localhost:9000/exec The node-fetch package can be installed using npm i node-fetch . const fetch = require ( \"node-fetch\" ) ; const HOST = \"http://localhost:9000\" ; async function createTable ( ) { try { const query = \"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value INT)\" ; const response = await fe"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": "( \"node-fetch\" ) ; const HOST = \"http://localhost:9000\" ; async function createTable ( ) { try { const query = \"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value INT)\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } async function insertData ( ) { try { const query = \"INSERT INTO trades VALUES('abc', 123456)\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } async function updateData ( ) { try { const query = \"UPDATE trades SET value = 9876 WHERE name = 'abc'\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } createTable ( ) . then ( insertData ) . then ( updateData ) ; import requests import json host = 'http://localhost:9000' def run_query ( sql_query ) : query_params = { 'query' : sq"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": "log ( error ) ; } } createTable ( ) . then ( insertData ) . then ( updateData ) ; import requests import json host = 'http://localhost:9000' def run_query ( sql_query ) : query_params = { 'query' : sql_query , 'fmt' : 'json' } try : response = requests . get ( host + '/exec' , params = query_params ) json_response = json . loads ( response . text ) print ( json_response ) except requests . exceptions . RequestException as e : print ( \"Error: %s\" % ( e ) ) # create table run_query ( \"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value INT)\" ) # insert row run_query ( \"INSERT INTO trades VALUES('abc', 123456)\" ) # update row run_query ( \"UPDATE trades SET value = 9876 WHERE name = 'abc'\" )\n\n# Create Table curl -G \\ --data-urlencode \"query=CREATE TABLE IF NOT EXISTS trades(name VARCHAR, value INT)\" \\ http://localhost:9000/exec # Insert a row curl -G \\ --data-urlencode \"query=INSERT INTO trades VALUES('abc', 123456)\" \\ http://localhost:9000/exec # Update a row curl -G \\ --data-urlencode \"query=UPDATE trades SET value = 9876 WHERE name = 'abc'\" \\ http://localhost:9000/exec\n\n# Create Table curl -G \\ --data-urlencode \"query=CREATE TABLE IF NOT EXISTS trades(name VARCHAR, value INT)\" \\"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": "\"query=UPDATE trades SET value = 9876 WHERE name = 'abc'\" \\ http://localhost:9000/exec\n\n# Create Table curl -G \\ --data-urlencode \"query=CREATE TABLE IF NOT EXISTS trades(name VARCHAR, value INT)\" \\ http://localhost:9000/exec # Insert a row curl -G \\ --data-urlencode \"query=INSERT INTO trades VALUES('abc', 123456)\" \\ http://localhost:9000/exec # Update a row curl -G \\ --data-urlencode \"query=UPDATE trades SET value = 9876 WHERE name = 'abc'\" \\ http://localhost:9000/exec\n\n# Create Table curl -G \\ --data-urlencode \"query=CREATE TABLE IF NOT EXISTS trades(name VARCHAR, value INT)\" \\ http://localhost:9000/exec # Insert a row curl -G \\ --data-urlencode \"query=INSERT INTO trades VALUES('abc', 123456)\" \\ http://localhost:9000/exec # Update a row curl -G \\ --data-urlencode \"query=UPDATE trades SET value = 9876 WHERE name = 'abc'\" \\ http://localhost:9000/exec\n\n# Create Table curl -G \\ --data-urlencode \"query=CREATE TABLE IF NOT EXISTS trades(name VARCHAR, value INT)\" \\ http://localhost:9000/exec # Insert a row curl -G \\ --data-urlencode \"query=INSERT INTO trades VALUES('abc', 123456)\" \\ http://localhost:9000/exec # Update a row curl -G \\ --data-urlencode \"query=UPDATE trades SET value = 98"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": "ec # Insert a row curl -G \\ --data-urlencode \"query=INSERT INTO trades VALUES('abc', 123456)\" \\ http://localhost:9000/exec # Update a row curl -G \\ --data-urlencode \"query=UPDATE trades SET value = 9876 WHERE name = 'abc'\" \\ http://localhost:9000/exec\n\n# Create Table curl -G \\ --data-urlencode \"query=CREATE TABLE IF NOT EXISTS trades(name VARCHAR, value INT)\" \\ http://localhost:9000/exec # Insert a row curl -G \\ --data-urlencode \"query=INSERT INTO trades VALUES('abc', 123456)\" \\ http://localhost:9000/exec # Update a row curl -G \\ --data-urlencode \"query=UPDATE trades SET value = 9876 WHERE name = 'abc'\" \\ http://localhost:9000/exec\n\nThe node-fetch package can be installed using npm i node-fetch . const fetch = require ( \"node-fetch\" ) ; const HOST = \"http://localhost:9000\" ; async function createTable ( ) { try { const query = \"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value INT)\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } async function insertData ( ) { try { const query = \"INSERT INTO trades VALUES('abc"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": "const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } async function insertData ( ) { try { const query = \"INSERT INTO trades VALUES('abc', 123456)\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } async function updateData ( ) { try { const query = \"UPDATE trades SET value = 9876 WHERE name = 'abc'\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } createTable ( ) . then ( insertData ) . then ( updateData ) ;\n\nThe node-fetch package can be installed using npm i node-fetch .\n\nnode-fetch\n\nnpm i node-fetch\n\nconst fetch = require ( \"node-fetch\" ) ; const HOST = \"http://localhost:9000\" ; async function createTable ( ) { try { const query = \"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value INT)\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , )"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": "n createTable ( ) { try { const query = \"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value INT)\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } async function insertData ( ) { try { const query = \"INSERT INTO trades VALUES('abc', 123456)\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } async function updateData ( ) { try { const query = \"UPDATE trades SET value = 9876 WHERE name = 'abc'\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } createTable ( ) . then ( insertData ) . then ( updateData ) ;\n\nconst fetch = require ( \"node-fetch\" ) ; const HOST = \"http://localhost:9000\" ; async function createTable ( ) { try { const query = \"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": "teData ) ;\n\nconst fetch = require ( \"node-fetch\" ) ; const HOST = \"http://localhost:9000\" ; async function createTable ( ) { try { const query = \"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value INT)\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } async function insertData ( ) { try { const query = \"INSERT INTO trades VALUES('abc', 123456)\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } async function updateData ( ) { try { const query = \"UPDATE trades SET value = 9876 WHERE name = 'abc'\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } createTable ( ) . then ( insertData ) . then ( updateData ) ;\n\nconst fetch = require ( \"node-fetch\" ) ; const HOST = \"http://localhost:9000\" ; asy"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": ") ; } catch ( error ) { console . log ( error ) ; } } createTable ( ) . then ( insertData ) . then ( updateData ) ;\n\nconst fetch = require ( \"node-fetch\" ) ; const HOST = \"http://localhost:9000\" ; async function createTable ( ) { try { const query = \"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value INT)\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } async function insertData ( ) { try { const query = \"INSERT INTO trades VALUES('abc', 123456)\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } async function updateData ( ) { try { const query = \"UPDATE trades SET value = 9876 WHERE name = 'abc'\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } createTable ( ) . then ( insertData ) . t"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": "${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } createTable ( ) . then ( insertData ) . then ( updateData ) ;\n\nconst fetch = require ( \"node-fetch\" ) ; const HOST = \"http://localhost:9000\" ; async function createTable ( ) { try { const query = \"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value INT)\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } async function insertData ( ) { try { const query = \"INSERT INTO trades VALUES('abc', 123456)\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } async function updateData ( ) { try { const query = \"UPDATE trades SET value = 9876 WHERE name = 'abc'\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . l"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": "ATE trades SET value = 9876 WHERE name = 'abc'\" ; const response = await fetch ( ` ${ HOST } /exec?query= ${ encodeURIComponent ( query ) } ` , ) ; const json = await response . json ( ) ; console . log ( json ) ; } catch ( error ) { console . log ( error ) ; } } createTable ( ) . then ( insertData ) . then ( updateData ) ;\n\nimport requests import json host = 'http://localhost:9000' def run_query ( sql_query ) : query_params = { 'query' : sql_query , 'fmt' : 'json' } try : response = requests . get ( host + '/exec' , params = query_params ) json_response = json . loads ( response . text ) print ( json_response ) except requests . exceptions . RequestException as e : print ( \"Error: %s\" % ( e ) ) # create table run_query ( \"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value INT)\" ) # insert row run_query ( \"INSERT INTO trades VALUES('abc', 123456)\" ) # update row run_query ( \"UPDATE trades SET value = 9876 WHERE name = 'abc'\" )\n\nimport requests import json host = 'http://localhost:9000' def run_query ( sql_query ) : query_params = { 'query' : sql_query , 'fmt' : 'json' } try : response = requests . get ( host + '/exec' , params = query_params ) json_response = json . loads ( res"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": "000' def run_query ( sql_query ) : query_params = { 'query' : sql_query , 'fmt' : 'json' } try : response = requests . get ( host + '/exec' , params = query_params ) json_response = json . loads ( response . text ) print ( json_response ) except requests . exceptions . RequestException as e : print ( \"Error: %s\" % ( e ) ) # create table run_query ( \"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value INT)\" ) # insert row run_query ( \"INSERT INTO trades VALUES('abc', 123456)\" ) # update row run_query ( \"UPDATE trades SET value = 9876 WHERE name = 'abc'\" )\n\nimport requests import json host = 'http://localhost:9000' def run_query ( sql_query ) : query_params = { 'query' : sql_query , 'fmt' : 'json' } try : response = requests . get ( host + '/exec' , params = query_params ) json_response = json . loads ( response . text ) print ( json_response ) except requests . exceptions . RequestException as e : print ( \"Error: %s\" % ( e ) ) # create table run_query ( \"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value INT)\" ) # insert row run_query ( \"INSERT INTO trades VALUES('abc', 123456)\" ) # update row run_query ( \"UPDATE trades SET value = 9876 WHERE name = 'abc'\" )\n\nimport requests"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": "ades (name VARCHAR, value INT)\" ) # insert row run_query ( \"INSERT INTO trades VALUES('abc', 123456)\" ) # update row run_query ( \"UPDATE trades SET value = 9876 WHERE name = 'abc'\" )\n\nimport requests import json host = 'http://localhost:9000' def run_query ( sql_query ) : query_params = { 'query' : sql_query , 'fmt' : 'json' } try : response = requests . get ( host + '/exec' , params = query_params ) json_response = json . loads ( response . text ) print ( json_response ) except requests . exceptions . RequestException as e : print ( \"Error: %s\" % ( e ) ) # create table run_query ( \"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value INT)\" ) # insert row run_query ( \"INSERT INTO trades VALUES('abc', 123456)\" ) # update row run_query ( \"UPDATE trades SET value = 9876 WHERE name = 'abc'\" )\n\nimport requests import json host = 'http://localhost:9000' def run_query ( sql_query ) : query_params = { 'query' : sql_query , 'fmt' : 'json' } try : response = requests . get ( host + '/exec' , params = query_params ) json_response = json . loads ( response . text ) print ( json_response ) except requests . exceptions . RequestException as e : print ( \"Error: %s\" % ( e ) ) # create table run_"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "REST HTTP API ​", "text": "arams = query_params ) json_response = json . loads ( response . text ) print ( json_response ) except requests . exceptions . RequestException as e : print ( \"Error: %s\" % ( e ) ) # create table run_query ( \"CREATE TABLE IF NOT EXISTS trades (name VARCHAR, value INT)\" ) # insert row run_query ( \"INSERT INTO trades VALUES('abc', 123456)\" ) # update row run_query ( \"UPDATE trades SET value = 9876 WHERE name = 'abc'\" )"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Apache Parquet ​", "text": "info Apache Parquet support is in beta . It may not be fit for production use. Please let us know if you run into issues. Either: Email us at support@questdb.io Join our public Slack Post on our Discourse community\n\ninfo\n\nApache Parquet support is in beta . It may not be fit for production use. Please let us know if you run into issues. Either: Email us at support@questdb.io Join our public Slack Post on our Discourse community\n\nApache Parquet support is in beta . It may not be fit for production use.\n\nPlease let us know if you run into issues. Either:\n\nEmail us at support@questdb.io\n\nJoin our public Slack\n\nPost on our Discourse community\n\nParquet files can be read and thus queried by QuestDB.\n\nQuestDB is shipped with a demo Parquet file, trades.parquet , which can be queried using the read_parquet function.\n\ntrades.parquet\n\nread_parquet\n\nExample:\n\nread_parquet example SELECT * FROM read_parquet('trades.parquet') WHERE side = 'buy';\n\nread_parquet example\n\nSELECT * FROM read_parquet('trades.parquet') WHERE side = 'buy';\n\nSELECT * FROM read_parquet('trades.parquet') WHERE side = 'buy';\n\nSELECT * FROM read_parquet('trades.parquet') WHERE side = 'buy';\n\nThe trades.parquet file is locat"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "Apache Parquet ​", "text": "ades.parquet') WHERE side = 'buy';\n\nSELECT * FROM read_parquet('trades.parquet') WHERE side = 'buy';\n\nSELECT * FROM read_parquet('trades.parquet') WHERE side = 'buy';\n\nThe trades.parquet file is located in the import subdirectory inside the QuestDB root directory. Drop your own Parquet files to the import directory and query them using the read_parquet() function.\n\nimport\n\nread_parquet()\n\nYou can change the allowed directory by setting the cairo.sql.copy.root configuration key.\n\ncairo.sql.copy.root\n\nFor more information, see the Parquet documentation ."}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "What's next? ​", "text": "Now... SQL! It's query time.\n\nWhether you want to use the Web Console , PostgreSQL or REST HTTP (or both), query construction is rich.\n\nTo brush up and learn what's unique in QuestDB, consider the following:\n\nData types\n\nSQL execution order\n\nAnd to learn about some of our favourite, most powerful syntax:\n\nWindow functions are a powerful analysis tool\n\nAggregate functions - aggregations are key!\n\nDate & time operators to learn about date and time\n\nSAMPLE BY to summarize data into chunks based on a specified time interval, from a year to a microsecond\n\nSAMPLE BY\n\nWHERE IN to compress time ranges into concise intervals\n\nWHERE IN\n\nLATEST ON for latest values within multiple series within a table\n\nLATEST ON\n\nASOF JOIN to associate timestamps between a series based on proximity; no extra indices required\n\nASOF JOIN\n\nMaterialized Views to pre-compute complex queries for optimal performance\n\nLooking for visuals?\n\nExplore Grafana\n\nJump quickly into the Web Console\n\nEdit this page\n\nEdit this page\n\nPrevious\n\nREST HTTP API\n\nNext\n\nOverview\n\nQuestDB Web Console PostgreSQL REST HTTP API Apache Parquet What's next?\n\nQuestDB Web Console PostgreSQL REST HTTP API Apache Parquet What's next?\n\nQuestDB"}
{"source_url": "https://questdb.com/docs/reference/sql/overview", "title": "Query & SQL Overview", "section": "What's next? ​", "text": "e\n\nPrevious\n\nREST HTTP API\n\nNext\n\nOverview\n\nQuestDB Web Console PostgreSQL REST HTTP API Apache Parquet What's next?\n\nQuestDB Web Console PostgreSQL REST HTTP API Apache Parquet What's next?\n\nQuestDB Web Console\n\nPostgreSQL\n\nREST HTTP API\n\nApache Parquet\n\nWhat's next?"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "Deployment Docker On this page Using Docker with QuestDB QuestDB has images for both Linux/macOS and Windows on Docker Hub . Install Docker ​ To begin, install Docker. You can find guides for your platform on the official documentation . Run QuestDB image ​ Once Docker is installed, you will need to pull QuestDB's image from Docker Hub and create a container. This can be done with a single command using: docker run \\ -p 9000:9000 -p 9009:9009 -p 8812:8812 -p 9003:9003 \\ questdb/questdb:9.1.0 This command starts a Docker container from questdb/questdb image. In addition, it exposes some ports, allowing you to explore QuestDB. In order to configure QuestDB, it is recommended to mount a volume to allow data persistance. This can be done by adding a -v flag to the above command: -v \"/host/volume/location:/var/lib/questdb\" Below each parameter is described in detail. -p parameter to expose ports ​ This parameter will expose a port to the host. You can specify: -p 9000:9000 - REST API and Web Console -p 9009:9009 - InfluxDB line protocol -p 8812:8812 - Postgres wire protocol -p 9003:9003 - Min health server All ports are optional, you can pick only the ones you need. For example, it is e"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "b Console -p 9009:9009 - InfluxDB line protocol -p 8812:8812 - Postgres wire protocol -p 9003:9003 - Min health server All ports are optional, you can pick only the ones you need. For example, it is enough to expose 8812 if you only plan to use Postgres wire protocol . -v parameter to mount storage ​ This parameter will make a local directory available to QuestDB Docker container. It will have all data ingested to QuestDB, server logs and configuration. The QuestDB root_directory is located at the /var/lib/questdb path in the container. Docker image version ​ By default, questdb/questdb points to the latest QuestDB version available on Docker. However, it is recommended to define the version used. questdb/questdb:9.1.0 Environment variables ​ Server configuration can be passed to QuestDB running in Docker by using the -e flag to pass an environment variable to a container: docker run -p 4000:4000 -e QDB_HTTP_BIND_TO=0.0.0.0:4000 questdb/questdb For a list of configuration options, see Configuration . Container status ​ You can check the status of your container with docker ps . It also lists the exposed ports, container name, uptime and more: Finding container status with docker ps"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "Configuration . Container status ​ You can check the status of your container with docker ps . It also lists the exposed ports, container name, uptime and more: Finding container status with docker ps docker ps Result of docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES dd363939f261 questdb/questdb \"/app/bin/java -m io…\" 3 seconds ago Up 2 seconds 8812/tcp, 9000/tcp frosty_gauss This container: has an id of dd363939f261 uses ports 8812 & 9000 , for Postgres wire protocol and HTTP respectively is using a questdb/questdb image ran java to start the binary is 3 seconds old has been up for 2 seconds has the unfortunate name of frosty_gauss For full container status information, see the docker ps manual . Debugging container logs ​ Docker may generate a runtime error. The error may not be accurate, as the true culprit is often indicated higher up in the logs. To see the full log, retrieve the UUID - also known as the CONTAINER ID - using docker ps : Finding the CONTAINER ID CONTAINER ID IMAGE ... dd363939f261 questdb/questdb ... Now pass the CONTAINER ID - or dd363939f261 - to the docker logs command: Generating a docker log from a CONTAINER ID $ docker logs dd363939f261 N"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "NTAINER ID IMAGE ... dd363939f261 questdb/questdb ... Now pass the CONTAINER ID - or dd363939f261 - to the docker logs command: Generating a docker log from a CONTAINER ID $ docker logs dd363939f261 No arguments found, start with default arguments Running as questdb user Log configuration loaded from: /var/lib/questdb/conf/log.conf ... ... Note that the log will pull from /var/lib/questdb/conf/log.conf by default. Sharing this log when seeking support for Docker deployments will help us find the root cause. Importing data and sending queries ​ When QuestDB is running, you can start interacting with it: Port 9000 is for REST. More info is available on the REST documentation page . Port 8812 is used for Postgres. Check our Postgres reference page . Port 9009 is dedicated to InfluxDB Line Protocol. Consult our InfluxDB protocol page . Data persistence ​ Mounting a volume ​ Volumes can be mounted to the QuestDB Docker container so that data may be persisted or server configuration settings may be passed to an instance. The following example demonstrated how to mount the current directory to a QuestDB container using the -v flag in a Docker run command: Mounting a volume docker run -p 9"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "may be passed to an instance. The following example demonstrated how to mount the current directory to a QuestDB container using the -v flag in a Docker run command: Mounting a volume docker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ -v \"$(pwd):/var/lib/questdb\" \\ questdb/questdb:9.1.0 The current directory will then have data persisted to disk for convenient migration or backups: Current directory contents ├── conf │ └── server.conf ├── db ├── log ├── public └── snapshot (optional) A server configuration file can also be provided by mounting a local directory in a QuestDB container. Given the following configuration file which overrides the default HTTP bind property: ./server.conf http.bind.to=0.0.0.0:4000 Running the container with the -v flag allows for mounting the current directory to QuestDB's conf directory in the container. With the server configuration above, HTTP ports for the Web Console and REST API will be available on localhost:4000 : docker run -v \"$(pwd):/var/lib/questdb/conf\" -p 4000:4000 questdb/questdb note If you wish to use ZFS for your QuestDB deployment, with Docker, then you will need to enable ZFS on the host volume that Docker uses."}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "\"$(pwd):/var/lib/questdb/conf\" -p 4000:4000 questdb/questdb note If you wish to use ZFS for your QuestDB deployment, with Docker, then you will need to enable ZFS on the host volume that Docker uses. Please see the docker documentation for more information. Upgrade QuestDB version ​ It is possible to upgrade your QuestDB instance on Docker when a volume is mounted to maintain data persistence. note Check the release notes and ensure that necessary backup is completed. Upgrading an instance is possible only when the original instance has a volume mounted. Without mounting a volume for the original instance, the following steps create a new instance and data in the old instance cannot be retrieved. Run docker ps to copy the container name or ID: Container status # The existing QuestDB version is 6.5.2: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES dd363939f261 questdb/questdb:6.5.2 \"/app/bin/java -m io…\" 3 seconds ago Up 2 seconds 8812/tcp, 9000/tcp frosty_gauss Stop the instance and then remove the container: docker stop dd363939f261 docker rm dd363939f261 Download the latest QuestDB image: docker pull questdb/questdb:9.1.0 Start a new container with the new version and the"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "and then remove the container: docker stop dd363939f261 docker rm dd363939f261 Download the latest QuestDB image: docker pull questdb/questdb:9.1.0 Start a new container with the new version and the same volume mounted: docker run -p 8812:8812 -p 9000:9000 -v \"$(pwd):/var/lib/questdb\" questdb/questdb:9.1.0 Writing logs to disk ​ When mounting a volume to a Docker container, a logging configuration file may be provided in the container located at /conf/log.conf : Current directory contents └── conf ├── log.conf └── server.conf For example, a file with the following contents can be created: ./conf/log.conf # list of configured writers writers=file,stdout,http.min # file writer w.file.class=io.questdb.log.LogFileWriter w.file.location=questdb-docker.log w.file.level=INFO,ERROR,DEBUG # stdout w.stdout.class=io.questdb.log.LogConsoleWriter w.stdout.level=INFO # min http server, used monitoring w.http.min.class=io.questdb.log.LogConsoleWriter w.http.min.level=ERROR w.http.min.scope=http-min-server The current directory can be mounted: Mounting the current directory to a QuestDB container docker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ -v \"$(pwd):/root/.questdb/\" q"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "-server The current directory can be mounted: Mounting the current directory to a QuestDB container docker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ -v \"$(pwd):/root/.questdb/\" questdb/questdb The container logs will be written to disk using the logging level and file name provided in the conf/log.conf file, in this case in ./questdb-docker.log : Current directory tree ├── conf │ ├── log.conf │ └── server.conf ├── db │ ├── table1 │ └── table2 ├── public │ ├── ui / assets │ ├── ... │ └── version.txt └── questdb-docker.log For more information on logging, see the configuration reference documentation . Restart an existing container ​ Running the following command will create a new container for the QuestDB image: docker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ questdb/questdb By giving the container a name with --name container_name , we have an easy way to refer to the container created by run later on: docker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ --name docker_questdb \\ questdb/questdb If we want to re-use this container and its data after it has been stopped, we can use the following commands: # bring the conta"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "\\ -p 8812:8812 \\ -p 9003:9003 \\ --name docker_questdb \\ questdb/questdb If we want to re-use this container and its data after it has been stopped, we can use the following commands: # bring the container up docker start docker_questdb # shut the container down docker stop docker_questdb Alternatively, restart it using the CONTAINER ID : Starting a container by CONTAINER ID docker start dd363939f261 Edit this page Previous Text Next Kubernetes Install Docker Run QuestDB image -p parameter to expose ports -v parameter to mount storage Docker image version Environment variables Container status Debugging container logs Importing data and sending queries Data persistence Mounting a volume Upgrade QuestDB version Writing logs to disk Restart an existing container\n\nDeployment Docker On this page Using Docker with QuestDB QuestDB has images for both Linux/macOS and Windows on Docker Hub . Install Docker ​ To begin, install Docker. You can find guides for your platform on the official documentation . Run QuestDB image ​ Once Docker is installed, you will need to pull QuestDB's image from Docker Hub and create a container. This can be done with a single command using: docker run \\ -p 9000:"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "tation . Run QuestDB image ​ Once Docker is installed, you will need to pull QuestDB's image from Docker Hub and create a container. This can be done with a single command using: docker run \\ -p 9000:9000 -p 9009:9009 -p 8812:8812 -p 9003:9003 \\ questdb/questdb:9.1.0 This command starts a Docker container from questdb/questdb image. In addition, it exposes some ports, allowing you to explore QuestDB. In order to configure QuestDB, it is recommended to mount a volume to allow data persistance. This can be done by adding a -v flag to the above command: -v \"/host/volume/location:/var/lib/questdb\" Below each parameter is described in detail. -p parameter to expose ports ​ This parameter will expose a port to the host. You can specify: -p 9000:9000 - REST API and Web Console -p 9009:9009 - InfluxDB line protocol -p 8812:8812 - Postgres wire protocol -p 9003:9003 - Min health server All ports are optional, you can pick only the ones you need. For example, it is enough to expose 8812 if you only plan to use Postgres wire protocol . -v parameter to mount storage ​ This parameter will make a local directory available to QuestDB Docker container. It will have all data ingested to QuestDB, se"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "nly plan to use Postgres wire protocol . -v parameter to mount storage ​ This parameter will make a local directory available to QuestDB Docker container. It will have all data ingested to QuestDB, server logs and configuration. The QuestDB root_directory is located at the /var/lib/questdb path in the container. Docker image version ​ By default, questdb/questdb points to the latest QuestDB version available on Docker. However, it is recommended to define the version used. questdb/questdb:9.1.0 Environment variables ​ Server configuration can be passed to QuestDB running in Docker by using the -e flag to pass an environment variable to a container: docker run -p 4000:4000 -e QDB_HTTP_BIND_TO=0.0.0.0:4000 questdb/questdb For a list of configuration options, see Configuration . Container status ​ You can check the status of your container with docker ps . It also lists the exposed ports, container name, uptime and more: Finding container status with docker ps docker ps Result of docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES dd363939f261 questdb/questdb \"/app/bin/java -m io…\" 3 seconds ago Up 2 seconds 8812/tcp, 9000/tcp frosty_gauss This container: has an id of dd36"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "s CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES dd363939f261 questdb/questdb \"/app/bin/java -m io…\" 3 seconds ago Up 2 seconds 8812/tcp, 9000/tcp frosty_gauss This container: has an id of dd363939f261 uses ports 8812 & 9000 , for Postgres wire protocol and HTTP respectively is using a questdb/questdb image ran java to start the binary is 3 seconds old has been up for 2 seconds has the unfortunate name of frosty_gauss For full container status information, see the docker ps manual . Debugging container logs ​ Docker may generate a runtime error. The error may not be accurate, as the true culprit is often indicated higher up in the logs. To see the full log, retrieve the UUID - also known as the CONTAINER ID - using docker ps : Finding the CONTAINER ID CONTAINER ID IMAGE ... dd363939f261 questdb/questdb ... Now pass the CONTAINER ID - or dd363939f261 - to the docker logs command: Generating a docker log from a CONTAINER ID $ docker logs dd363939f261 No arguments found, start with default arguments Running as questdb user Log configuration loaded from: /var/lib/questdb/conf/log.conf ... ... Note that the log will pull from /var/lib/questdb/conf/log.conf by default. Sharing thi"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "default arguments Running as questdb user Log configuration loaded from: /var/lib/questdb/conf/log.conf ... ... Note that the log will pull from /var/lib/questdb/conf/log.conf by default. Sharing this log when seeking support for Docker deployments will help us find the root cause. Importing data and sending queries ​ When QuestDB is running, you can start interacting with it: Port 9000 is for REST. More info is available on the REST documentation page . Port 8812 is used for Postgres. Check our Postgres reference page . Port 9009 is dedicated to InfluxDB Line Protocol. Consult our InfluxDB protocol page . Data persistence ​ Mounting a volume ​ Volumes can be mounted to the QuestDB Docker container so that data may be persisted or server configuration settings may be passed to an instance. The following example demonstrated how to mount the current directory to a QuestDB container using the -v flag in a Docker run command: Mounting a volume docker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ -v \"$(pwd):/var/lib/questdb\" \\ questdb/questdb:9.1.0 The current directory will then have data persisted to disk for convenient migration or backups: Current directory conte"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "8812:8812 \\ -p 9003:9003 \\ -v \"$(pwd):/var/lib/questdb\" \\ questdb/questdb:9.1.0 The current directory will then have data persisted to disk for convenient migration or backups: Current directory contents ├── conf │ └── server.conf ├── db ├── log ├── public └── snapshot (optional) A server configuration file can also be provided by mounting a local directory in a QuestDB container. Given the following configuration file which overrides the default HTTP bind property: ./server.conf http.bind.to=0.0.0.0:4000 Running the container with the -v flag allows for mounting the current directory to QuestDB's conf directory in the container. With the server configuration above, HTTP ports for the Web Console and REST API will be available on localhost:4000 : docker run -v \"$(pwd):/var/lib/questdb/conf\" -p 4000:4000 questdb/questdb note If you wish to use ZFS for your QuestDB deployment, with Docker, then you will need to enable ZFS on the host volume that Docker uses. Please see the docker documentation for more information. Upgrade QuestDB version ​ It is possible to upgrade your QuestDB instance on Docker when a volume is mounted to maintain data persistence. note Check the release notes and"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "tation for more information. Upgrade QuestDB version ​ It is possible to upgrade your QuestDB instance on Docker when a volume is mounted to maintain data persistence. note Check the release notes and ensure that necessary backup is completed. Upgrading an instance is possible only when the original instance has a volume mounted. Without mounting a volume for the original instance, the following steps create a new instance and data in the old instance cannot be retrieved. Run docker ps to copy the container name or ID: Container status # The existing QuestDB version is 6.5.2: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES dd363939f261 questdb/questdb:6.5.2 \"/app/bin/java -m io…\" 3 seconds ago Up 2 seconds 8812/tcp, 9000/tcp frosty_gauss Stop the instance and then remove the container: docker stop dd363939f261 docker rm dd363939f261 Download the latest QuestDB image: docker pull questdb/questdb:9.1.0 Start a new container with the new version and the same volume mounted: docker run -p 8812:8812 -p 9000:9000 -v \"$(pwd):/var/lib/questdb\" questdb/questdb:9.1.0 Writing logs to disk ​ When mounting a volume to a Docker container, a logging configuration file may be provided in the"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "un -p 8812:8812 -p 9000:9000 -v \"$(pwd):/var/lib/questdb\" questdb/questdb:9.1.0 Writing logs to disk ​ When mounting a volume to a Docker container, a logging configuration file may be provided in the container located at /conf/log.conf : Current directory contents └── conf ├── log.conf └── server.conf For example, a file with the following contents can be created: ./conf/log.conf # list of configured writers writers=file,stdout,http.min # file writer w.file.class=io.questdb.log.LogFileWriter w.file.location=questdb-docker.log w.file.level=INFO,ERROR,DEBUG # stdout w.stdout.class=io.questdb.log.LogConsoleWriter w.stdout.level=INFO # min http server, used monitoring w.http.min.class=io.questdb.log.LogConsoleWriter w.http.min.level=ERROR w.http.min.scope=http-min-server The current directory can be mounted: Mounting the current directory to a QuestDB container docker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ -v \"$(pwd):/root/.questdb/\" questdb/questdb The container logs will be written to disk using the logging level and file name provided in the conf/log.conf file, in this case in ./questdb-docker.log : Current directory tree ├── conf │ ├── log.conf │ └── serve"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "logs will be written to disk using the logging level and file name provided in the conf/log.conf file, in this case in ./questdb-docker.log : Current directory tree ├── conf │ ├── log.conf │ └── server.conf ├── db │ ├── table1 │ └── table2 ├── public │ ├── ui / assets │ ├── ... │ └── version.txt └── questdb-docker.log For more information on logging, see the configuration reference documentation . Restart an existing container ​ Running the following command will create a new container for the QuestDB image: docker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ questdb/questdb By giving the container a name with --name container_name , we have an easy way to refer to the container created by run later on: docker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ --name docker_questdb \\ questdb/questdb If we want to re-use this container and its data after it has been stopped, we can use the following commands: # bring the container up docker start docker_questdb # shut the container down docker stop docker_questdb Alternatively, restart it using the CONTAINER ID : Starting a container by CONTAINER ID docker start dd363939f261 Edit this page Previous Te"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "uestdb # shut the container down docker stop docker_questdb Alternatively, restart it using the CONTAINER ID : Starting a container by CONTAINER ID docker start dd363939f261 Edit this page Previous Text Next Kubernetes Install Docker Run QuestDB image -p parameter to expose ports -v parameter to mount storage Docker image version Environment variables Container status Debugging container logs Importing data and sending queries Data persistence Mounting a volume Upgrade QuestDB version Writing logs to disk Restart an existing container\n\nDeployment Docker On this page Using Docker with QuestDB QuestDB has images for both Linux/macOS and Windows on Docker Hub . Install Docker ​ To begin, install Docker. You can find guides for your platform on the official documentation . Run QuestDB image ​ Once Docker is installed, you will need to pull QuestDB's image from Docker Hub and create a container. This can be done with a single command using: docker run \\ -p 9000:9000 -p 9009:9009 -p 8812:8812 -p 9003:9003 \\ questdb/questdb:9.1.0 This command starts a Docker container from questdb/questdb image. In addition, it exposes some ports, allowing you to explore QuestDB. In order to configure Que"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "2 -p 9003:9003 \\ questdb/questdb:9.1.0 This command starts a Docker container from questdb/questdb image. In addition, it exposes some ports, allowing you to explore QuestDB. In order to configure QuestDB, it is recommended to mount a volume to allow data persistance. This can be done by adding a -v flag to the above command: -v \"/host/volume/location:/var/lib/questdb\" Below each parameter is described in detail. -p parameter to expose ports ​ This parameter will expose a port to the host. You can specify: -p 9000:9000 - REST API and Web Console -p 9009:9009 - InfluxDB line protocol -p 8812:8812 - Postgres wire protocol -p 9003:9003 - Min health server All ports are optional, you can pick only the ones you need. For example, it is enough to expose 8812 if you only plan to use Postgres wire protocol . -v parameter to mount storage ​ This parameter will make a local directory available to QuestDB Docker container. It will have all data ingested to QuestDB, server logs and configuration. The QuestDB root_directory is located at the /var/lib/questdb path in the container. Docker image version ​ By default, questdb/questdb points to the latest QuestDB version available on Docker. Howeve"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "The QuestDB root_directory is located at the /var/lib/questdb path in the container. Docker image version ​ By default, questdb/questdb points to the latest QuestDB version available on Docker. However, it is recommended to define the version used. questdb/questdb:9.1.0 Environment variables ​ Server configuration can be passed to QuestDB running in Docker by using the -e flag to pass an environment variable to a container: docker run -p 4000:4000 -e QDB_HTTP_BIND_TO=0.0.0.0:4000 questdb/questdb For a list of configuration options, see Configuration . Container status ​ You can check the status of your container with docker ps . It also lists the exposed ports, container name, uptime and more: Finding container status with docker ps docker ps Result of docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES dd363939f261 questdb/questdb \"/app/bin/java -m io…\" 3 seconds ago Up 2 seconds 8812/tcp, 9000/tcp frosty_gauss This container: has an id of dd363939f261 uses ports 8812 & 9000 , for Postgres wire protocol and HTTP respectively is using a questdb/questdb image ran java to start the binary is 3 seconds old has been up for 2 seconds has the unfortunate name of frosty_gauss"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "00 , for Postgres wire protocol and HTTP respectively is using a questdb/questdb image ran java to start the binary is 3 seconds old has been up for 2 seconds has the unfortunate name of frosty_gauss For full container status information, see the docker ps manual . Debugging container logs ​ Docker may generate a runtime error. The error may not be accurate, as the true culprit is often indicated higher up in the logs. To see the full log, retrieve the UUID - also known as the CONTAINER ID - using docker ps : Finding the CONTAINER ID CONTAINER ID IMAGE ... dd363939f261 questdb/questdb ... Now pass the CONTAINER ID - or dd363939f261 - to the docker logs command: Generating a docker log from a CONTAINER ID $ docker logs dd363939f261 No arguments found, start with default arguments Running as questdb user Log configuration loaded from: /var/lib/questdb/conf/log.conf ... ... Note that the log will pull from /var/lib/questdb/conf/log.conf by default. Sharing this log when seeking support for Docker deployments will help us find the root cause. Importing data and sending queries ​ When QuestDB is running, you can start interacting with it: Port 9000 is for REST. More info is available on"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "r Docker deployments will help us find the root cause. Importing data and sending queries ​ When QuestDB is running, you can start interacting with it: Port 9000 is for REST. More info is available on the REST documentation page . Port 8812 is used for Postgres. Check our Postgres reference page . Port 9009 is dedicated to InfluxDB Line Protocol. Consult our InfluxDB protocol page . Data persistence ​ Mounting a volume ​ Volumes can be mounted to the QuestDB Docker container so that data may be persisted or server configuration settings may be passed to an instance. The following example demonstrated how to mount the current directory to a QuestDB container using the -v flag in a Docker run command: Mounting a volume docker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ -v \"$(pwd):/var/lib/questdb\" \\ questdb/questdb:9.1.0 The current directory will then have data persisted to disk for convenient migration or backups: Current directory contents ├── conf │ └── server.conf ├── db ├── log ├── public └── snapshot (optional) A server configuration file can also be provided by mounting a local directory in a QuestDB container. Given the following configuration file which"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "f ├── db ├── log ├── public └── snapshot (optional) A server configuration file can also be provided by mounting a local directory in a QuestDB container. Given the following configuration file which overrides the default HTTP bind property: ./server.conf http.bind.to=0.0.0.0:4000 Running the container with the -v flag allows for mounting the current directory to QuestDB's conf directory in the container. With the server configuration above, HTTP ports for the Web Console and REST API will be available on localhost:4000 : docker run -v \"$(pwd):/var/lib/questdb/conf\" -p 4000:4000 questdb/questdb note If you wish to use ZFS for your QuestDB deployment, with Docker, then you will need to enable ZFS on the host volume that Docker uses. Please see the docker documentation for more information. Upgrade QuestDB version ​ It is possible to upgrade your QuestDB instance on Docker when a volume is mounted to maintain data persistence. note Check the release notes and ensure that necessary backup is completed. Upgrading an instance is possible only when the original instance has a volume mounted. Without mounting a volume for the original instance, the following steps create a new instance an"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "is completed. Upgrading an instance is possible only when the original instance has a volume mounted. Without mounting a volume for the original instance, the following steps create a new instance and data in the old instance cannot be retrieved. Run docker ps to copy the container name or ID: Container status # The existing QuestDB version is 6.5.2: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES dd363939f261 questdb/questdb:6.5.2 \"/app/bin/java -m io…\" 3 seconds ago Up 2 seconds 8812/tcp, 9000/tcp frosty_gauss Stop the instance and then remove the container: docker stop dd363939f261 docker rm dd363939f261 Download the latest QuestDB image: docker pull questdb/questdb:9.1.0 Start a new container with the new version and the same volume mounted: docker run -p 8812:8812 -p 9000:9000 -v \"$(pwd):/var/lib/questdb\" questdb/questdb:9.1.0 Writing logs to disk ​ When mounting a volume to a Docker container, a logging configuration file may be provided in the container located at /conf/log.conf : Current directory contents └── conf ├── log.conf └── server.conf For example, a file with the following contents can be created: ./conf/log.conf # list of configured writers writers=file,std"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "og.conf : Current directory contents └── conf ├── log.conf └── server.conf For example, a file with the following contents can be created: ./conf/log.conf # list of configured writers writers=file,stdout,http.min # file writer w.file.class=io.questdb.log.LogFileWriter w.file.location=questdb-docker.log w.file.level=INFO,ERROR,DEBUG # stdout w.stdout.class=io.questdb.log.LogConsoleWriter w.stdout.level=INFO # min http server, used monitoring w.http.min.class=io.questdb.log.LogConsoleWriter w.http.min.level=ERROR w.http.min.scope=http-min-server The current directory can be mounted: Mounting the current directory to a QuestDB container docker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ -v \"$(pwd):/root/.questdb/\" questdb/questdb The container logs will be written to disk using the logging level and file name provided in the conf/log.conf file, in this case in ./questdb-docker.log : Current directory tree ├── conf │ ├── log.conf │ └── server.conf ├── db │ ├── table1 │ └── table2 ├── public │ ├── ui / assets │ ├── ... │ └── version.txt └── questdb-docker.log For more information on logging, see the configuration reference documentation . Restart an existing containe"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "└── table2 ├── public │ ├── ui / assets │ ├── ... │ └── version.txt └── questdb-docker.log For more information on logging, see the configuration reference documentation . Restart an existing container ​ Running the following command will create a new container for the QuestDB image: docker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ questdb/questdb By giving the container a name with --name container_name , we have an easy way to refer to the container created by run later on: docker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ --name docker_questdb \\ questdb/questdb If we want to re-use this container and its data after it has been stopped, we can use the following commands: # bring the container up docker start docker_questdb # shut the container down docker stop docker_questdb Alternatively, restart it using the CONTAINER ID : Starting a container by CONTAINER ID docker start dd363939f261 Edit this page Previous Text Next Kubernetes\n\nDeployment Docker On this page Using Docker with QuestDB QuestDB has images for both Linux/macOS and Windows on Docker Hub . Install Docker ​ To begin, install Docker. You can find guides for your platform on"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "t Docker On this page Using Docker with QuestDB QuestDB has images for both Linux/macOS and Windows on Docker Hub . Install Docker ​ To begin, install Docker. You can find guides for your platform on the official documentation . Run QuestDB image ​ Once Docker is installed, you will need to pull QuestDB's image from Docker Hub and create a container. This can be done with a single command using: docker run \\ -p 9000:9000 -p 9009:9009 -p 8812:8812 -p 9003:9003 \\ questdb/questdb:9.1.0 This command starts a Docker container from questdb/questdb image. In addition, it exposes some ports, allowing you to explore QuestDB. In order to configure QuestDB, it is recommended to mount a volume to allow data persistance. This can be done by adding a -v flag to the above command: -v \"/host/volume/location:/var/lib/questdb\" Below each parameter is described in detail. -p parameter to expose ports ​ This parameter will expose a port to the host. You can specify: -p 9000:9000 - REST API and Web Console -p 9009:9009 - InfluxDB line protocol -p 8812:8812 - Postgres wire protocol -p 9003:9003 - Min health server All ports are optional, you can pick only the ones you need. For example, it is enough to"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "-p 9009:9009 - InfluxDB line protocol -p 8812:8812 - Postgres wire protocol -p 9003:9003 - Min health server All ports are optional, you can pick only the ones you need. For example, it is enough to expose 8812 if you only plan to use Postgres wire protocol . -v parameter to mount storage ​ This parameter will make a local directory available to QuestDB Docker container. It will have all data ingested to QuestDB, server logs and configuration. The QuestDB root_directory is located at the /var/lib/questdb path in the container. Docker image version ​ By default, questdb/questdb points to the latest QuestDB version available on Docker. However, it is recommended to define the version used. questdb/questdb:9.1.0 Environment variables ​ Server configuration can be passed to QuestDB running in Docker by using the -e flag to pass an environment variable to a container: docker run -p 4000:4000 -e QDB_HTTP_BIND_TO=0.0.0.0:4000 questdb/questdb For a list of configuration options, see Configuration . Container status ​ You can check the status of your container with docker ps . It also lists the exposed ports, container name, uptime and more: Finding container status with docker ps docker p"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "tion . Container status ​ You can check the status of your container with docker ps . It also lists the exposed ports, container name, uptime and more: Finding container status with docker ps docker ps Result of docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES dd363939f261 questdb/questdb \"/app/bin/java -m io…\" 3 seconds ago Up 2 seconds 8812/tcp, 9000/tcp frosty_gauss This container: has an id of dd363939f261 uses ports 8812 & 9000 , for Postgres wire protocol and HTTP respectively is using a questdb/questdb image ran java to start the binary is 3 seconds old has been up for 2 seconds has the unfortunate name of frosty_gauss For full container status information, see the docker ps manual . Debugging container logs ​ Docker may generate a runtime error. The error may not be accurate, as the true culprit is often indicated higher up in the logs. To see the full log, retrieve the UUID - also known as the CONTAINER ID - using docker ps : Finding the CONTAINER ID CONTAINER ID IMAGE ... dd363939f261 questdb/questdb ... Now pass the CONTAINER ID - or dd363939f261 - to the docker logs command: Generating a docker log from a CONTAINER ID $ docker logs dd363939f261 No argumen"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "D IMAGE ... dd363939f261 questdb/questdb ... Now pass the CONTAINER ID - or dd363939f261 - to the docker logs command: Generating a docker log from a CONTAINER ID $ docker logs dd363939f261 No arguments found, start with default arguments Running as questdb user Log configuration loaded from: /var/lib/questdb/conf/log.conf ... ... Note that the log will pull from /var/lib/questdb/conf/log.conf by default. Sharing this log when seeking support for Docker deployments will help us find the root cause. Importing data and sending queries ​ When QuestDB is running, you can start interacting with it: Port 9000 is for REST. More info is available on the REST documentation page . Port 8812 is used for Postgres. Check our Postgres reference page . Port 9009 is dedicated to InfluxDB Line Protocol. Consult our InfluxDB protocol page . Data persistence ​ Mounting a volume ​ Volumes can be mounted to the QuestDB Docker container so that data may be persisted or server configuration settings may be passed to an instance. The following example demonstrated how to mount the current directory to a QuestDB container using the -v flag in a Docker run command: Mounting a volume docker run -p 9000:9000"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "assed to an instance. The following example demonstrated how to mount the current directory to a QuestDB container using the -v flag in a Docker run command: Mounting a volume docker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ -v \"$(pwd):/var/lib/questdb\" \\ questdb/questdb:9.1.0 The current directory will then have data persisted to disk for convenient migration or backups: Current directory contents ├── conf │ └── server.conf ├── db ├── log ├── public └── snapshot (optional) A server configuration file can also be provided by mounting a local directory in a QuestDB container. Given the following configuration file which overrides the default HTTP bind property: ./server.conf http.bind.to=0.0.0.0:4000 Running the container with the -v flag allows for mounting the current directory to QuestDB's conf directory in the container. With the server configuration above, HTTP ports for the Web Console and REST API will be available on localhost:4000 : docker run -v \"$(pwd):/var/lib/questdb/conf\" -p 4000:4000 questdb/questdb note If you wish to use ZFS for your QuestDB deployment, with Docker, then you will need to enable ZFS on the host volume that Docker uses. Please se"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "var/lib/questdb/conf\" -p 4000:4000 questdb/questdb note If you wish to use ZFS for your QuestDB deployment, with Docker, then you will need to enable ZFS on the host volume that Docker uses. Please see the docker documentation for more information. Upgrade QuestDB version ​ It is possible to upgrade your QuestDB instance on Docker when a volume is mounted to maintain data persistence. note Check the release notes and ensure that necessary backup is completed. Upgrading an instance is possible only when the original instance has a volume mounted. Without mounting a volume for the original instance, the following steps create a new instance and data in the old instance cannot be retrieved. Run docker ps to copy the container name or ID: Container status # The existing QuestDB version is 6.5.2: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES dd363939f261 questdb/questdb:6.5.2 \"/app/bin/java -m io…\" 3 seconds ago Up 2 seconds 8812/tcp, 9000/tcp frosty_gauss Stop the instance and then remove the container: docker stop dd363939f261 docker rm dd363939f261 Download the latest QuestDB image: docker pull questdb/questdb:9.1.0 Start a new container with the new version and the same volu"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "remove the container: docker stop dd363939f261 docker rm dd363939f261 Download the latest QuestDB image: docker pull questdb/questdb:9.1.0 Start a new container with the new version and the same volume mounted: docker run -p 8812:8812 -p 9000:9000 -v \"$(pwd):/var/lib/questdb\" questdb/questdb:9.1.0 Writing logs to disk ​ When mounting a volume to a Docker container, a logging configuration file may be provided in the container located at /conf/log.conf : Current directory contents └── conf ├── log.conf └── server.conf For example, a file with the following contents can be created: ./conf/log.conf # list of configured writers writers=file,stdout,http.min # file writer w.file.class=io.questdb.log.LogFileWriter w.file.location=questdb-docker.log w.file.level=INFO,ERROR,DEBUG # stdout w.stdout.class=io.questdb.log.LogConsoleWriter w.stdout.level=INFO # min http server, used monitoring w.http.min.class=io.questdb.log.LogConsoleWriter w.http.min.level=ERROR w.http.min.scope=http-min-server The current directory can be mounted: Mounting the current directory to a QuestDB container docker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ -v \"$(pwd):/root/.questdb/\" questdb/qu"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "he current directory can be mounted: Mounting the current directory to a QuestDB container docker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ -v \"$(pwd):/root/.questdb/\" questdb/questdb The container logs will be written to disk using the logging level and file name provided in the conf/log.conf file, in this case in ./questdb-docker.log : Current directory tree ├── conf │ ├── log.conf │ └── server.conf ├── db │ ├── table1 │ └── table2 ├── public │ ├── ui / assets │ ├── ... │ └── version.txt └── questdb-docker.log For more information on logging, see the configuration reference documentation . Restart an existing container ​ Running the following command will create a new container for the QuestDB image: docker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ questdb/questdb By giving the container a name with --name container_name , we have an easy way to refer to the container created by run later on: docker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ --name docker_questdb \\ questdb/questdb If we want to re-use this container and its data after it has been stopped, we can use the following commands: # bring the container up d"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": ":8812 \\ -p 9003:9003 \\ --name docker_questdb \\ questdb/questdb If we want to re-use this container and its data after it has been stopped, we can use the following commands: # bring the container up docker start docker_questdb # shut the container down docker stop docker_questdb Alternatively, restart it using the CONTAINER ID : Starting a container by CONTAINER ID docker start dd363939f261 Edit this page Previous Text Next Kubernetes\n\nDeployment\n\nDocker\n\nOn this page\n\nUsing Docker with QuestDB QuestDB has images for both Linux/macOS and Windows on Docker Hub . Install Docker ​ To begin, install Docker. You can find guides for your platform on the official documentation . Run QuestDB image ​ Once Docker is installed, you will need to pull QuestDB's image from Docker Hub and create a container. This can be done with a single command using: docker run \\ -p 9000:9000 -p 9009:9009 -p 8812:8812 -p 9003:9003 \\ questdb/questdb:9.1.0 This command starts a Docker container from questdb/questdb image. In addition, it exposes some ports, allowing you to explore QuestDB. In order to configure QuestDB, it is recommended to mount a volume to allow data persistance. This can be done by adding a -"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "image. In addition, it exposes some ports, allowing you to explore QuestDB. In order to configure QuestDB, it is recommended to mount a volume to allow data persistance. This can be done by adding a -v flag to the above command: -v \"/host/volume/location:/var/lib/questdb\" Below each parameter is described in detail. -p parameter to expose ports ​ This parameter will expose a port to the host. You can specify: -p 9000:9000 - REST API and Web Console -p 9009:9009 - InfluxDB line protocol -p 8812:8812 - Postgres wire protocol -p 9003:9003 - Min health server All ports are optional, you can pick only the ones you need. For example, it is enough to expose 8812 if you only plan to use Postgres wire protocol . -v parameter to mount storage ​ This parameter will make a local directory available to QuestDB Docker container. It will have all data ingested to QuestDB, server logs and configuration. The QuestDB root_directory is located at the /var/lib/questdb path in the container. Docker image version ​ By default, questdb/questdb points to the latest QuestDB version available on Docker. However, it is recommended to define the version used. questdb/questdb:9.1.0 Environment variables ​ Serv"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "ersion ​ By default, questdb/questdb points to the latest QuestDB version available on Docker. However, it is recommended to define the version used. questdb/questdb:9.1.0 Environment variables ​ Server configuration can be passed to QuestDB running in Docker by using the -e flag to pass an environment variable to a container: docker run -p 4000:4000 -e QDB_HTTP_BIND_TO=0.0.0.0:4000 questdb/questdb For a list of configuration options, see Configuration . Container status ​ You can check the status of your container with docker ps . It also lists the exposed ports, container name, uptime and more: Finding container status with docker ps docker ps Result of docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES dd363939f261 questdb/questdb \"/app/bin/java -m io…\" 3 seconds ago Up 2 seconds 8812/tcp, 9000/tcp frosty_gauss This container: has an id of dd363939f261 uses ports 8812 & 9000 , for Postgres wire protocol and HTTP respectively is using a questdb/questdb image ran java to start the binary is 3 seconds old has been up for 2 seconds has the unfortunate name of frosty_gauss For full container status information, see the docker ps manual . Debugging container logs ​ Docker"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "start the binary is 3 seconds old has been up for 2 seconds has the unfortunate name of frosty_gauss For full container status information, see the docker ps manual . Debugging container logs ​ Docker may generate a runtime error. The error may not be accurate, as the true culprit is often indicated higher up in the logs. To see the full log, retrieve the UUID - also known as the CONTAINER ID - using docker ps : Finding the CONTAINER ID CONTAINER ID IMAGE ... dd363939f261 questdb/questdb ... Now pass the CONTAINER ID - or dd363939f261 - to the docker logs command: Generating a docker log from a CONTAINER ID $ docker logs dd363939f261 No arguments found, start with default arguments Running as questdb user Log configuration loaded from: /var/lib/questdb/conf/log.conf ... ... Note that the log will pull from /var/lib/questdb/conf/log.conf by default. Sharing this log when seeking support for Docker deployments will help us find the root cause. Importing data and sending queries ​ When QuestDB is running, you can start interacting with it: Port 9000 is for REST. More info is available on the REST documentation page . Port 8812 is used for Postgres. Check our Postgres reference page ."}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "estDB is running, you can start interacting with it: Port 9000 is for REST. More info is available on the REST documentation page . Port 8812 is used for Postgres. Check our Postgres reference page . Port 9009 is dedicated to InfluxDB Line Protocol. Consult our InfluxDB protocol page . Data persistence ​ Mounting a volume ​ Volumes can be mounted to the QuestDB Docker container so that data may be persisted or server configuration settings may be passed to an instance. The following example demonstrated how to mount the current directory to a QuestDB container using the -v flag in a Docker run command: Mounting a volume docker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ -v \"$(pwd):/var/lib/questdb\" \\ questdb/questdb:9.1.0 The current directory will then have data persisted to disk for convenient migration or backups: Current directory contents ├── conf │ └── server.conf ├── db ├── log ├── public └── snapshot (optional) A server configuration file can also be provided by mounting a local directory in a QuestDB container. Given the following configuration file which overrides the default HTTP bind property: ./server.conf http.bind.to=0.0.0.0:4000 Running the conta"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "d by mounting a local directory in a QuestDB container. Given the following configuration file which overrides the default HTTP bind property: ./server.conf http.bind.to=0.0.0.0:4000 Running the container with the -v flag allows for mounting the current directory to QuestDB's conf directory in the container. With the server configuration above, HTTP ports for the Web Console and REST API will be available on localhost:4000 : docker run -v \"$(pwd):/var/lib/questdb/conf\" -p 4000:4000 questdb/questdb note If you wish to use ZFS for your QuestDB deployment, with Docker, then you will need to enable ZFS on the host volume that Docker uses. Please see the docker documentation for more information. Upgrade QuestDB version ​ It is possible to upgrade your QuestDB instance on Docker when a volume is mounted to maintain data persistence. note Check the release notes and ensure that necessary backup is completed. Upgrading an instance is possible only when the original instance has a volume mounted. Without mounting a volume for the original instance, the following steps create a new instance and data in the old instance cannot be retrieved. Run docker ps to copy the container name or ID: Con"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "ed. Without mounting a volume for the original instance, the following steps create a new instance and data in the old instance cannot be retrieved. Run docker ps to copy the container name or ID: Container status # The existing QuestDB version is 6.5.2: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES dd363939f261 questdb/questdb:6.5.2 \"/app/bin/java -m io…\" 3 seconds ago Up 2 seconds 8812/tcp, 9000/tcp frosty_gauss Stop the instance and then remove the container: docker stop dd363939f261 docker rm dd363939f261 Download the latest QuestDB image: docker pull questdb/questdb:9.1.0 Start a new container with the new version and the same volume mounted: docker run -p 8812:8812 -p 9000:9000 -v \"$(pwd):/var/lib/questdb\" questdb/questdb:9.1.0 Writing logs to disk ​ When mounting a volume to a Docker container, a logging configuration file may be provided in the container located at /conf/log.conf : Current directory contents └── conf ├── log.conf └── server.conf For example, a file with the following contents can be created: ./conf/log.conf # list of configured writers writers=file,stdout,http.min # file writer w.file.class=io.questdb.log.LogFileWriter w.file.location=questdb-docker"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "the following contents can be created: ./conf/log.conf # list of configured writers writers=file,stdout,http.min # file writer w.file.class=io.questdb.log.LogFileWriter w.file.location=questdb-docker.log w.file.level=INFO,ERROR,DEBUG # stdout w.stdout.class=io.questdb.log.LogConsoleWriter w.stdout.level=INFO # min http server, used monitoring w.http.min.class=io.questdb.log.LogConsoleWriter w.http.min.level=ERROR w.http.min.scope=http-min-server The current directory can be mounted: Mounting the current directory to a QuestDB container docker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ -v \"$(pwd):/root/.questdb/\" questdb/questdb The container logs will be written to disk using the logging level and file name provided in the conf/log.conf file, in this case in ./questdb-docker.log : Current directory tree ├── conf │ ├── log.conf │ └── server.conf ├── db │ ├── table1 │ └── table2 ├── public │ ├── ui / assets │ ├── ... │ └── version.txt └── questdb-docker.log For more information on logging, see the configuration reference documentation . Restart an existing container ​ Running the following command will create a new container for the QuestDB image: docker run -p"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Introduction", "text": "information on logging, see the configuration reference documentation . Restart an existing container ​ Running the following command will create a new container for the QuestDB image: docker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ questdb/questdb By giving the container a name with --name container_name , we have an easy way to refer to the container created by run later on: docker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ --name docker_questdb \\ questdb/questdb If we want to re-use this container and its data after it has been stopped, we can use the following commands: # bring the container up docker start docker_questdb # shut the container down docker stop docker_questdb Alternatively, restart it using the CONTAINER ID : Starting a container by CONTAINER ID docker start dd363939f261"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Using Docker with QuestDB", "text": "QuestDB has images for both Linux/macOS and Windows on Docker Hub ."}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Install Docker ​", "text": "To begin, install Docker. You can find guides for your platform on the official documentation ."}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Run QuestDB image ​", "text": "Once Docker is installed, you will need to pull QuestDB's image from Docker Hub and create a container.\n\nThis can be done with a single command using:\n\ndocker run \\ -p 9000:9000 -p 9009:9009 -p 8812:8812 -p 9003:9003 \\ questdb/questdb:9.1.0\n\ndocker run \\ -p 9000:9000 -p 9009:9009 -p 8812:8812 -p 9003:9003 \\ questdb/questdb:9.1.0\n\ndocker run \\ -p 9000:9000 -p 9009:9009 -p 8812:8812 -p 9003:9003 \\ questdb/questdb:9.1.0\n\ndocker run \\ -p 9000:9000 -p 9009:9009 -p 8812:8812 -p 9003:9003 \\ questdb/questdb:9.1.0\n\nThis command starts a Docker container from questdb/questdb image. In addition, it exposes some ports, allowing you to explore QuestDB.\n\nquestdb/questdb\n\nIn order to configure QuestDB, it is recommended to mount a volume to allow data persistance. This can be done by adding a -v flag to the above command:\n\n-v\n\n-v \"/host/volume/location:/var/lib/questdb\"\n\n-v \"/host/volume/location:/var/lib/questdb\"\n\n-v \"/host/volume/location:/var/lib/questdb\"\n\n-v \"/host/volume/location:/var/lib/questdb\"\n\nBelow each parameter is described in detail."}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "-p parameter to expose ports ​", "text": "-p\n\nThis parameter will expose a port to the host. You can specify:\n\n-p 9000:9000 - REST API and Web Console\n\n-p 9000:9000\n\n-p 9009:9009 - InfluxDB line protocol\n\n-p 9009:9009\n\n-p 8812:8812 - Postgres wire protocol\n\n-p 8812:8812\n\n-p 9003:9003 - Min health server\n\n-p 9003:9003\n\nAll ports are optional, you can pick only the ones you need. For example, it is enough to expose 8812 if you only plan to use Postgres wire protocol .\n\n8812"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "-v parameter to mount storage ​", "text": "-v\n\nThis parameter will make a local directory available to QuestDB Docker container. It will have all data ingested to QuestDB, server logs and configuration.\n\nThe QuestDB root_directory is located at the /var/lib/questdb path in the container.\n\n/var/lib/questdb"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Docker image version ​", "text": "By default, questdb/questdb points to the latest QuestDB version available on Docker. However, it is recommended to define the version used.\n\nquestdb/questdb\n\nquestdb/questdb:9.1.0\n\nquestdb/questdb:9.1.0\n\nquestdb/questdb:9.1.0\n\nquestdb/questdb:9.1.0"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Environment variables ​", "text": "Server configuration can be passed to QuestDB running in Docker by using the -e flag to pass an environment variable to a container:\n\n-e\n\ndocker run -p 4000:4000 -e QDB_HTTP_BIND_TO=0.0.0.0:4000 questdb/questdb\n\ndocker run -p 4000:4000 -e QDB_HTTP_BIND_TO=0.0.0.0:4000 questdb/questdb\n\ndocker run -p 4000:4000 -e QDB_HTTP_BIND_TO=0.0.0.0:4000 questdb/questdb\n\ndocker run -p 4000:4000 -e QDB_HTTP_BIND_TO=0.0.0.0:4000 questdb/questdb\n\nFor a list of configuration options, see Configuration ."}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Container status ​", "text": "You can check the status of your container with docker ps .\n\ndocker ps\n\nIt also lists the exposed ports, container name, uptime and more:\n\nFinding container status with docker ps docker ps\n\nFinding container status with docker ps\n\ndocker ps\n\ndocker ps\n\ndocker ps\n\nResult of docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES dd363939f261 questdb/questdb \"/app/bin/java -m io…\" 3 seconds ago Up 2 seconds 8812/tcp, 9000/tcp frosty_gauss\n\nResult of docker ps\n\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES dd363939f261 questdb/questdb \"/app/bin/java -m io…\" 3 seconds ago Up 2 seconds 8812/tcp, 9000/tcp frosty_gauss\n\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES dd363939f261 questdb/questdb \"/app/bin/java -m io…\" 3 seconds ago Up 2 seconds 8812/tcp, 9000/tcp frosty_gauss\n\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES dd363939f261 questdb/questdb \"/app/bin/java -m io…\" 3 seconds ago Up 2 seconds 8812/tcp, 9000/tcp frosty_gauss\n\nThis container:\n\nhas an id of dd363939f261\n\ndd363939f261\n\nuses ports 8812 & 9000 , for Postgres wire protocol and HTTP respectively\n\n8812\n\n9000\n\nis using a questdb/questdb image\n\nquestdb/questdb\n\nran java to start the binary\n\nis 3"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Container status ​", "text": "d363939f261\n\ndd363939f261\n\nuses ports 8812 & 9000 , for Postgres wire protocol and HTTP respectively\n\n8812\n\n9000\n\nis using a questdb/questdb image\n\nquestdb/questdb\n\nran java to start the binary\n\nis 3 seconds old\n\nhas been up for 2 seconds\n\nhas the unfortunate name of frosty_gauss\n\nfrosty_gauss\n\nFor full container status information, see the docker ps manual .\n\ndocker ps"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Debugging container logs ​", "text": "Docker may generate a runtime error.\n\nThe error may not be accurate, as the true culprit is often indicated higher up in the logs.\n\nTo see the full log, retrieve the UUID - also known as the CONTAINER ID - using docker ps :\n\nCONTAINER ID\n\ndocker ps\n\nFinding the CONTAINER ID CONTAINER ID IMAGE ... dd363939f261 questdb/questdb ...\n\nFinding the CONTAINER ID\n\nCONTAINER ID IMAGE ... dd363939f261 questdb/questdb ...\n\nCONTAINER ID IMAGE ... dd363939f261 questdb/questdb ...\n\nCONTAINER ID IMAGE ... dd363939f261 questdb/questdb ...\n\nNow pass the CONTAINER ID - or dd363939f261 - to the docker logs command:\n\nCONTAINER ID\n\ndd363939f261\n\ndocker logs\n\nGenerating a docker log from a CONTAINER ID $ docker logs dd363939f261 No arguments found, start with default arguments Running as questdb user Log configuration loaded from: /var/lib/questdb/conf/log.conf ... ...\n\nGenerating a docker log from a CONTAINER ID\n\n$ docker logs dd363939f261 No arguments found, start with default arguments Running as questdb user Log configuration loaded from: /var/lib/questdb/conf/log.conf ... ...\n\n$ docker logs dd363939f261 No arguments found, start with default arguments Running as questdb user Log configuration loaded"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Debugging container logs ​", "text": "user Log configuration loaded from: /var/lib/questdb/conf/log.conf ... ...\n\n$ docker logs dd363939f261 No arguments found, start with default arguments Running as questdb user Log configuration loaded from: /var/lib/questdb/conf/log.conf ... ...\n\n$ docker logs dd363939f261 No arguments found, start with default arguments Running as questdb user Log configuration loaded from: /var/lib/questdb/conf/log.conf ... ...\n\nNote that the log will pull from /var/lib/questdb/conf/log.conf by default.\n\n/var/lib/questdb/conf/log.conf\n\nSharing this log when seeking support for Docker deployments will help us find the root cause."}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Importing data and sending queries ​", "text": "When QuestDB is running, you can start interacting with it:\n\nPort 9000 is for REST. More info is available on the REST documentation page .\n\n9000\n\nPort 8812 is used for Postgres. Check our Postgres reference page .\n\n8812\n\nPort 9009 is dedicated to InfluxDB Line Protocol. Consult our InfluxDB protocol page .\n\n9009"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Mounting a volume ​", "text": "Volumes can be mounted to the QuestDB Docker container so that data may be persisted or server configuration settings may be passed to an instance. The following example demonstrated how to mount the current directory to a QuestDB container using the -v flag in a Docker run command:\n\n-v\n\nrun\n\nMounting a volume docker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ -v \"$(pwd):/var/lib/questdb\" \\ questdb/questdb:9.1.0\n\nMounting a volume\n\ndocker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ -v \"$(pwd):/var/lib/questdb\" \\ questdb/questdb:9.1.0\n\ndocker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ -v \"$(pwd):/var/lib/questdb\" \\ questdb/questdb:9.1.0\n\ndocker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ -v \"$(pwd):/var/lib/questdb\" \\ questdb/questdb:9.1.0\n\nThe current directory will then have data persisted to disk for convenient migration or backups:\n\nCurrent directory contents ├── conf │ └── server.conf ├── db ├── log ├── public └── snapshot (optional)\n\nCurrent directory contents\n\n├── conf │ └── server.conf ├── db ├── log ├── public └── snapshot (optional)\n\n├── conf │ └── server.conf ├── db ├── log ├── public └── sna"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Mounting a volume ​", "text": "─ public └── snapshot (optional)\n\nCurrent directory contents\n\n├── conf │ └── server.conf ├── db ├── log ├── public └── snapshot (optional)\n\n├── conf │ └── server.conf ├── db ├── log ├── public └── snapshot (optional)\n\n├── conf │ └── server.conf ├── db ├── log ├── public └── snapshot (optional)\n\nA server configuration file can also be provided by mounting a local directory in a QuestDB container. Given the following configuration file which overrides the default HTTP bind property:\n\n./server.conf http.bind.to=0.0.0.0:4000\n\n./server.conf\n\nhttp.bind.to=0.0.0.0:4000\n\nhttp.bind.to=0.0.0.0:4000\n\nhttp.bind.to=0.0.0.0:4000\n\nRunning the container with the -v flag allows for mounting the current directory to QuestDB's conf directory in the container. With the server configuration above, HTTP ports for the Web Console and REST API will be available on localhost:4000 :\n\n-v\n\nconf\n\ndocker run -v \"$(pwd):/var/lib/questdb/conf\" -p 4000:4000 questdb/questdb\n\ndocker run -v \"$(pwd):/var/lib/questdb/conf\" -p 4000:4000 questdb/questdb\n\ndocker run -v \"$(pwd):/var/lib/questdb/conf\" -p 4000:4000 questdb/questdb\n\ndocker run -v \"$(pwd):/var/lib/questdb/conf\" -p 4000:4000 questdb/questdb\n\nnote If you wish to"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Mounting a volume ​", "text": "-p 4000:4000 questdb/questdb\n\ndocker run -v \"$(pwd):/var/lib/questdb/conf\" -p 4000:4000 questdb/questdb\n\ndocker run -v \"$(pwd):/var/lib/questdb/conf\" -p 4000:4000 questdb/questdb\n\nnote If you wish to use ZFS for your QuestDB deployment, with Docker, then you will need to enable ZFS on the host volume that Docker uses. Please see the docker documentation for more information.\n\nnote\n\nIf you wish to use ZFS for your QuestDB deployment, with Docker, then you will need to enable ZFS on the host volume that Docker uses. Please see the docker documentation for more information.\n\nIf you wish to use ZFS for your QuestDB deployment, with Docker, then you will need to enable ZFS on the host volume that Docker uses.\n\nPlease see the docker documentation for more information."}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Upgrade QuestDB version ​", "text": "It is possible to upgrade your QuestDB instance on Docker when a volume is mounted to maintain data persistence.\n\nnote Check the release notes and ensure that necessary backup is completed. Upgrading an instance is possible only when the original instance has a volume mounted. Without mounting a volume for the original instance, the following steps create a new instance and data in the old instance cannot be retrieved.\n\nnote\n\nCheck the release notes and ensure that necessary backup is completed. Upgrading an instance is possible only when the original instance has a volume mounted. Without mounting a volume for the original instance, the following steps create a new instance and data in the old instance cannot be retrieved.\n\nCheck the release notes and ensure that necessary backup is completed.\n\nUpgrading an instance is possible only when the original instance has a volume mounted. Without mounting a volume for the original instance, the following steps create a new instance and data in the old instance cannot be retrieved.\n\nRun docker ps to copy the container name or ID:\n\ndocker ps\n\nContainer status # The existing QuestDB version is 6.5.2: CONTAINER ID IMAGE COMMAND CREATED STATUS"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Upgrade QuestDB version ​", "text": "n the old instance cannot be retrieved.\n\nRun docker ps to copy the container name or ID:\n\ndocker ps\n\nContainer status # The existing QuestDB version is 6.5.2: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES dd363939f261 questdb/questdb:6.5.2 \"/app/bin/java -m io…\" 3 seconds ago Up 2 seconds 8812/tcp, 9000/tcp frosty_gauss\n\nContainer status\n\n# The existing QuestDB version is 6.5.2: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES dd363939f261 questdb/questdb:6.5.2 \"/app/bin/java -m io…\" 3 seconds ago Up 2 seconds 8812/tcp, 9000/tcp frosty_gauss\n\n# The existing QuestDB version is 6.5.2: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES dd363939f261 questdb/questdb:6.5.2 \"/app/bin/java -m io…\" 3 seconds ago Up 2 seconds 8812/tcp, 9000/tcp frosty_gauss\n\n# The existing QuestDB version is 6.5.2: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES dd363939f261 questdb/questdb:6.5.2 \"/app/bin/java -m io…\" 3 seconds ago Up 2 seconds 8812/tcp, 9000/tcp frosty_gauss\n\nStop the instance and then remove the container:\n\ndocker stop dd363939f261 docker rm dd363939f261\n\ndocker stop dd363939f261 docker rm dd363939f261\n\ndocker stop dd363939f261 docker rm dd363939f261\n\ndocker stop dd3"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Upgrade QuestDB version ​", "text": "tance and then remove the container:\n\ndocker stop dd363939f261 docker rm dd363939f261\n\ndocker stop dd363939f261 docker rm dd363939f261\n\ndocker stop dd363939f261 docker rm dd363939f261\n\ndocker stop dd363939f261 docker rm dd363939f261\n\nDownload the latest QuestDB image:\n\ndocker pull questdb/questdb:9.1.0\n\ndocker pull questdb/questdb:9.1.0\n\ndocker pull questdb/questdb:9.1.0\n\ndocker pull questdb/questdb:9.1.0\n\nStart a new container with the new version and the same volume mounted:\n\ndocker run -p 8812:8812 -p 9000:9000 -v \"$(pwd):/var/lib/questdb\" questdb/questdb:9.1.0\n\ndocker run -p 8812:8812 -p 9000:9000 -v \"$(pwd):/var/lib/questdb\" questdb/questdb:9.1.0\n\ndocker run -p 8812:8812 -p 9000:9000 -v \"$(pwd):/var/lib/questdb\" questdb/questdb:9.1.0\n\ndocker run -p 8812:8812 -p 9000:9000 -v \"$(pwd):/var/lib/questdb\" questdb/questdb:9.1.0"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Writing logs to disk ​", "text": "When mounting a volume to a Docker container, a logging configuration file may be provided in the container located at /conf/log.conf :\n\n/conf/log.conf\n\nCurrent directory contents └── conf ├── log.conf └── server.conf\n\nCurrent directory contents\n\n└── conf ├── log.conf └── server.conf\n\n└── conf ├── log.conf └── server.conf\n\n└── conf ├── log.conf └── server.conf\n\nFor example, a file with the following contents can be created:\n\n./conf/log.conf # list of configured writers writers=file,stdout,http.min # file writer w.file.class=io.questdb.log.LogFileWriter w.file.location=questdb-docker.log w.file.level=INFO,ERROR,DEBUG # stdout w.stdout.class=io.questdb.log.LogConsoleWriter w.stdout.level=INFO # min http server, used monitoring w.http.min.class=io.questdb.log.LogConsoleWriter w.http.min.level=ERROR w.http.min.scope=http-min-server\n\n./conf/log.conf\n\n# list of configured writers writers=file,stdout,http.min # file writer w.file.class=io.questdb.log.LogFileWriter w.file.location=questdb-docker.log w.file.level=INFO,ERROR,DEBUG # stdout w.stdout.class=io.questdb.log.LogConsoleWriter w.stdout.level=INFO # min http server, used monitoring w.http.min.class=io.questdb.log.LogConsoleWriter w.h"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Writing logs to disk ​", "text": "er.log w.file.level=INFO,ERROR,DEBUG # stdout w.stdout.class=io.questdb.log.LogConsoleWriter w.stdout.level=INFO # min http server, used monitoring w.http.min.class=io.questdb.log.LogConsoleWriter w.http.min.level=ERROR w.http.min.scope=http-min-server\n\n# list of configured writers writers=file,stdout,http.min # file writer w.file.class=io.questdb.log.LogFileWriter w.file.location=questdb-docker.log w.file.level=INFO,ERROR,DEBUG # stdout w.stdout.class=io.questdb.log.LogConsoleWriter w.stdout.level=INFO # min http server, used monitoring w.http.min.class=io.questdb.log.LogConsoleWriter w.http.min.level=ERROR w.http.min.scope=http-min-server\n\n# list of configured writers writers=file,stdout,http.min # file writer w.file.class=io.questdb.log.LogFileWriter w.file.location=questdb-docker.log w.file.level=INFO,ERROR,DEBUG # stdout w.stdout.class=io.questdb.log.LogConsoleWriter w.stdout.level=INFO # min http server, used monitoring w.http.min.class=io.questdb.log.LogConsoleWriter w.http.min.level=ERROR w.http.min.scope=http-min-server\n\nThe current directory can be mounted:\n\nMounting the current directory to a QuestDB container docker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 90"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Writing logs to disk ​", "text": "level=ERROR w.http.min.scope=http-min-server\n\nThe current directory can be mounted:\n\nMounting the current directory to a QuestDB container docker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ -v \"$(pwd):/root/.questdb/\" questdb/questdb\n\nMounting the current directory to a QuestDB container\n\ndocker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ -v \"$(pwd):/root/.questdb/\" questdb/questdb\n\ndocker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ -v \"$(pwd):/root/.questdb/\" questdb/questdb\n\ndocker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ -v \"$(pwd):/root/.questdb/\" questdb/questdb\n\nThe container logs will be written to disk using the logging level and file name provided in the conf/log.conf file, in this case in ./questdb-docker.log :\n\nconf/log.conf\n\n./questdb-docker.log\n\nCurrent directory tree ├── conf │ ├── log.conf │ └── server.conf ├── db │ ├── table1 │ └── table2 ├── public │ ├── ui / assets │ ├── ... │ └── version.txt └── questdb-docker.log\n\nCurrent directory tree\n\n├── conf │ ├── log.conf │ └── server.conf ├── db │ ├── table1 │ └── table2 ├── public │ ├── ui / assets │ ├── ... │ └── version.txt └── questdb-d"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Writing logs to disk ​", "text": "on.txt └── questdb-docker.log\n\nCurrent directory tree\n\n├── conf │ ├── log.conf │ └── server.conf ├── db │ ├── table1 │ └── table2 ├── public │ ├── ui / assets │ ├── ... │ └── version.txt └── questdb-docker.log\n\n├── conf │ ├── log.conf │ └── server.conf ├── db │ ├── table1 │ └── table2 ├── public │ ├── ui / assets │ ├── ... │ └── version.txt └── questdb-docker.log\n\n├── conf │ ├── log.conf │ └── server.conf ├── db │ ├── table1 │ └── table2 ├── public │ ├── ui / assets │ ├── ... │ └── version.txt └── questdb-docker.log\n\nFor more information on logging, see the configuration reference documentation ."}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Restart an existing container ​", "text": "Running the following command will create a new container for the QuestDB image:\n\ndocker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ questdb/questdb\n\ndocker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ questdb/questdb\n\ndocker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ questdb/questdb\n\ndocker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ questdb/questdb\n\nBy giving the container a name with --name container_name , we have an easy way to refer to the container created by run later on:\n\n--name container_name\n\ndocker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ --name docker_questdb \\ questdb/questdb\n\ndocker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ --name docker_questdb \\ questdb/questdb\n\ndocker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ --name docker_questdb \\ questdb/questdb\n\ndocker run -p 9000:9000 \\ -p 9009:9009 \\ -p 8812:8812 \\ -p 9003:9003 \\ --name docker_questdb \\ questdb/questdb\n\nIf we want to re-use this container and its data after it has been stopped, we can use the following commands:\n\n# bring the container up docker start docker_questd"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Restart an existing container ​", "text": "me docker_questdb \\ questdb/questdb\n\nIf we want to re-use this container and its data after it has been stopped, we can use the following commands:\n\n# bring the container up docker start docker_questdb # shut the container down docker stop docker_questdb\n\n# bring the container up docker start docker_questdb # shut the container down docker stop docker_questdb\n\n# bring the container up docker start docker_questdb # shut the container down docker stop docker_questdb\n\n# bring the container up docker start docker_questdb # shut the container down docker stop docker_questdb\n\nAlternatively, restart it using the CONTAINER ID :\n\nCONTAINER ID\n\nStarting a container by CONTAINER ID docker start dd363939f261\n\nStarting a container by CONTAINER ID\n\ndocker start dd363939f261\n\ndocker start dd363939f261\n\ndocker start dd363939f261\n\nEdit this page\n\nEdit this page\n\nPrevious\n\nText\n\nNext\n\nKubernetes\n\nInstall Docker Run QuestDB image -p parameter to expose ports -v parameter to mount storage Docker image version Environment variables Container status Debugging container logs Importing data and sending queries Data persistence Mounting a volume Upgrade QuestDB version Writing logs to disk Restart an exist"}
{"source_url": "https://questdb.com/docs/deployment/docker", "title": "Using Docker with QuestDB", "section": "Restart an existing container ​", "text": "sion Environment variables Container status Debugging container logs Importing data and sending queries Data persistence Mounting a volume Upgrade QuestDB version Writing logs to disk Restart an existing container\n\nInstall Docker Run QuestDB image -p parameter to expose ports -v parameter to mount storage Docker image version Environment variables Container status Debugging container logs Importing data and sending queries Data persistence Mounting a volume Upgrade QuestDB version Writing logs to disk Restart an existing container\n\nInstall Docker\n\nRun QuestDB image -p parameter to expose ports -v parameter to mount storage Docker image version\n\n-p parameter to expose ports\n\n-p\n\n-v parameter to mount storage\n\n-v\n\nDocker image version\n\nEnvironment variables\n\nContainer status Debugging container logs\n\nDebugging container logs\n\nImporting data and sending queries\n\nData persistence Mounting a volume Upgrade QuestDB version Writing logs to disk Restart an existing container\n\nMounting a volume\n\nUpgrade QuestDB version\n\nWriting logs to disk\n\nRestart an existing container"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Introduction", "text": "Operations Migrate to Enterprise edition On this page Migrate to Enterprise edition This page covers the steps to migrate a database instance from QuestDB open source edition to QuestDB Enterprise edition. Overview ​ The QuestDB Enterprise edition is based on the Open Source edition of the database. Migrating from QuestDB Open Source to QuestDB Enterprise is a straightforward process. It is nonetheless important to follow the steps carefully to ensure a successful migration. Migration Workflow ​ Step 0: Backup your data ​ Before you start the migration process, consider backing up your data. You can follow the instructions in the Backup and restore guide create a restore point. It is unlikely that you will need the restore point, but it is a good idea to have it in case something goes wrong. In short: Checkpoint the database: CHECKPOINT CREATE Back up the files, such as creating a .tar archive or taking an AWS EBS volume snapshot. info If backing up is not possible, you can safely skip this step. Step 1: Download the Enterprise binaries ​ You should have received an email detailing the download steps and credentials to obtain the Enterprise binaries. Each Enterprise version is buil"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Introduction", "text": "this step. Step 1: Download the Enterprise binaries ​ You should have received an email detailing the download steps and credentials to obtain the Enterprise binaries. Each Enterprise version is built for a specific operating system and architecture. You should always opt for the latest version of the Enterprise edition. tip You may consult the release notes to consult the latest changes and features. Step 2: Install and restart the database ​ Unpack the binaries in a directory of your choice. Replace your existing QuestDB binaries with the new ones. Stop the database. Start the database with the new binaries in place . info The first time you start the database with the new binaries, the migration process will kick in and ready the database state for the new features. Step 3: Set up TLS connection encryption ​ Prepare your server TLS certificates in PEM format and continue with the TLS Encryption guide. info If you don't have a TLS certificate yet, you can ask QuestDB to generate a self-signed demo certificate . Step 4: Set up user accounts and permissions ​ First override the default admin credentials in server.conf . It is recommended to select a non-trivial username and passwo"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Introduction", "text": "elf-signed demo certificate . Step 4: Set up user accounts and permissions ​ First override the default admin credentials in server.conf . It is recommended to select a non-trivial username and password. acl.admin.user=myadmin acl.admin.password=mypwd The above settings will replace the default built-in admin account ( admin / quest ) to make the database safer. However, the password is still stored in the configuration file as plain text. We recommend to create your own admin account(s), and completely disable the built-in admin. To create your own database administrators, start the database up, and login via the Web Console with the admin credentials specified previously in server.conf . Then create one or more admin accounts which will be used for database and user management. For example, the simplest way to create a full admin: CREATE USER administrator WITH PASSWORD adminpwd; GRANT ALL TO administrator WITH GRANT OPTION; The above administrator user replaces the built-in admin, which can be disabled now in the configuration file: acl.admin.user.enabled=false The built-in admin settings can stay in server.conf , and can be re-enabled in emergency situations, such as all databa"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Introduction", "text": "hich can be disabled now in the configuration file: acl.admin.user.enabled=false The built-in admin settings can stay in server.conf , and can be re-enabled in emergency situations, such as all database administrators have forgotten their passwords. Now we can go ahead and setup groups, user and service accounts with the help of the new database administrator(s). More details on this topic can be found in the RBAC documentation . For setting up Single Sign-On (SSO), please, refer to the OIDC Integration guide, which explains how QuestDB integrates with OAuth2/OIDC providers in general. Although we cannot cover all OAuth2 providers, we have documented PingFederate and Microsoft EntraID example setups. Other providers should be configured similarly. warning It is important to disable the built-in admin user for security purposes. Step 5: Setting up replication ​ If you wish to use the replication features, continue with setting up the object store and server.conf changes as detailed in the Database Replication guide. Unsupported Migration Workflows ​ When a database is migrated from the Open Source to the Enterprise edition, the database upgrades the state in each of the database tab"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Introduction", "text": "he Database Replication guide. Unsupported Migration Workflows ​ When a database is migrated from the Open Source to the Enterprise edition, the database upgrades the state in each of the database tables to enable the new features of the Enterprise edition, such as database replication. Because of this, we only support an in-place edition migration. warning We do not support directly copying table data directories and files from an Open Source installation to an Enterprise installation, as the required state for Enterprise features would not be initialized and the database would not operate correctly. If you have a production instance of QuestDB Open Source and have already been testing the new features of the Enterprise edition on a second instance, ensure that the two instances don't share any filesystem directories. If you had activated the replication features on this second instance and want to reuse the same object store location, you must first clear it to transfer object store ownership to the new migrated database instance. If you have a more complex migration scenario, please contact us and we'll be happy to help with your specific setup. Edit this page Previous Digital O"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Introduction", "text": "ownership to the new migrated database instance. If you have a more complex migration scenario, please contact us and we'll be happy to help with your specific setup. Edit this page Previous Digital Ocean Next Capacity planning Overview Migration Workflow Step 0: Backup your data Step 1: Download the Enterprise binaries Step 2: Install and restart the database Step 3: Set up TLS connection encryption Step 4: Set up user accounts and permissions Step 5: Setting up replication Unsupported Migration Workflows\n\nOperations Migrate to Enterprise edition On this page Migrate to Enterprise edition This page covers the steps to migrate a database instance from QuestDB open source edition to QuestDB Enterprise edition. Overview ​ The QuestDB Enterprise edition is based on the Open Source edition of the database. Migrating from QuestDB Open Source to QuestDB Enterprise is a straightforward process. It is nonetheless important to follow the steps carefully to ensure a successful migration. Migration Workflow ​ Step 0: Backup your data ​ Before you start the migration process, consider backing up your data. You can follow the instructions in the Backup and restore guide create a restore point."}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Introduction", "text": "on Workflow ​ Step 0: Backup your data ​ Before you start the migration process, consider backing up your data. You can follow the instructions in the Backup and restore guide create a restore point. It is unlikely that you will need the restore point, but it is a good idea to have it in case something goes wrong. In short: Checkpoint the database: CHECKPOINT CREATE Back up the files, such as creating a .tar archive or taking an AWS EBS volume snapshot. info If backing up is not possible, you can safely skip this step. Step 1: Download the Enterprise binaries ​ You should have received an email detailing the download steps and credentials to obtain the Enterprise binaries. Each Enterprise version is built for a specific operating system and architecture. You should always opt for the latest version of the Enterprise edition. tip You may consult the release notes to consult the latest changes and features. Step 2: Install and restart the database ​ Unpack the binaries in a directory of your choice. Replace your existing QuestDB binaries with the new ones. Stop the database. Start the database with the new binaries in place . info The first time you start the database with the new bi"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Introduction", "text": "your choice. Replace your existing QuestDB binaries with the new ones. Stop the database. Start the database with the new binaries in place . info The first time you start the database with the new binaries, the migration process will kick in and ready the database state for the new features. Step 3: Set up TLS connection encryption ​ Prepare your server TLS certificates in PEM format and continue with the TLS Encryption guide. info If you don't have a TLS certificate yet, you can ask QuestDB to generate a self-signed demo certificate . Step 4: Set up user accounts and permissions ​ First override the default admin credentials in server.conf . It is recommended to select a non-trivial username and password. acl.admin.user=myadmin acl.admin.password=mypwd The above settings will replace the default built-in admin account ( admin / quest ) to make the database safer. However, the password is still stored in the configuration file as plain text. We recommend to create your own admin account(s), and completely disable the built-in admin. To create your own database administrators, start the database up, and login via the Web Console with the admin credentials specified previously in se"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Introduction", "text": "nt(s), and completely disable the built-in admin. To create your own database administrators, start the database up, and login via the Web Console with the admin credentials specified previously in server.conf . Then create one or more admin accounts which will be used for database and user management. For example, the simplest way to create a full admin: CREATE USER administrator WITH PASSWORD adminpwd; GRANT ALL TO administrator WITH GRANT OPTION; The above administrator user replaces the built-in admin, which can be disabled now in the configuration file: acl.admin.user.enabled=false The built-in admin settings can stay in server.conf , and can be re-enabled in emergency situations, such as all database administrators have forgotten their passwords. Now we can go ahead and setup groups, user and service accounts with the help of the new database administrator(s). More details on this topic can be found in the RBAC documentation . For setting up Single Sign-On (SSO), please, refer to the OIDC Integration guide, which explains how QuestDB integrates with OAuth2/OIDC providers in general. Although we cannot cover all OAuth2 providers, we have documented PingFederate and Microsoft E"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Introduction", "text": "the OIDC Integration guide, which explains how QuestDB integrates with OAuth2/OIDC providers in general. Although we cannot cover all OAuth2 providers, we have documented PingFederate and Microsoft EntraID example setups. Other providers should be configured similarly. warning It is important to disable the built-in admin user for security purposes. Step 5: Setting up replication ​ If you wish to use the replication features, continue with setting up the object store and server.conf changes as detailed in the Database Replication guide. Unsupported Migration Workflows ​ When a database is migrated from the Open Source to the Enterprise edition, the database upgrades the state in each of the database tables to enable the new features of the Enterprise edition, such as database replication. Because of this, we only support an in-place edition migration. warning We do not support directly copying table data directories and files from an Open Source installation to an Enterprise installation, as the required state for Enterprise features would not be initialized and the database would not operate correctly. If you have a production instance of QuestDB Open Source and have already been"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Introduction", "text": "ion, as the required state for Enterprise features would not be initialized and the database would not operate correctly. If you have a production instance of QuestDB Open Source and have already been testing the new features of the Enterprise edition on a second instance, ensure that the two instances don't share any filesystem directories. If you had activated the replication features on this second instance and want to reuse the same object store location, you must first clear it to transfer object store ownership to the new migrated database instance. If you have a more complex migration scenario, please contact us and we'll be happy to help with your specific setup. Edit this page Previous Digital Ocean Next Capacity planning Overview Migration Workflow Step 0: Backup your data Step 1: Download the Enterprise binaries Step 2: Install and restart the database Step 3: Set up TLS connection encryption Step 4: Set up user accounts and permissions Step 5: Setting up replication Unsupported Migration Workflows\n\nOperations Migrate to Enterprise edition On this page Migrate to Enterprise edition This page covers the steps to migrate a database instance from QuestDB open source edition"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Introduction", "text": "rted Migration Workflows\n\nOperations Migrate to Enterprise edition On this page Migrate to Enterprise edition This page covers the steps to migrate a database instance from QuestDB open source edition to QuestDB Enterprise edition. Overview ​ The QuestDB Enterprise edition is based on the Open Source edition of the database. Migrating from QuestDB Open Source to QuestDB Enterprise is a straightforward process. It is nonetheless important to follow the steps carefully to ensure a successful migration. Migration Workflow ​ Step 0: Backup your data ​ Before you start the migration process, consider backing up your data. You can follow the instructions in the Backup and restore guide create a restore point. It is unlikely that you will need the restore point, but it is a good idea to have it in case something goes wrong. In short: Checkpoint the database: CHECKPOINT CREATE Back up the files, such as creating a .tar archive or taking an AWS EBS volume snapshot. info If backing up is not possible, you can safely skip this step. Step 1: Download the Enterprise binaries ​ You should have received an email detailing the download steps and credentials to obtain the Enterprise binaries. Each"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Introduction", "text": "sible, you can safely skip this step. Step 1: Download the Enterprise binaries ​ You should have received an email detailing the download steps and credentials to obtain the Enterprise binaries. Each Enterprise version is built for a specific operating system and architecture. You should always opt for the latest version of the Enterprise edition. tip You may consult the release notes to consult the latest changes and features. Step 2: Install and restart the database ​ Unpack the binaries in a directory of your choice. Replace your existing QuestDB binaries with the new ones. Stop the database. Start the database with the new binaries in place . info The first time you start the database with the new binaries, the migration process will kick in and ready the database state for the new features. Step 3: Set up TLS connection encryption ​ Prepare your server TLS certificates in PEM format and continue with the TLS Encryption guide. info If you don't have a TLS certificate yet, you can ask QuestDB to generate a self-signed demo certificate . Step 4: Set up user accounts and permissions ​ First override the default admin credentials in server.conf . It is recommended to select a non-t"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Introduction", "text": "sk QuestDB to generate a self-signed demo certificate . Step 4: Set up user accounts and permissions ​ First override the default admin credentials in server.conf . It is recommended to select a non-trivial username and password. acl.admin.user=myadmin acl.admin.password=mypwd The above settings will replace the default built-in admin account ( admin / quest ) to make the database safer. However, the password is still stored in the configuration file as plain text. We recommend to create your own admin account(s), and completely disable the built-in admin. To create your own database administrators, start the database up, and login via the Web Console with the admin credentials specified previously in server.conf . Then create one or more admin accounts which will be used for database and user management. For example, the simplest way to create a full admin: CREATE USER administrator WITH PASSWORD adminpwd; GRANT ALL TO administrator WITH GRANT OPTION; The above administrator user replaces the built-in admin, which can be disabled now in the configuration file: acl.admin.user.enabled=false The built-in admin settings can stay in server.conf , and can be re-enabled in emergency situ"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Introduction", "text": "aces the built-in admin, which can be disabled now in the configuration file: acl.admin.user.enabled=false The built-in admin settings can stay in server.conf , and can be re-enabled in emergency situations, such as all database administrators have forgotten their passwords. Now we can go ahead and setup groups, user and service accounts with the help of the new database administrator(s). More details on this topic can be found in the RBAC documentation . For setting up Single Sign-On (SSO), please, refer to the OIDC Integration guide, which explains how QuestDB integrates with OAuth2/OIDC providers in general. Although we cannot cover all OAuth2 providers, we have documented PingFederate and Microsoft EntraID example setups. Other providers should be configured similarly. warning It is important to disable the built-in admin user for security purposes. Step 5: Setting up replication ​ If you wish to use the replication features, continue with setting up the object store and server.conf changes as detailed in the Database Replication guide. Unsupported Migration Workflows ​ When a database is migrated from the Open Source to the Enterprise edition, the database upgrades the state i"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Introduction", "text": "f changes as detailed in the Database Replication guide. Unsupported Migration Workflows ​ When a database is migrated from the Open Source to the Enterprise edition, the database upgrades the state in each of the database tables to enable the new features of the Enterprise edition, such as database replication. Because of this, we only support an in-place edition migration. warning We do not support directly copying table data directories and files from an Open Source installation to an Enterprise installation, as the required state for Enterprise features would not be initialized and the database would not operate correctly. If you have a production instance of QuestDB Open Source and have already been testing the new features of the Enterprise edition on a second instance, ensure that the two instances don't share any filesystem directories. If you had activated the replication features on this second instance and want to reuse the same object store location, you must first clear it to transfer object store ownership to the new migrated database instance. If you have a more complex migration scenario, please contact us and we'll be happy to help with your specific setup. Edit th"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Introduction", "text": "to transfer object store ownership to the new migrated database instance. If you have a more complex migration scenario, please contact us and we'll be happy to help with your specific setup. Edit this page Previous Digital Ocean Next Capacity planning\n\nOperations Migrate to Enterprise edition On this page Migrate to Enterprise edition This page covers the steps to migrate a database instance from QuestDB open source edition to QuestDB Enterprise edition. Overview ​ The QuestDB Enterprise edition is based on the Open Source edition of the database. Migrating from QuestDB Open Source to QuestDB Enterprise is a straightforward process. It is nonetheless important to follow the steps carefully to ensure a successful migration. Migration Workflow ​ Step 0: Backup your data ​ Before you start the migration process, consider backing up your data. You can follow the instructions in the Backup and restore guide create a restore point. It is unlikely that you will need the restore point, but it is a good idea to have it in case something goes wrong. In short: Checkpoint the database: CHECKPOINT CREATE Back up the files, such as creating a .tar archive or taking an AWS EBS volume snapshot."}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Introduction", "text": "t is a good idea to have it in case something goes wrong. In short: Checkpoint the database: CHECKPOINT CREATE Back up the files, such as creating a .tar archive or taking an AWS EBS volume snapshot. info If backing up is not possible, you can safely skip this step. Step 1: Download the Enterprise binaries ​ You should have received an email detailing the download steps and credentials to obtain the Enterprise binaries. Each Enterprise version is built for a specific operating system and architecture. You should always opt for the latest version of the Enterprise edition. tip You may consult the release notes to consult the latest changes and features. Step 2: Install and restart the database ​ Unpack the binaries in a directory of your choice. Replace your existing QuestDB binaries with the new ones. Stop the database. Start the database with the new binaries in place . info The first time you start the database with the new binaries, the migration process will kick in and ready the database state for the new features. Step 3: Set up TLS connection encryption ​ Prepare your server TLS certificates in PEM format and continue with the TLS Encryption guide. info If you don't have a T"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Introduction", "text": "atabase state for the new features. Step 3: Set up TLS connection encryption ​ Prepare your server TLS certificates in PEM format and continue with the TLS Encryption guide. info If you don't have a TLS certificate yet, you can ask QuestDB to generate a self-signed demo certificate . Step 4: Set up user accounts and permissions ​ First override the default admin credentials in server.conf . It is recommended to select a non-trivial username and password. acl.admin.user=myadmin acl.admin.password=mypwd The above settings will replace the default built-in admin account ( admin / quest ) to make the database safer. However, the password is still stored in the configuration file as plain text. We recommend to create your own admin account(s), and completely disable the built-in admin. To create your own database administrators, start the database up, and login via the Web Console with the admin credentials specified previously in server.conf . Then create one or more admin accounts which will be used for database and user management. For example, the simplest way to create a full admin: CREATE USER administrator WITH PASSWORD adminpwd; GRANT ALL TO administrator WITH GRANT OPTION; The"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Introduction", "text": "ill be used for database and user management. For example, the simplest way to create a full admin: CREATE USER administrator WITH PASSWORD adminpwd; GRANT ALL TO administrator WITH GRANT OPTION; The above administrator user replaces the built-in admin, which can be disabled now in the configuration file: acl.admin.user.enabled=false The built-in admin settings can stay in server.conf , and can be re-enabled in emergency situations, such as all database administrators have forgotten their passwords. Now we can go ahead and setup groups, user and service accounts with the help of the new database administrator(s). More details on this topic can be found in the RBAC documentation . For setting up Single Sign-On (SSO), please, refer to the OIDC Integration guide, which explains how QuestDB integrates with OAuth2/OIDC providers in general. Although we cannot cover all OAuth2 providers, we have documented PingFederate and Microsoft EntraID example setups. Other providers should be configured similarly. warning It is important to disable the built-in admin user for security purposes. Step 5: Setting up replication ​ If you wish to use the replication features, continue with setting up th"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Introduction", "text": "d similarly. warning It is important to disable the built-in admin user for security purposes. Step 5: Setting up replication ​ If you wish to use the replication features, continue with setting up the object store and server.conf changes as detailed in the Database Replication guide. Unsupported Migration Workflows ​ When a database is migrated from the Open Source to the Enterprise edition, the database upgrades the state in each of the database tables to enable the new features of the Enterprise edition, such as database replication. Because of this, we only support an in-place edition migration. warning We do not support directly copying table data directories and files from an Open Source installation to an Enterprise installation, as the required state for Enterprise features would not be initialized and the database would not operate correctly. If you have a production instance of QuestDB Open Source and have already been testing the new features of the Enterprise edition on a second instance, ensure that the two instances don't share any filesystem directories. If you had activated the replication features on this second instance and want to reuse the same object store loca"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Introduction", "text": "econd instance, ensure that the two instances don't share any filesystem directories. If you had activated the replication features on this second instance and want to reuse the same object store location, you must first clear it to transfer object store ownership to the new migrated database instance. If you have a more complex migration scenario, please contact us and we'll be happy to help with your specific setup. Edit this page Previous Digital Ocean Next Capacity planning\n\nOperations\n\nMigrate to Enterprise edition\n\nOn this page\n\nMigrate to Enterprise edition This page covers the steps to migrate a database instance from QuestDB open source edition to QuestDB Enterprise edition. Overview ​ The QuestDB Enterprise edition is based on the Open Source edition of the database. Migrating from QuestDB Open Source to QuestDB Enterprise is a straightforward process. It is nonetheless important to follow the steps carefully to ensure a successful migration. Migration Workflow ​ Step 0: Backup your data ​ Before you start the migration process, consider backing up your data. You can follow the instructions in the Backup and restore guide create a restore point. It is unlikely that you wi"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Introduction", "text": "up your data ​ Before you start the migration process, consider backing up your data. You can follow the instructions in the Backup and restore guide create a restore point. It is unlikely that you will need the restore point, but it is a good idea to have it in case something goes wrong. In short: Checkpoint the database: CHECKPOINT CREATE Back up the files, such as creating a .tar archive or taking an AWS EBS volume snapshot. info If backing up is not possible, you can safely skip this step. Step 1: Download the Enterprise binaries ​ You should have received an email detailing the download steps and credentials to obtain the Enterprise binaries. Each Enterprise version is built for a specific operating system and architecture. You should always opt for the latest version of the Enterprise edition. tip You may consult the release notes to consult the latest changes and features. Step 2: Install and restart the database ​ Unpack the binaries in a directory of your choice. Replace your existing QuestDB binaries with the new ones. Stop the database. Start the database with the new binaries in place . info The first time you start the database with the new binaries, the migration proc"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Introduction", "text": "existing QuestDB binaries with the new ones. Stop the database. Start the database with the new binaries in place . info The first time you start the database with the new binaries, the migration process will kick in and ready the database state for the new features. Step 3: Set up TLS connection encryption ​ Prepare your server TLS certificates in PEM format and continue with the TLS Encryption guide. info If you don't have a TLS certificate yet, you can ask QuestDB to generate a self-signed demo certificate . Step 4: Set up user accounts and permissions ​ First override the default admin credentials in server.conf . It is recommended to select a non-trivial username and password. acl.admin.user=myadmin acl.admin.password=mypwd The above settings will replace the default built-in admin account ( admin / quest ) to make the database safer. However, the password is still stored in the configuration file as plain text. We recommend to create your own admin account(s), and completely disable the built-in admin. To create your own database administrators, start the database up, and login via the Web Console with the admin credentials specified previously in server.conf . Then create on"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Introduction", "text": "ble the built-in admin. To create your own database administrators, start the database up, and login via the Web Console with the admin credentials specified previously in server.conf . Then create one or more admin accounts which will be used for database and user management. For example, the simplest way to create a full admin: CREATE USER administrator WITH PASSWORD adminpwd; GRANT ALL TO administrator WITH GRANT OPTION; The above administrator user replaces the built-in admin, which can be disabled now in the configuration file: acl.admin.user.enabled=false The built-in admin settings can stay in server.conf , and can be re-enabled in emergency situations, such as all database administrators have forgotten their passwords. Now we can go ahead and setup groups, user and service accounts with the help of the new database administrator(s). More details on this topic can be found in the RBAC documentation . For setting up Single Sign-On (SSO), please, refer to the OIDC Integration guide, which explains how QuestDB integrates with OAuth2/OIDC providers in general. Although we cannot cover all OAuth2 providers, we have documented PingFederate and Microsoft EntraID example setups. Oth"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Introduction", "text": "e, which explains how QuestDB integrates with OAuth2/OIDC providers in general. Although we cannot cover all OAuth2 providers, we have documented PingFederate and Microsoft EntraID example setups. Other providers should be configured similarly. warning It is important to disable the built-in admin user for security purposes. Step 5: Setting up replication ​ If you wish to use the replication features, continue with setting up the object store and server.conf changes as detailed in the Database Replication guide. Unsupported Migration Workflows ​ When a database is migrated from the Open Source to the Enterprise edition, the database upgrades the state in each of the database tables to enable the new features of the Enterprise edition, such as database replication. Because of this, we only support an in-place edition migration. warning We do not support directly copying table data directories and files from an Open Source installation to an Enterprise installation, as the required state for Enterprise features would not be initialized and the database would not operate correctly. If you have a production instance of QuestDB Open Source and have already been testing the new features"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Introduction", "text": "for Enterprise features would not be initialized and the database would not operate correctly. If you have a production instance of QuestDB Open Source and have already been testing the new features of the Enterprise edition on a second instance, ensure that the two instances don't share any filesystem directories. If you had activated the replication features on this second instance and want to reuse the same object store location, you must first clear it to transfer object store ownership to the new migrated database instance. If you have a more complex migration scenario, please contact us and we'll be happy to help with your specific setup."}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Migrate to Enterprise edition", "text": "This page covers the steps to migrate a database instance from QuestDB open source edition to QuestDB Enterprise edition."}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Overview ​", "text": "The QuestDB Enterprise edition is based on the Open Source edition of the database.\n\nMigrating from QuestDB Open Source to QuestDB Enterprise is a straightforward process. It is nonetheless important to follow the steps carefully to ensure a successful migration."}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Step 0: Backup your data ​", "text": "Before you start the migration process, consider backing up your data.\n\nYou can follow the instructions in the Backup and restore guide create a restore point. It is unlikely that you will need the restore point, but it is a good idea to have it in case something goes wrong.\n\nIn short:\n\nCheckpoint the database: CHECKPOINT CREATE\n\nCheckpoint the database:\n\nCHECKPOINT CREATE\n\nCHECKPOINT CREATE\n\nCHECKPOINT CREATE\n\nCHECKPOINT CREATE\n\nBack up the files, such as creating a .tar archive or taking an AWS EBS volume snapshot.\n\nBack up the files, such as creating a .tar archive or taking an AWS EBS volume snapshot.\n\n.tar\n\ninfo If backing up is not possible, you can safely skip this step.\n\ninfo\n\nIf backing up is not possible, you can safely skip this step.\n\nIf backing up is not possible, you can safely skip this step."}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Step 1: Download the Enterprise binaries ​", "text": "You should have received an email detailing the download steps and credentials to obtain the Enterprise binaries.\n\nEach Enterprise version is built for a specific operating system and architecture. You should always opt for the latest version of the Enterprise edition.\n\ntip You may consult the release notes to consult the latest changes and features.\n\ntip\n\nYou may consult the release notes to consult the latest changes and features.\n\nYou may consult the release notes to consult the latest changes and features."}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Step 2: Install and restart the database ​", "text": "Unpack the binaries in a directory of your choice.\n\nReplace your existing QuestDB binaries with the new ones.\n\nStop the database.\n\nStart the database with the new binaries in place .\n\ninfo The first time you start the database with the new binaries, the migration process will kick in and ready the database state for the new features.\n\ninfo\n\nThe first time you start the database with the new binaries, the migration process will kick in and ready the database state for the new features.\n\nThe first time you start the database with the new binaries, the migration process will kick in and ready the database state for the new features."}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Step 3: Set up TLS connection encryption ​", "text": "Prepare your server TLS certificates in PEM format and continue with the TLS Encryption guide.\n\ninfo If you don't have a TLS certificate yet, you can ask QuestDB to generate a self-signed demo certificate .\n\ninfo\n\nIf you don't have a TLS certificate yet, you can ask QuestDB to generate a self-signed demo certificate .\n\nIf you don't have a TLS certificate yet, you can ask QuestDB to generate a self-signed demo certificate ."}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Step 4: Set up user accounts and permissions ​", "text": "First override the default admin credentials in server.conf . It is recommended to select a non-trivial username and password.\n\nserver.conf\n\nacl.admin.user=myadmin acl.admin.password=mypwd\n\nacl.admin.user=myadmin acl.admin.password=mypwd\n\nacl.admin.user=myadmin acl.admin.password=mypwd\n\nacl.admin.user=myadmin acl.admin.password=mypwd\n\nThe above settings will replace the default built-in admin account ( admin / quest ) to make the database safer.\n\nadmin\n\nquest\n\nHowever, the password is still stored in the configuration file as plain text. We recommend to create your own admin account(s), and completely disable the built-in admin.\n\nTo create your own database administrators, start the database up, and login via the Web Console with the admin credentials specified previously in server.conf . Then create one or more admin accounts which will be used for database and user management.\n\nserver.conf\n\nFor example, the simplest way to create a full admin:\n\nCREATE USER administrator WITH PASSWORD adminpwd; GRANT ALL TO administrator WITH GRANT OPTION;\n\nCREATE USER administrator WITH PASSWORD adminpwd; GRANT ALL TO administrator WITH GRANT OPTION;\n\nCREATE USER administrator WITH PASSWORD admin"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Step 4: Set up user accounts and permissions ​", "text": "adminpwd; GRANT ALL TO administrator WITH GRANT OPTION;\n\nCREATE USER administrator WITH PASSWORD adminpwd; GRANT ALL TO administrator WITH GRANT OPTION;\n\nCREATE USER administrator WITH PASSWORD adminpwd; GRANT ALL TO administrator WITH GRANT OPTION;\n\nCREATE USER administrator WITH PASSWORD adminpwd; GRANT ALL TO administrator WITH GRANT OPTION;\n\nThe above administrator user replaces the built-in admin, which can be disabled now in the configuration file:\n\nadministrator\n\nacl.admin.user.enabled=false\n\nacl.admin.user.enabled=false\n\nacl.admin.user.enabled=false\n\nacl.admin.user.enabled=false\n\nThe built-in admin settings can stay in server.conf , and can be re-enabled in emergency situations, such as all database administrators have forgotten their passwords.\n\nserver.conf\n\nNow we can go ahead and setup groups, user and service accounts with the help of the new database administrator(s). More details on this topic can be found in the RBAC documentation .\n\nFor setting up Single Sign-On (SSO), please, refer to the OIDC Integration guide, which explains how QuestDB integrates with OAuth2/OIDC providers in general. Although we cannot cover all OAuth2 providers, we have documented PingFederat"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Step 4: Set up user accounts and permissions ​", "text": "please, refer to the OIDC Integration guide, which explains how QuestDB integrates with OAuth2/OIDC providers in general. Although we cannot cover all OAuth2 providers, we have documented PingFederate and Microsoft EntraID example setups.\n\nOther providers should be configured similarly.\n\nwarning It is important to disable the built-in admin user for security purposes.\n\nwarning\n\nIt is important to disable the built-in admin user for security purposes.\n\nIt is important to disable the built-in admin user for security purposes."}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Step 5: Setting up replication ​", "text": "If you wish to use the replication features, continue with setting up the object store and server.conf changes as detailed in the Database Replication guide.\n\nserver.conf"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Unsupported Migration Workflows ​", "text": "When a database is migrated from the Open Source to the Enterprise edition, the database upgrades the state in each of the database tables to enable the new features of the Enterprise edition, such as database replication.\n\nBecause of this, we only support an in-place edition migration.\n\nwarning We do not support directly copying table data directories and files from an Open Source installation to an Enterprise installation, as the required state for Enterprise features would not be initialized and the database would not operate correctly. If you have a production instance of QuestDB Open Source and have already been testing the new features of the Enterprise edition on a second instance, ensure that the two instances don't share any filesystem directories. If you had activated the replication features on this second instance and want to reuse the same object store location, you must first clear it to transfer object store ownership to the new migrated database instance. If you have a more complex migration scenario, please contact us and we'll be happy to help with your specific setup.\n\nwarning\n\nWe do not support directly copying table data directories and files from an Open Sourc"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Unsupported Migration Workflows ​", "text": "more complex migration scenario, please contact us and we'll be happy to help with your specific setup.\n\nwarning\n\nWe do not support directly copying table data directories and files from an Open Source installation to an Enterprise installation, as the required state for Enterprise features would not be initialized and the database would not operate correctly. If you have a production instance of QuestDB Open Source and have already been testing the new features of the Enterprise edition on a second instance, ensure that the two instances don't share any filesystem directories. If you had activated the replication features on this second instance and want to reuse the same object store location, you must first clear it to transfer object store ownership to the new migrated database instance. If you have a more complex migration scenario, please contact us and we'll be happy to help with your specific setup.\n\nWe do not support directly copying table data directories and files from an Open Source installation to an Enterprise installation, as the required state for Enterprise features would not be initialized and the database would not operate correctly.\n\nIf you have a production ins"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Unsupported Migration Workflows ​", "text": "en Source installation to an Enterprise installation, as the required state for Enterprise features would not be initialized and the database would not operate correctly.\n\nIf you have a production instance of QuestDB Open Source and have already been testing the new features of the Enterprise edition on a second instance, ensure that the two instances don't share any filesystem directories. If you had activated the replication features on this second instance and want to reuse the same object store location, you must first clear it to transfer object store ownership to the new migrated database instance.\n\nIf you have a more complex migration scenario, please contact us and we'll be happy to help with your specific setup.\n\nEdit this page\n\nEdit this page\n\nPrevious\n\nDigital Ocean\n\nNext\n\nCapacity planning\n\nOverview Migration Workflow Step 0: Backup your data Step 1: Download the Enterprise binaries Step 2: Install and restart the database Step 3: Set up TLS connection encryption Step 4: Set up user accounts and permissions Step 5: Setting up replication Unsupported Migration Workflows\n\nOverview Migration Workflow Step 0: Backup your data Step 1: Download the Enterprise binaries Step 2:"}
{"source_url": "https://questdb.com/docs/operations/migrate-to-enterprise", "title": "Migrate to Enterprise edition", "section": "Unsupported Migration Workflows ​", "text": "t up user accounts and permissions Step 5: Setting up replication Unsupported Migration Workflows\n\nOverview Migration Workflow Step 0: Backup your data Step 1: Download the Enterprise binaries Step 2: Install and restart the database Step 3: Set up TLS connection encryption Step 4: Set up user accounts and permissions Step 5: Setting up replication Unsupported Migration Workflows\n\nOverview\n\nMigration Workflow Step 0: Backup your data Step 1: Download the Enterprise binaries Step 2: Install and restart the database Step 3: Set up TLS connection encryption Step 4: Set up user accounts and permissions Step 5: Setting up replication\n\nStep 0: Backup your data\n\nStep 1: Download the Enterprise binaries\n\nStep 2: Install and restart the database\n\nStep 3: Set up TLS connection encryption\n\nStep 4: Set up user accounts and permissions\n\nStep 5: Setting up replication\n\nUnsupported Migration Workflows"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "Guides & Tutorials Create a sample database On this page Create a sample database This guide walks you through creating a sample dataset. It utilizes rnd_ functions and basic SQL grammar to generate 'mock' data of specific types. For most applications, you will import your data using methods like the InfluxDB Line Protocol, CSV imports, or integration with third-party tools such as Telegraf, Kafka , or Prometheus. If your interest lies in data ingestion rather than generation, refer to our ingestion overview . Alternatively, the QuestDB demo instance offers a practical way to explore data creation and manipulation without setting up your dataset. All that said, in this tutorial you will learn how to: Create tables Populate tables with sample data Run simple and advanced queries Delete tables Before we begin... ​ All commands are run through the Web Console accessible at http://localhost:9000 . You can also run the same SQL via the Postgres endpoint or the REST API . If QuestDB is not running locally, checkout the quick start . Creating a table ​ With QuestDB running, the first step is to create a table. We'll start with one representing financial market data. Then in the insert sec"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "unning locally, checkout the quick start . Creating a table ​ With QuestDB running, the first step is to create a table. We'll start with one representing financial market data. Then in the insert section, we'll create another pair of tables representing temperature sensors and their readings. Let's start by creating the trades table: CREATE TABLE trades ( timestamp TIMESTAMP, symbol SYMBOL, side SYMBOL, price DOUBLE, amount DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY WAL DEDUP UPSERT KEYS(timestamp, symbol); This is a basic yet robust table. It applies SYMBOL s for ticker and side, a price, and a designated timestamp . It's partitioned by day , is a WAL table , and deduplicates the timestamp and ticker columns. As the links above show, there's lots to unpack in this table! Feel free to learn more about the nuances. We've done all of this to match the nature of how we'll query this data. We're focused on a the flow of the market, the pulse of the market's day-to-day, hence we've partitioned it as such. We're also leery of duplicates, for accuracy of data, so we'll ensure that if timestamps are identical that we do not create a duplicate. Timestamps are essential for time-series"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "titioned it as such. We're also leery of duplicates, for accuracy of data, so we'll ensure that if timestamps are identical that we do not create a duplicate. Timestamps are essential for time-series analysis. We'll proceed forward to INSERT. Inserting data ​ Financial market data ​ Let's populate our trades table with procedurally-generated data: Insert as SELECT INSERT INTO trades SELECT timestamp_sequence('2024-01-01T00:00:00', 60000L * x) timestamp, -- Generate a timestamp every minute starting from Jan 1, 2024 rnd_str('ETH-USD', 'BTC-USD', 'SOL-USD', 'LTC-USD', 'UNI-USD') symbol, -- Random ticker symbols rnd_str('buy', 'sell') side, -- Random side (BUY or SELL) rnd_double() * 1000 + 100 price, -- Random price between 100.0 and 1100.0, rnd_double() * 2000 + 0.1 amount -- Random price between 0.1 and 2000.1 FROM long_sequence(10000) x; Our trades table now contains 10,000 randomly-generated trades. The comments indicate how we've structured our random data. We picked a few companies, BUY vs. SELL, and created a timestamp every minute. We've dictated the overall number of rows generated via long_sequence(10000) . We can bump that up, if we want. We've also conservatively generate"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": ", BUY vs. SELL, and created a timestamp every minute. We've dictated the overall number of rows generated via long_sequence(10000) . We can bump that up, if we want. We've also conservatively generated a timestamp per minute, even though in reality trades against these companies are likely much more frequent. This helps keep our basic examples basic. Now let's look at the table and its data: 'trades'; It will look similar to this, albeit with alternative randomized values. timestamp symbol side price amount 2024-01-01T00:00:00.000000Z BTC-USD sell 483.904143675277 139.449481016294 2024-01-01T00:00:00.060000Z ETH-USD sell 920.296245196274 920.296245196274 2024-01-01T00:00:00.180000Z UNI-USD sell 643.277468441839 643.277468441839 2024-01-01T00:00:00.360000Z LTC-USD buy 218.0920768859 729.81119178972 2024-01-01T00:00:00.600000Z BTC-USD sell 157.596416931116 691.081778396176 That's some fake market data. What about, say, sensor data? Sensors and readings ​ This next example will create and populate two more tables. One table will contain the metadata of our sensors, and the other will contain the actual readings (payload data) from these sensors. In both cases, we will create the table"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "populate two more tables. One table will contain the metadata of our sensors, and the other will contain the actual readings (payload data) from these sensors. In both cases, we will create the table and generate the data at the same time. This combines the CREATE & SELECT operations to perform a create-and-insert: Create table as, readings CREATE TABLE readings AS( SELECT x ID, timestamp_sequence(to_timestamp('2019-10-17T00:00:00', 'yyyy-MM-ddTHH:mm:ss'), rnd_long(1,10,0) * 100000L) ts, rnd_double(0)*8 + 15 temp, rnd_long(0, 10000, 0) sensorId FROM long_sequence(10000000) x) TIMESTAMP(ts) PARTITION BY MONTH DEDUP UPSERT KEYS(ts); For our table, we've again hit the following key notes: TIMESTAMP(ts) elects the ts column as a designated timestamp for partitioning over time. PARTITION BY MONTH creates a monthly partition, where the stored data is effectively sharded by month. DEDUP UPSERT KEYS(ts) deduplicates the timestamp column The generated data will look like the following: ID ts temp sensorId 1 2019-10-17T00:00:00.000000Z 19.37373911 9160 2 2019-10-17T00:00:00.600000Z 21.91184617 9671 3 2019-10-17T00:00:01.400000Z 16.58367834 8731 4 2019-10-17T00:00:01.500000Z 16.69308815 3447"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "emp sensorId 1 2019-10-17T00:00:00.000000Z 19.37373911 9160 2 2019-10-17T00:00:00.600000Z 21.91184617 9671 3 2019-10-17T00:00:01.400000Z 16.58367834 8731 4 2019-10-17T00:00:01.500000Z 16.69308815 3447 5 2019-10-17T00:00:01.600000Z 19.67991569 7985 ... ... ... ... Nice - and our next table, which includes the sensors themselves and their detail: Create table as, sensors CREATE TABLE sensors AS( SELECT x ID, -- Increasing integer rnd_str('Eberle', 'Honeywell', 'Omron', 'United Automation', 'RS Pro') make, -- Random manufacturer rnd_str('New York', 'Miami', 'Boston', 'Chicago', 'San Francisco') city -- Random city FROM long_sequence(10000) x) Note that we've not included a timestamp in this sensors column. This is one of the rare, demonstrative examples where we're not including it, and thus not taking advantage of the bulk of the benefits received via time-series optimization. As we have a timestamp in the paired readings table, it's helpful to demonstrate them as a pair. With these two new tables, and our prior financial market data table, we've got a lot of useful queries we can test. Running queries ​ Our financial market data table is a great place to test various aggregate funct"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "new tables, and our prior financial market data table, we've got a lot of useful queries we can test. Running queries ​ Our financial market data table is a great place to test various aggregate functions , to compute price over time intervals, and similar anaylsis. However, we'll expand on the readings * sensors tables. First, let's look at readings , running our shorthand for SELECT * FROM readings; : readings; Let's then select the count of records from readings : SELECT count() FROM readings; count 10,000,000 And then the average reading: SELECT avg(temp) FROM readings; average 18.999217780895 We can now use the sensors table alongside the readings table to get more interesting results using a JOIN : SELECT * FROM readings JOIN( SELECT ID sensId, make, city FROM sensors) ON readings.sensorId = sensId; The results should look like the table below: ID ts temp sensorId sensId make city 1 2019-10-17T00:00:00.000000Z 16.472200460982 3211 3211 Omron New York 2 2019-10-17T00:00:00.100000Z 16.598432033599 2319 2319 Honeywell San Francisco 3 2019-10-17T00:00:00.100000Z 20.293681747009 8723 8723 Honeywell New York 4 2019-10-17T00:00:00.100000Z 20.939263119843 885 885 RS Pro San Francisco"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "16.598432033599 2319 2319 Honeywell San Francisco 3 2019-10-17T00:00:00.100000Z 20.293681747009 8723 8723 Honeywell New York 4 2019-10-17T00:00:00.100000Z 20.939263119843 885 885 RS Pro San Francisco 5 2019-10-17T00:00:00.200000Z 19.336660059029 3200 3200 Honeywell San Francisco 6 2019-10-17T00:00:01.100000Z 20.946643576954 4053 4053 Honeywell Miami Note the timestamps returned as we've JOIN'd the tables together. Let's try another type of aggregation: Aggregation keyed by city SELECT city, max(temp) FROM readings JOIN( SELECT ID sensId, city FROM sensors) a ON readings.sensorId = a.sensId; The results should look like the table below: city max New York 22.999998786398 San Francisco 22.999998138348 Miami 22.99999994818 Chicago 22.999991705861 Boston 22.999999233377 Back to time, given we have one table ( readings ) partitioned by time, let's see what we can do when we JOIN the tables together to perform an aggregation based on an hour of time: Aggregation by hourly time buckets SELECT ts, city, make, avg(temp) FROM readings timestamp(ts) JOIN (SELECT ID sensId, city, make FROM sensors WHERE city='Miami' AND make='Omron') a ON readings.sensorId = a.sensId WHERE ts IN '2019-10-21;1d"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "ts, city, make, avg(temp) FROM readings timestamp(ts) JOIN (SELECT ID sensId, city, make FROM sensors WHERE city='Miami' AND make='Omron') a ON readings.sensorId = a.sensId WHERE ts IN '2019-10-21;1d' -- this is an interval between 2019/10/21 and the next day SAMPLE BY 1h -- aggregation by hourly time buckets ALIGN TO CALENDAR; -- align the ts with the start of the hour (hh:00:00) The results should look like the table below: ts city make average 2019-10-21T00:00:00.000000Z Miami Omron 20.004285872098 2019-10-21T00:01:00.000000Z Miami Omron 16.68436714013 2019-10-21T00:02:00.000000Z Miami Omron 15.243684089291 2019-10-21T00:03:00.000000Z Miami Omron 17.193984104315 2019-10-21T00:04:00.000000Z Miami Omron 20.778686822666 ... ... ... ... For more information about these statements, please refer to the SELECT , JOIN and SAMPLE BY pages. Deleting tables ​ We can now clean up the demo data by using DROP TABLE SQL. Be careful using this statement as QuestDB cannot recover data that is deleted in this way: DROP TABLE readings; DROP TABLE sensors; DROP TABLE trades; Edit this page Previous Configuration Next Import CSV Before we begin... Creating a table Inserting data Sensors and reading"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "ed in this way: DROP TABLE readings; DROP TABLE sensors; DROP TABLE trades; Edit this page Previous Configuration Next Import CSV Before we begin... Creating a table Inserting data Sensors and readings Running queries Deleting tables\n\nGuides & Tutorials Create a sample database On this page Create a sample database This guide walks you through creating a sample dataset. It utilizes rnd_ functions and basic SQL grammar to generate 'mock' data of specific types. For most applications, you will import your data using methods like the InfluxDB Line Protocol, CSV imports, or integration with third-party tools such as Telegraf, Kafka , or Prometheus. If your interest lies in data ingestion rather than generation, refer to our ingestion overview . Alternatively, the QuestDB demo instance offers a practical way to explore data creation and manipulation without setting up your dataset. All that said, in this tutorial you will learn how to: Create tables Populate tables with sample data Run simple and advanced queries Delete tables Before we begin... ​ All commands are run through the Web Console accessible at http://localhost:9000 . You can also run the same SQL via the Postgres endpoint or"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "le and advanced queries Delete tables Before we begin... ​ All commands are run through the Web Console accessible at http://localhost:9000 . You can also run the same SQL via the Postgres endpoint or the REST API . If QuestDB is not running locally, checkout the quick start . Creating a table ​ With QuestDB running, the first step is to create a table. We'll start with one representing financial market data. Then in the insert section, we'll create another pair of tables representing temperature sensors and their readings. Let's start by creating the trades table: CREATE TABLE trades ( timestamp TIMESTAMP, symbol SYMBOL, side SYMBOL, price DOUBLE, amount DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY WAL DEDUP UPSERT KEYS(timestamp, symbol); This is a basic yet robust table. It applies SYMBOL s for ticker and side, a price, and a designated timestamp . It's partitioned by day , is a WAL table , and deduplicates the timestamp and ticker columns. As the links above show, there's lots to unpack in this table! Feel free to learn more about the nuances. We've done all of this to match the nature of how we'll query this data. We're focused on a the flow of the market, the pulse of the m"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "unpack in this table! Feel free to learn more about the nuances. We've done all of this to match the nature of how we'll query this data. We're focused on a the flow of the market, the pulse of the market's day-to-day, hence we've partitioned it as such. We're also leery of duplicates, for accuracy of data, so we'll ensure that if timestamps are identical that we do not create a duplicate. Timestamps are essential for time-series analysis. We'll proceed forward to INSERT. Inserting data ​ Financial market data ​ Let's populate our trades table with procedurally-generated data: Insert as SELECT INSERT INTO trades SELECT timestamp_sequence('2024-01-01T00:00:00', 60000L * x) timestamp, -- Generate a timestamp every minute starting from Jan 1, 2024 rnd_str('ETH-USD', 'BTC-USD', 'SOL-USD', 'LTC-USD', 'UNI-USD') symbol, -- Random ticker symbols rnd_str('buy', 'sell') side, -- Random side (BUY or SELL) rnd_double() * 1000 + 100 price, -- Random price between 100.0 and 1100.0, rnd_double() * 2000 + 0.1 amount -- Random price between 0.1 and 2000.1 FROM long_sequence(10000) x; Our trades table now contains 10,000 randomly-generated trades. The comments indicate how we've structured our ran"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "2000 + 0.1 amount -- Random price between 0.1 and 2000.1 FROM long_sequence(10000) x; Our trades table now contains 10,000 randomly-generated trades. The comments indicate how we've structured our random data. We picked a few companies, BUY vs. SELL, and created a timestamp every minute. We've dictated the overall number of rows generated via long_sequence(10000) . We can bump that up, if we want. We've also conservatively generated a timestamp per minute, even though in reality trades against these companies are likely much more frequent. This helps keep our basic examples basic. Now let's look at the table and its data: 'trades'; It will look similar to this, albeit with alternative randomized values. timestamp symbol side price amount 2024-01-01T00:00:00.000000Z BTC-USD sell 483.904143675277 139.449481016294 2024-01-01T00:00:00.060000Z ETH-USD sell 920.296245196274 920.296245196274 2024-01-01T00:00:00.180000Z UNI-USD sell 643.277468441839 643.277468441839 2024-01-01T00:00:00.360000Z LTC-USD buy 218.0920768859 729.81119178972 2024-01-01T00:00:00.600000Z BTC-USD sell 157.596416931116 691.081778396176 That's some fake market data. What about, say, sensor data? Sensors and readings"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "LTC-USD buy 218.0920768859 729.81119178972 2024-01-01T00:00:00.600000Z BTC-USD sell 157.596416931116 691.081778396176 That's some fake market data. What about, say, sensor data? Sensors and readings ​ This next example will create and populate two more tables. One table will contain the metadata of our sensors, and the other will contain the actual readings (payload data) from these sensors. In both cases, we will create the table and generate the data at the same time. This combines the CREATE & SELECT operations to perform a create-and-insert: Create table as, readings CREATE TABLE readings AS( SELECT x ID, timestamp_sequence(to_timestamp('2019-10-17T00:00:00', 'yyyy-MM-ddTHH:mm:ss'), rnd_long(1,10,0) * 100000L) ts, rnd_double(0)*8 + 15 temp, rnd_long(0, 10000, 0) sensorId FROM long_sequence(10000000) x) TIMESTAMP(ts) PARTITION BY MONTH DEDUP UPSERT KEYS(ts); For our table, we've again hit the following key notes: TIMESTAMP(ts) elects the ts column as a designated timestamp for partitioning over time. PARTITION BY MONTH creates a monthly partition, where the stored data is effectively sharded by month. DEDUP UPSERT KEYS(ts) deduplicates the timestamp column The generated data wi"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "itioning over time. PARTITION BY MONTH creates a monthly partition, where the stored data is effectively sharded by month. DEDUP UPSERT KEYS(ts) deduplicates the timestamp column The generated data will look like the following: ID ts temp sensorId 1 2019-10-17T00:00:00.000000Z 19.37373911 9160 2 2019-10-17T00:00:00.600000Z 21.91184617 9671 3 2019-10-17T00:00:01.400000Z 16.58367834 8731 4 2019-10-17T00:00:01.500000Z 16.69308815 3447 5 2019-10-17T00:00:01.600000Z 19.67991569 7985 ... ... ... ... Nice - and our next table, which includes the sensors themselves and their detail: Create table as, sensors CREATE TABLE sensors AS( SELECT x ID, -- Increasing integer rnd_str('Eberle', 'Honeywell', 'Omron', 'United Automation', 'RS Pro') make, -- Random manufacturer rnd_str('New York', 'Miami', 'Boston', 'Chicago', 'San Francisco') city -- Random city FROM long_sequence(10000) x) Note that we've not included a timestamp in this sensors column. This is one of the rare, demonstrative examples where we're not including it, and thus not taking advantage of the bulk of the benefits received via time-series optimization. As we have a timestamp in the paired readings table, it's helpful to demonstr"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "e we're not including it, and thus not taking advantage of the bulk of the benefits received via time-series optimization. As we have a timestamp in the paired readings table, it's helpful to demonstrate them as a pair. With these two new tables, and our prior financial market data table, we've got a lot of useful queries we can test. Running queries ​ Our financial market data table is a great place to test various aggregate functions , to compute price over time intervals, and similar anaylsis. However, we'll expand on the readings * sensors tables. First, let's look at readings , running our shorthand for SELECT * FROM readings; : readings; Let's then select the count of records from readings : SELECT count() FROM readings; count 10,000,000 And then the average reading: SELECT avg(temp) FROM readings; average 18.999217780895 We can now use the sensors table alongside the readings table to get more interesting results using a JOIN : SELECT * FROM readings JOIN( SELECT ID sensId, make, city FROM sensors) ON readings.sensorId = sensId; The results should look like the table below: ID ts temp sensorId sensId make city 1 2019-10-17T00:00:00.000000Z 16.472200460982 3211 3211 Omron New"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": ", city FROM sensors) ON readings.sensorId = sensId; The results should look like the table below: ID ts temp sensorId sensId make city 1 2019-10-17T00:00:00.000000Z 16.472200460982 3211 3211 Omron New York 2 2019-10-17T00:00:00.100000Z 16.598432033599 2319 2319 Honeywell San Francisco 3 2019-10-17T00:00:00.100000Z 20.293681747009 8723 8723 Honeywell New York 4 2019-10-17T00:00:00.100000Z 20.939263119843 885 885 RS Pro San Francisco 5 2019-10-17T00:00:00.200000Z 19.336660059029 3200 3200 Honeywell San Francisco 6 2019-10-17T00:00:01.100000Z 20.946643576954 4053 4053 Honeywell Miami Note the timestamps returned as we've JOIN'd the tables together. Let's try another type of aggregation: Aggregation keyed by city SELECT city, max(temp) FROM readings JOIN( SELECT ID sensId, city FROM sensors) a ON readings.sensorId = a.sensId; The results should look like the table below: city max New York 22.999998786398 San Francisco 22.999998138348 Miami 22.99999994818 Chicago 22.999991705861 Boston 22.999999233377 Back to time, given we have one table ( readings ) partitioned by time, let's see what we can do when we JOIN the tables together to perform an aggregation based on an hour of time: Aggreg"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "99999233377 Back to time, given we have one table ( readings ) partitioned by time, let's see what we can do when we JOIN the tables together to perform an aggregation based on an hour of time: Aggregation by hourly time buckets SELECT ts, city, make, avg(temp) FROM readings timestamp(ts) JOIN (SELECT ID sensId, city, make FROM sensors WHERE city='Miami' AND make='Omron') a ON readings.sensorId = a.sensId WHERE ts IN '2019-10-21;1d' -- this is an interval between 2019/10/21 and the next day SAMPLE BY 1h -- aggregation by hourly time buckets ALIGN TO CALENDAR; -- align the ts with the start of the hour (hh:00:00) The results should look like the table below: ts city make average 2019-10-21T00:00:00.000000Z Miami Omron 20.004285872098 2019-10-21T00:01:00.000000Z Miami Omron 16.68436714013 2019-10-21T00:02:00.000000Z Miami Omron 15.243684089291 2019-10-21T00:03:00.000000Z Miami Omron 17.193984104315 2019-10-21T00:04:00.000000Z Miami Omron 20.778686822666 ... ... ... ... For more information about these statements, please refer to the SELECT , JOIN and SAMPLE BY pages. Deleting tables ​ We can now clean up the demo data by using DROP TABLE SQL. Be careful using this statement as QuestD"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "on about these statements, please refer to the SELECT , JOIN and SAMPLE BY pages. Deleting tables ​ We can now clean up the demo data by using DROP TABLE SQL. Be careful using this statement as QuestDB cannot recover data that is deleted in this way: DROP TABLE readings; DROP TABLE sensors; DROP TABLE trades; Edit this page Previous Configuration Next Import CSV Before we begin... Creating a table Inserting data Sensors and readings Running queries Deleting tables\n\nGuides & Tutorials Create a sample database On this page Create a sample database This guide walks you through creating a sample dataset. It utilizes rnd_ functions and basic SQL grammar to generate 'mock' data of specific types. For most applications, you will import your data using methods like the InfluxDB Line Protocol, CSV imports, or integration with third-party tools such as Telegraf, Kafka , or Prometheus. If your interest lies in data ingestion rather than generation, refer to our ingestion overview . Alternatively, the QuestDB demo instance offers a practical way to explore data creation and manipulation without setting up your dataset. All that said, in this tutorial you will learn how to: Create tables Popula"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "the QuestDB demo instance offers a practical way to explore data creation and manipulation without setting up your dataset. All that said, in this tutorial you will learn how to: Create tables Populate tables with sample data Run simple and advanced queries Delete tables Before we begin... ​ All commands are run through the Web Console accessible at http://localhost:9000 . You can also run the same SQL via the Postgres endpoint or the REST API . If QuestDB is not running locally, checkout the quick start . Creating a table ​ With QuestDB running, the first step is to create a table. We'll start with one representing financial market data. Then in the insert section, we'll create another pair of tables representing temperature sensors and their readings. Let's start by creating the trades table: CREATE TABLE trades ( timestamp TIMESTAMP, symbol SYMBOL, side SYMBOL, price DOUBLE, amount DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY WAL DEDUP UPSERT KEYS(timestamp, symbol); This is a basic yet robust table. It applies SYMBOL s for ticker and side, a price, and a designated timestamp . It's partitioned by day , is a WAL table , and deduplicates the timestamp and ticker columns. As th"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "sic yet robust table. It applies SYMBOL s for ticker and side, a price, and a designated timestamp . It's partitioned by day , is a WAL table , and deduplicates the timestamp and ticker columns. As the links above show, there's lots to unpack in this table! Feel free to learn more about the nuances. We've done all of this to match the nature of how we'll query this data. We're focused on a the flow of the market, the pulse of the market's day-to-day, hence we've partitioned it as such. We're also leery of duplicates, for accuracy of data, so we'll ensure that if timestamps are identical that we do not create a duplicate. Timestamps are essential for time-series analysis. We'll proceed forward to INSERT. Inserting data ​ Financial market data ​ Let's populate our trades table with procedurally-generated data: Insert as SELECT INSERT INTO trades SELECT timestamp_sequence('2024-01-01T00:00:00', 60000L * x) timestamp, -- Generate a timestamp every minute starting from Jan 1, 2024 rnd_str('ETH-USD', 'BTC-USD', 'SOL-USD', 'LTC-USD', 'UNI-USD') symbol, -- Random ticker symbols rnd_str('buy', 'sell') side, -- Random side (BUY or SELL) rnd_double() * 1000 + 100 price, -- Random price betwee"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "ETH-USD', 'BTC-USD', 'SOL-USD', 'LTC-USD', 'UNI-USD') symbol, -- Random ticker symbols rnd_str('buy', 'sell') side, -- Random side (BUY or SELL) rnd_double() * 1000 + 100 price, -- Random price between 100.0 and 1100.0, rnd_double() * 2000 + 0.1 amount -- Random price between 0.1 and 2000.1 FROM long_sequence(10000) x; Our trades table now contains 10,000 randomly-generated trades. The comments indicate how we've structured our random data. We picked a few companies, BUY vs. SELL, and created a timestamp every minute. We've dictated the overall number of rows generated via long_sequence(10000) . We can bump that up, if we want. We've also conservatively generated a timestamp per minute, even though in reality trades against these companies are likely much more frequent. This helps keep our basic examples basic. Now let's look at the table and its data: 'trades'; It will look similar to this, albeit with alternative randomized values. timestamp symbol side price amount 2024-01-01T00:00:00.000000Z BTC-USD sell 483.904143675277 139.449481016294 2024-01-01T00:00:00.060000Z ETH-USD sell 920.296245196274 920.296245196274 2024-01-01T00:00:00.180000Z UNI-USD sell 643.277468441839 643.27746"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "00.000000Z BTC-USD sell 483.904143675277 139.449481016294 2024-01-01T00:00:00.060000Z ETH-USD sell 920.296245196274 920.296245196274 2024-01-01T00:00:00.180000Z UNI-USD sell 643.277468441839 643.277468441839 2024-01-01T00:00:00.360000Z LTC-USD buy 218.0920768859 729.81119178972 2024-01-01T00:00:00.600000Z BTC-USD sell 157.596416931116 691.081778396176 That's some fake market data. What about, say, sensor data? Sensors and readings ​ This next example will create and populate two more tables. One table will contain the metadata of our sensors, and the other will contain the actual readings (payload data) from these sensors. In both cases, we will create the table and generate the data at the same time. This combines the CREATE & SELECT operations to perform a create-and-insert: Create table as, readings CREATE TABLE readings AS( SELECT x ID, timestamp_sequence(to_timestamp('2019-10-17T00:00:00', 'yyyy-MM-ddTHH:mm:ss'), rnd_long(1,10,0) * 100000L) ts, rnd_double(0)*8 + 15 temp, rnd_long(0, 10000, 0) sensorId FROM long_sequence(10000000) x) TIMESTAMP(ts) PARTITION BY MONTH DEDUP UPSERT KEYS(ts); For our table, we've again hit the following key notes: TIMESTAMP(ts) elects the ts column"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "0, 10000, 0) sensorId FROM long_sequence(10000000) x) TIMESTAMP(ts) PARTITION BY MONTH DEDUP UPSERT KEYS(ts); For our table, we've again hit the following key notes: TIMESTAMP(ts) elects the ts column as a designated timestamp for partitioning over time. PARTITION BY MONTH creates a monthly partition, where the stored data is effectively sharded by month. DEDUP UPSERT KEYS(ts) deduplicates the timestamp column The generated data will look like the following: ID ts temp sensorId 1 2019-10-17T00:00:00.000000Z 19.37373911 9160 2 2019-10-17T00:00:00.600000Z 21.91184617 9671 3 2019-10-17T00:00:01.400000Z 16.58367834 8731 4 2019-10-17T00:00:01.500000Z 16.69308815 3447 5 2019-10-17T00:00:01.600000Z 19.67991569 7985 ... ... ... ... Nice - and our next table, which includes the sensors themselves and their detail: Create table as, sensors CREATE TABLE sensors AS( SELECT x ID, -- Increasing integer rnd_str('Eberle', 'Honeywell', 'Omron', 'United Automation', 'RS Pro') make, -- Random manufacturer rnd_str('New York', 'Miami', 'Boston', 'Chicago', 'San Francisco') city -- Random city FROM long_sequence(10000) x) Note that we've not included a timestamp in this sensors column. This is one of th"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "r rnd_str('New York', 'Miami', 'Boston', 'Chicago', 'San Francisco') city -- Random city FROM long_sequence(10000) x) Note that we've not included a timestamp in this sensors column. This is one of the rare, demonstrative examples where we're not including it, and thus not taking advantage of the bulk of the benefits received via time-series optimization. As we have a timestamp in the paired readings table, it's helpful to demonstrate them as a pair. With these two new tables, and our prior financial market data table, we've got a lot of useful queries we can test. Running queries ​ Our financial market data table is a great place to test various aggregate functions , to compute price over time intervals, and similar anaylsis. However, we'll expand on the readings * sensors tables. First, let's look at readings , running our shorthand for SELECT * FROM readings; : readings; Let's then select the count of records from readings : SELECT count() FROM readings; count 10,000,000 And then the average reading: SELECT avg(temp) FROM readings; average 18.999217780895 We can now use the sensors table alongside the readings table to get more interesting results using a JOIN : SELECT * FROM re"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "e average reading: SELECT avg(temp) FROM readings; average 18.999217780895 We can now use the sensors table alongside the readings table to get more interesting results using a JOIN : SELECT * FROM readings JOIN( SELECT ID sensId, make, city FROM sensors) ON readings.sensorId = sensId; The results should look like the table below: ID ts temp sensorId sensId make city 1 2019-10-17T00:00:00.000000Z 16.472200460982 3211 3211 Omron New York 2 2019-10-17T00:00:00.100000Z 16.598432033599 2319 2319 Honeywell San Francisco 3 2019-10-17T00:00:00.100000Z 20.293681747009 8723 8723 Honeywell New York 4 2019-10-17T00:00:00.100000Z 20.939263119843 885 885 RS Pro San Francisco 5 2019-10-17T00:00:00.200000Z 19.336660059029 3200 3200 Honeywell San Francisco 6 2019-10-17T00:00:01.100000Z 20.946643576954 4053 4053 Honeywell Miami Note the timestamps returned as we've JOIN'd the tables together. Let's try another type of aggregation: Aggregation keyed by city SELECT city, max(temp) FROM readings JOIN( SELECT ID sensId, city FROM sensors) a ON readings.sensorId = a.sensId; The results should look like the table below: city max New York 22.999998786398 San Francisco 22.999998138348 Miami 22.99999994818"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "ECT ID sensId, city FROM sensors) a ON readings.sensorId = a.sensId; The results should look like the table below: city max New York 22.999998786398 San Francisco 22.999998138348 Miami 22.99999994818 Chicago 22.999991705861 Boston 22.999999233377 Back to time, given we have one table ( readings ) partitioned by time, let's see what we can do when we JOIN the tables together to perform an aggregation based on an hour of time: Aggregation by hourly time buckets SELECT ts, city, make, avg(temp) FROM readings timestamp(ts) JOIN (SELECT ID sensId, city, make FROM sensors WHERE city='Miami' AND make='Omron') a ON readings.sensorId = a.sensId WHERE ts IN '2019-10-21;1d' -- this is an interval between 2019/10/21 and the next day SAMPLE BY 1h -- aggregation by hourly time buckets ALIGN TO CALENDAR; -- align the ts with the start of the hour (hh:00:00) The results should look like the table below: ts city make average 2019-10-21T00:00:00.000000Z Miami Omron 20.004285872098 2019-10-21T00:01:00.000000Z Miami Omron 16.68436714013 2019-10-21T00:02:00.000000Z Miami Omron 15.243684089291 2019-10-21T00:03:00.000000Z Miami Omron 17.193984104315 2019-10-21T00:04:00.000000Z Miami Omron 20.778686822666"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "0000Z Miami Omron 16.68436714013 2019-10-21T00:02:00.000000Z Miami Omron 15.243684089291 2019-10-21T00:03:00.000000Z Miami Omron 17.193984104315 2019-10-21T00:04:00.000000Z Miami Omron 20.778686822666 ... ... ... ... For more information about these statements, please refer to the SELECT , JOIN and SAMPLE BY pages. Deleting tables ​ We can now clean up the demo data by using DROP TABLE SQL. Be careful using this statement as QuestDB cannot recover data that is deleted in this way: DROP TABLE readings; DROP TABLE sensors; DROP TABLE trades; Edit this page Previous Configuration Next Import CSV\n\nGuides & Tutorials Create a sample database On this page Create a sample database This guide walks you through creating a sample dataset. It utilizes rnd_ functions and basic SQL grammar to generate 'mock' data of specific types. For most applications, you will import your data using methods like the InfluxDB Line Protocol, CSV imports, or integration with third-party tools such as Telegraf, Kafka , or Prometheus. If your interest lies in data ingestion rather than generation, refer to our ingestion overview . Alternatively, the QuestDB demo instance offers a practical way to explore data cre"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "a , or Prometheus. If your interest lies in data ingestion rather than generation, refer to our ingestion overview . Alternatively, the QuestDB demo instance offers a practical way to explore data creation and manipulation without setting up your dataset. All that said, in this tutorial you will learn how to: Create tables Populate tables with sample data Run simple and advanced queries Delete tables Before we begin... ​ All commands are run through the Web Console accessible at http://localhost:9000 . You can also run the same SQL via the Postgres endpoint or the REST API . If QuestDB is not running locally, checkout the quick start . Creating a table ​ With QuestDB running, the first step is to create a table. We'll start with one representing financial market data. Then in the insert section, we'll create another pair of tables representing temperature sensors and their readings. Let's start by creating the trades table: CREATE TABLE trades ( timestamp TIMESTAMP, symbol SYMBOL, side SYMBOL, price DOUBLE, amount DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY WAL DEDUP UPSERT KEYS(timestamp, symbol); This is a basic yet robust table. It applies SYMBOL s for ticker and side, a pric"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "SYMBOL, price DOUBLE, amount DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY WAL DEDUP UPSERT KEYS(timestamp, symbol); This is a basic yet robust table. It applies SYMBOL s for ticker and side, a price, and a designated timestamp . It's partitioned by day , is a WAL table , and deduplicates the timestamp and ticker columns. As the links above show, there's lots to unpack in this table! Feel free to learn more about the nuances. We've done all of this to match the nature of how we'll query this data. We're focused on a the flow of the market, the pulse of the market's day-to-day, hence we've partitioned it as such. We're also leery of duplicates, for accuracy of data, so we'll ensure that if timestamps are identical that we do not create a duplicate. Timestamps are essential for time-series analysis. We'll proceed forward to INSERT. Inserting data ​ Financial market data ​ Let's populate our trades table with procedurally-generated data: Insert as SELECT INSERT INTO trades SELECT timestamp_sequence('2024-01-01T00:00:00', 60000L * x) timestamp, -- Generate a timestamp every minute starting from Jan 1, 2024 rnd_str('ETH-USD', 'BTC-USD', 'SOL-USD', 'LTC-USD', 'UNI-USD') symbol, -- Rand"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "amp_sequence('2024-01-01T00:00:00', 60000L * x) timestamp, -- Generate a timestamp every minute starting from Jan 1, 2024 rnd_str('ETH-USD', 'BTC-USD', 'SOL-USD', 'LTC-USD', 'UNI-USD') symbol, -- Random ticker symbols rnd_str('buy', 'sell') side, -- Random side (BUY or SELL) rnd_double() * 1000 + 100 price, -- Random price between 100.0 and 1100.0, rnd_double() * 2000 + 0.1 amount -- Random price between 0.1 and 2000.1 FROM long_sequence(10000) x; Our trades table now contains 10,000 randomly-generated trades. The comments indicate how we've structured our random data. We picked a few companies, BUY vs. SELL, and created a timestamp every minute. We've dictated the overall number of rows generated via long_sequence(10000) . We can bump that up, if we want. We've also conservatively generated a timestamp per minute, even though in reality trades against these companies are likely much more frequent. This helps keep our basic examples basic. Now let's look at the table and its data: 'trades'; It will look similar to this, albeit with alternative randomized values. timestamp symbol side price amount 2024-01-01T00:00:00.000000Z BTC-USD sell 483.904143675277 139.449481016294 2024-01-01T"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "des'; It will look similar to this, albeit with alternative randomized values. timestamp symbol side price amount 2024-01-01T00:00:00.000000Z BTC-USD sell 483.904143675277 139.449481016294 2024-01-01T00:00:00.060000Z ETH-USD sell 920.296245196274 920.296245196274 2024-01-01T00:00:00.180000Z UNI-USD sell 643.277468441839 643.277468441839 2024-01-01T00:00:00.360000Z LTC-USD buy 218.0920768859 729.81119178972 2024-01-01T00:00:00.600000Z BTC-USD sell 157.596416931116 691.081778396176 That's some fake market data. What about, say, sensor data? Sensors and readings ​ This next example will create and populate two more tables. One table will contain the metadata of our sensors, and the other will contain the actual readings (payload data) from these sensors. In both cases, we will create the table and generate the data at the same time. This combines the CREATE & SELECT operations to perform a create-and-insert: Create table as, readings CREATE TABLE readings AS( SELECT x ID, timestamp_sequence(to_timestamp('2019-10-17T00:00:00', 'yyyy-MM-ddTHH:mm:ss'), rnd_long(1,10,0) * 100000L) ts, rnd_double(0)*8 + 15 temp, rnd_long(0, 10000, 0) sensorId FROM long_sequence(10000000) x) TIMESTAMP(ts) P"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "ce(to_timestamp('2019-10-17T00:00:00', 'yyyy-MM-ddTHH:mm:ss'), rnd_long(1,10,0) * 100000L) ts, rnd_double(0)*8 + 15 temp, rnd_long(0, 10000, 0) sensorId FROM long_sequence(10000000) x) TIMESTAMP(ts) PARTITION BY MONTH DEDUP UPSERT KEYS(ts); For our table, we've again hit the following key notes: TIMESTAMP(ts) elects the ts column as a designated timestamp for partitioning over time. PARTITION BY MONTH creates a monthly partition, where the stored data is effectively sharded by month. DEDUP UPSERT KEYS(ts) deduplicates the timestamp column The generated data will look like the following: ID ts temp sensorId 1 2019-10-17T00:00:00.000000Z 19.37373911 9160 2 2019-10-17T00:00:00.600000Z 21.91184617 9671 3 2019-10-17T00:00:01.400000Z 16.58367834 8731 4 2019-10-17T00:00:01.500000Z 16.69308815 3447 5 2019-10-17T00:00:01.600000Z 19.67991569 7985 ... ... ... ... Nice - and our next table, which includes the sensors themselves and their detail: Create table as, sensors CREATE TABLE sensors AS( SELECT x ID, -- Increasing integer rnd_str('Eberle', 'Honeywell', 'Omron', 'United Automation', 'RS Pro') make, -- Random manufacturer rnd_str('New York', 'Miami', 'Boston', 'Chicago', 'San Francisco')"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "LECT x ID, -- Increasing integer rnd_str('Eberle', 'Honeywell', 'Omron', 'United Automation', 'RS Pro') make, -- Random manufacturer rnd_str('New York', 'Miami', 'Boston', 'Chicago', 'San Francisco') city -- Random city FROM long_sequence(10000) x) Note that we've not included a timestamp in this sensors column. This is one of the rare, demonstrative examples where we're not including it, and thus not taking advantage of the bulk of the benefits received via time-series optimization. As we have a timestamp in the paired readings table, it's helpful to demonstrate them as a pair. With these two new tables, and our prior financial market data table, we've got a lot of useful queries we can test. Running queries ​ Our financial market data table is a great place to test various aggregate functions , to compute price over time intervals, and similar anaylsis. However, we'll expand on the readings * sensors tables. First, let's look at readings , running our shorthand for SELECT * FROM readings; : readings; Let's then select the count of records from readings : SELECT count() FROM readings; count 10,000,000 And then the average reading: SELECT avg(temp) FROM readings; average 18.9992177"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "ings; : readings; Let's then select the count of records from readings : SELECT count() FROM readings; count 10,000,000 And then the average reading: SELECT avg(temp) FROM readings; average 18.999217780895 We can now use the sensors table alongside the readings table to get more interesting results using a JOIN : SELECT * FROM readings JOIN( SELECT ID sensId, make, city FROM sensors) ON readings.sensorId = sensId; The results should look like the table below: ID ts temp sensorId sensId make city 1 2019-10-17T00:00:00.000000Z 16.472200460982 3211 3211 Omron New York 2 2019-10-17T00:00:00.100000Z 16.598432033599 2319 2319 Honeywell San Francisco 3 2019-10-17T00:00:00.100000Z 20.293681747009 8723 8723 Honeywell New York 4 2019-10-17T00:00:00.100000Z 20.939263119843 885 885 RS Pro San Francisco 5 2019-10-17T00:00:00.200000Z 19.336660059029 3200 3200 Honeywell San Francisco 6 2019-10-17T00:00:01.100000Z 20.946643576954 4053 4053 Honeywell Miami Note the timestamps returned as we've JOIN'd the tables together. Let's try another type of aggregation: Aggregation keyed by city SELECT city, max(temp) FROM readings JOIN( SELECT ID sensId, city FROM sensors) a ON readings.sensorId = a.sensId;"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "he tables together. Let's try another type of aggregation: Aggregation keyed by city SELECT city, max(temp) FROM readings JOIN( SELECT ID sensId, city FROM sensors) a ON readings.sensorId = a.sensId; The results should look like the table below: city max New York 22.999998786398 San Francisco 22.999998138348 Miami 22.99999994818 Chicago 22.999991705861 Boston 22.999999233377 Back to time, given we have one table ( readings ) partitioned by time, let's see what we can do when we JOIN the tables together to perform an aggregation based on an hour of time: Aggregation by hourly time buckets SELECT ts, city, make, avg(temp) FROM readings timestamp(ts) JOIN (SELECT ID sensId, city, make FROM sensors WHERE city='Miami' AND make='Omron') a ON readings.sensorId = a.sensId WHERE ts IN '2019-10-21;1d' -- this is an interval between 2019/10/21 and the next day SAMPLE BY 1h -- aggregation by hourly time buckets ALIGN TO CALENDAR; -- align the ts with the start of the hour (hh:00:00) The results should look like the table below: ts city make average 2019-10-21T00:00:00.000000Z Miami Omron 20.004285872098 2019-10-21T00:01:00.000000Z Miami Omron 16.68436714013 2019-10-21T00:02:00.000000Z Miami Om"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "ould look like the table below: ts city make average 2019-10-21T00:00:00.000000Z Miami Omron 20.004285872098 2019-10-21T00:01:00.000000Z Miami Omron 16.68436714013 2019-10-21T00:02:00.000000Z Miami Omron 15.243684089291 2019-10-21T00:03:00.000000Z Miami Omron 17.193984104315 2019-10-21T00:04:00.000000Z Miami Omron 20.778686822666 ... ... ... ... For more information about these statements, please refer to the SELECT , JOIN and SAMPLE BY pages. Deleting tables ​ We can now clean up the demo data by using DROP TABLE SQL. Be careful using this statement as QuestDB cannot recover data that is deleted in this way: DROP TABLE readings; DROP TABLE sensors; DROP TABLE trades; Edit this page Previous Configuration Next Import CSV\n\nGuides & Tutorials\n\nCreate a sample database\n\nOn this page\n\nCreate a sample database This guide walks you through creating a sample dataset. It utilizes rnd_ functions and basic SQL grammar to generate 'mock' data of specific types. For most applications, you will import your data using methods like the InfluxDB Line Protocol, CSV imports, or integration with third-party tools such as Telegraf, Kafka , or Prometheus. If your interest lies in data ingestion rather"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "ort your data using methods like the InfluxDB Line Protocol, CSV imports, or integration with third-party tools such as Telegraf, Kafka , or Prometheus. If your interest lies in data ingestion rather than generation, refer to our ingestion overview . Alternatively, the QuestDB demo instance offers a practical way to explore data creation and manipulation without setting up your dataset. All that said, in this tutorial you will learn how to: Create tables Populate tables with sample data Run simple and advanced queries Delete tables Before we begin... ​ All commands are run through the Web Console accessible at http://localhost:9000 . You can also run the same SQL via the Postgres endpoint or the REST API . If QuestDB is not running locally, checkout the quick start . Creating a table ​ With QuestDB running, the first step is to create a table. We'll start with one representing financial market data. Then in the insert section, we'll create another pair of tables representing temperature sensors and their readings. Let's start by creating the trades table: CREATE TABLE trades ( timestamp TIMESTAMP, symbol SYMBOL, side SYMBOL, price DOUBLE, amount DOUBLE ) TIMESTAMP(timestamp) PARTIT"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "e sensors and their readings. Let's start by creating the trades table: CREATE TABLE trades ( timestamp TIMESTAMP, symbol SYMBOL, side SYMBOL, price DOUBLE, amount DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY WAL DEDUP UPSERT KEYS(timestamp, symbol); This is a basic yet robust table. It applies SYMBOL s for ticker and side, a price, and a designated timestamp . It's partitioned by day , is a WAL table , and deduplicates the timestamp and ticker columns. As the links above show, there's lots to unpack in this table! Feel free to learn more about the nuances. We've done all of this to match the nature of how we'll query this data. We're focused on a the flow of the market, the pulse of the market's day-to-day, hence we've partitioned it as such. We're also leery of duplicates, for accuracy of data, so we'll ensure that if timestamps are identical that we do not create a duplicate. Timestamps are essential for time-series analysis. We'll proceed forward to INSERT. Inserting data ​ Financial market data ​ Let's populate our trades table with procedurally-generated data: Insert as SELECT INSERT INTO trades SELECT timestamp_sequence('2024-01-01T00:00:00', 60000L * x) timestamp, -- Gene"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "cial market data ​ Let's populate our trades table with procedurally-generated data: Insert as SELECT INSERT INTO trades SELECT timestamp_sequence('2024-01-01T00:00:00', 60000L * x) timestamp, -- Generate a timestamp every minute starting from Jan 1, 2024 rnd_str('ETH-USD', 'BTC-USD', 'SOL-USD', 'LTC-USD', 'UNI-USD') symbol, -- Random ticker symbols rnd_str('buy', 'sell') side, -- Random side (BUY or SELL) rnd_double() * 1000 + 100 price, -- Random price between 100.0 and 1100.0, rnd_double() * 2000 + 0.1 amount -- Random price between 0.1 and 2000.1 FROM long_sequence(10000) x; Our trades table now contains 10,000 randomly-generated trades. The comments indicate how we've structured our random data. We picked a few companies, BUY vs. SELL, and created a timestamp every minute. We've dictated the overall number of rows generated via long_sequence(10000) . We can bump that up, if we want. We've also conservatively generated a timestamp per minute, even though in reality trades against these companies are likely much more frequent. This helps keep our basic examples basic. Now let's look at the table and its data: 'trades'; It will look similar to this, albeit with alternative random"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "hese companies are likely much more frequent. This helps keep our basic examples basic. Now let's look at the table and its data: 'trades'; It will look similar to this, albeit with alternative randomized values. timestamp symbol side price amount 2024-01-01T00:00:00.000000Z BTC-USD sell 483.904143675277 139.449481016294 2024-01-01T00:00:00.060000Z ETH-USD sell 920.296245196274 920.296245196274 2024-01-01T00:00:00.180000Z UNI-USD sell 643.277468441839 643.277468441839 2024-01-01T00:00:00.360000Z LTC-USD buy 218.0920768859 729.81119178972 2024-01-01T00:00:00.600000Z BTC-USD sell 157.596416931116 691.081778396176 That's some fake market data. What about, say, sensor data? Sensors and readings ​ This next example will create and populate two more tables. One table will contain the metadata of our sensors, and the other will contain the actual readings (payload data) from these sensors. In both cases, we will create the table and generate the data at the same time. This combines the CREATE & SELECT operations to perform a create-and-insert: Create table as, readings CREATE TABLE readings AS( SELECT x ID, timestamp_sequence(to_timestamp('2019-10-17T00:00:00', 'yyyy-MM-ddTHH:mm:ss'), rnd"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "& SELECT operations to perform a create-and-insert: Create table as, readings CREATE TABLE readings AS( SELECT x ID, timestamp_sequence(to_timestamp('2019-10-17T00:00:00', 'yyyy-MM-ddTHH:mm:ss'), rnd_long(1,10,0) * 100000L) ts, rnd_double(0)*8 + 15 temp, rnd_long(0, 10000, 0) sensorId FROM long_sequence(10000000) x) TIMESTAMP(ts) PARTITION BY MONTH DEDUP UPSERT KEYS(ts); For our table, we've again hit the following key notes: TIMESTAMP(ts) elects the ts column as a designated timestamp for partitioning over time. PARTITION BY MONTH creates a monthly partition, where the stored data is effectively sharded by month. DEDUP UPSERT KEYS(ts) deduplicates the timestamp column The generated data will look like the following: ID ts temp sensorId 1 2019-10-17T00:00:00.000000Z 19.37373911 9160 2 2019-10-17T00:00:00.600000Z 21.91184617 9671 3 2019-10-17T00:00:01.400000Z 16.58367834 8731 4 2019-10-17T00:00:01.500000Z 16.69308815 3447 5 2019-10-17T00:00:01.600000Z 19.67991569 7985 ... ... ... ... Nice - and our next table, which includes the sensors themselves and their detail: Create table as, sensors CREATE TABLE sensors AS( SELECT x ID, -- Increasing integer rnd_str('Eberle', 'Honeywell', 'O"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "ice - and our next table, which includes the sensors themselves and their detail: Create table as, sensors CREATE TABLE sensors AS( SELECT x ID, -- Increasing integer rnd_str('Eberle', 'Honeywell', 'Omron', 'United Automation', 'RS Pro') make, -- Random manufacturer rnd_str('New York', 'Miami', 'Boston', 'Chicago', 'San Francisco') city -- Random city FROM long_sequence(10000) x) Note that we've not included a timestamp in this sensors column. This is one of the rare, demonstrative examples where we're not including it, and thus not taking advantage of the bulk of the benefits received via time-series optimization. As we have a timestamp in the paired readings table, it's helpful to demonstrate them as a pair. With these two new tables, and our prior financial market data table, we've got a lot of useful queries we can test. Running queries ​ Our financial market data table is a great place to test various aggregate functions , to compute price over time intervals, and similar anaylsis. However, we'll expand on the readings * sensors tables. First, let's look at readings , running our shorthand for SELECT * FROM readings; : readings; Let's then select the count of records from read"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": ". However, we'll expand on the readings * sensors tables. First, let's look at readings , running our shorthand for SELECT * FROM readings; : readings; Let's then select the count of records from readings : SELECT count() FROM readings; count 10,000,000 And then the average reading: SELECT avg(temp) FROM readings; average 18.999217780895 We can now use the sensors table alongside the readings table to get more interesting results using a JOIN : SELECT * FROM readings JOIN( SELECT ID sensId, make, city FROM sensors) ON readings.sensorId = sensId; The results should look like the table below: ID ts temp sensorId sensId make city 1 2019-10-17T00:00:00.000000Z 16.472200460982 3211 3211 Omron New York 2 2019-10-17T00:00:00.100000Z 16.598432033599 2319 2319 Honeywell San Francisco 3 2019-10-17T00:00:00.100000Z 20.293681747009 8723 8723 Honeywell New York 4 2019-10-17T00:00:00.100000Z 20.939263119843 885 885 RS Pro San Francisco 5 2019-10-17T00:00:00.200000Z 19.336660059029 3200 3200 Honeywell San Francisco 6 2019-10-17T00:00:01.100000Z 20.946643576954 4053 4053 Honeywell Miami Note the timestamps returned as we've JOIN'd the tables together. Let's try another type of aggregation: Aggrega"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "l San Francisco 6 2019-10-17T00:00:01.100000Z 20.946643576954 4053 4053 Honeywell Miami Note the timestamps returned as we've JOIN'd the tables together. Let's try another type of aggregation: Aggregation keyed by city SELECT city, max(temp) FROM readings JOIN( SELECT ID sensId, city FROM sensors) a ON readings.sensorId = a.sensId; The results should look like the table below: city max New York 22.999998786398 San Francisco 22.999998138348 Miami 22.99999994818 Chicago 22.999991705861 Boston 22.999999233377 Back to time, given we have one table ( readings ) partitioned by time, let's see what we can do when we JOIN the tables together to perform an aggregation based on an hour of time: Aggregation by hourly time buckets SELECT ts, city, make, avg(temp) FROM readings timestamp(ts) JOIN (SELECT ID sensId, city, make FROM sensors WHERE city='Miami' AND make='Omron') a ON readings.sensorId = a.sensId WHERE ts IN '2019-10-21;1d' -- this is an interval between 2019/10/21 and the next day SAMPLE BY 1h -- aggregation by hourly time buckets ALIGN TO CALENDAR; -- align the ts with the start of the hour (hh:00:00) The results should look like the table below: ts city make average 2019-10-21T00"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Introduction", "text": "LE BY 1h -- aggregation by hourly time buckets ALIGN TO CALENDAR; -- align the ts with the start of the hour (hh:00:00) The results should look like the table below: ts city make average 2019-10-21T00:00:00.000000Z Miami Omron 20.004285872098 2019-10-21T00:01:00.000000Z Miami Omron 16.68436714013 2019-10-21T00:02:00.000000Z Miami Omron 15.243684089291 2019-10-21T00:03:00.000000Z Miami Omron 17.193984104315 2019-10-21T00:04:00.000000Z Miami Omron 20.778686822666 ... ... ... ... For more information about these statements, please refer to the SELECT , JOIN and SAMPLE BY pages. Deleting tables ​ We can now clean up the demo data by using DROP TABLE SQL. Be careful using this statement as QuestDB cannot recover data that is deleted in this way: DROP TABLE readings; DROP TABLE sensors; DROP TABLE trades;"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Create a sample database", "text": "This guide walks you through creating a sample dataset.\n\nIt utilizes rnd_ functions and basic SQL grammar to generate 'mock' data of specific types.\n\nrnd_\n\nFor most applications, you will import your data using methods like the InfluxDB Line Protocol, CSV imports, or integration with third-party tools such as Telegraf, Kafka , or Prometheus. If your interest lies in data ingestion rather than generation, refer to our ingestion overview . Alternatively, the QuestDB demo instance offers a practical way to explore data creation and manipulation without setting up your dataset.\n\nAll that said, in this tutorial you will learn how to:\n\nCreate tables\n\nPopulate tables with sample data\n\nRun simple and advanced queries\n\nDelete tables"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Before we begin... ​", "text": "All commands are run through the Web Console accessible at http://localhost:9000 .\n\nYou can also run the same SQL via the Postgres endpoint or the REST API .\n\nIf QuestDB is not running locally, checkout the quick start ."}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Creating a table ​", "text": "With QuestDB running, the first step is to create a table.\n\nWe'll start with one representing financial market data. Then in the insert section, we'll create another pair of tables representing temperature sensors and their readings.\n\nLet's start by creating the trades table:\n\ntrades\n\nCREATE TABLE trades ( timestamp TIMESTAMP, symbol SYMBOL, side SYMBOL, price DOUBLE, amount DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY WAL DEDUP UPSERT KEYS(timestamp, symbol);\n\nCREATE TABLE trades ( timestamp TIMESTAMP, symbol SYMBOL, side SYMBOL, price DOUBLE, amount DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY WAL DEDUP UPSERT KEYS(timestamp, symbol);\n\nCREATE TABLE trades ( timestamp TIMESTAMP, symbol SYMBOL, side SYMBOL, price DOUBLE, amount DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY WAL DEDUP UPSERT KEYS(timestamp, symbol);\n\nCREATE TABLE trades ( timestamp TIMESTAMP, symbol SYMBOL, side SYMBOL, price DOUBLE, amount DOUBLE ) TIMESTAMP(timestamp) PARTITION BY DAY WAL DEDUP UPSERT KEYS(timestamp, symbol);\n\nThis is a basic yet robust table. It applies SYMBOL s for ticker and side, a price, and a designated timestamp . It's partitioned by day , is a WAL table , and deduplicates the timestamp"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Creating a table ​", "text": "mp, symbol);\n\nThis is a basic yet robust table. It applies SYMBOL s for ticker and side, a price, and a designated timestamp . It's partitioned by day , is a WAL table , and deduplicates the timestamp and ticker columns. As the links above show, there's lots to unpack in this table! Feel free to learn more about the nuances.\n\nWe've done all of this to match the nature of how we'll query this data. We're focused on a the flow of the market, the pulse of the market's day-to-day, hence we've partitioned it as such. We're also leery of duplicates, for accuracy of data, so we'll ensure that if timestamps are identical that we do not create a duplicate. Timestamps are essential for time-series analysis.\n\nWe'll proceed forward to INSERT."}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Inserting data ​", "text": "Let's populate our trades table with procedurally-generated data:\n\ntrades\n\nInsert as SELECT INSERT INTO trades SELECT timestamp_sequence('2024-01-01T00:00:00', 60000L * x) timestamp, -- Generate a timestamp every minute starting from Jan 1, 2024 rnd_str('ETH-USD', 'BTC-USD', 'SOL-USD', 'LTC-USD', 'UNI-USD') symbol, -- Random ticker symbols rnd_str('buy', 'sell') side, -- Random side (BUY or SELL) rnd_double() * 1000 + 100 price, -- Random price between 100.0 and 1100.0, rnd_double() * 2000 + 0.1 amount -- Random price between 0.1 and 2000.1 FROM long_sequence(10000) x;\n\nInsert as SELECT\n\nINSERT INTO trades SELECT timestamp_sequence('2024-01-01T00:00:00', 60000L * x) timestamp, -- Generate a timestamp every minute starting from Jan 1, 2024 rnd_str('ETH-USD', 'BTC-USD', 'SOL-USD', 'LTC-USD', 'UNI-USD') symbol, -- Random ticker symbols rnd_str('buy', 'sell') side, -- Random side (BUY or SELL) rnd_double() * 1000 + 100 price, -- Random price between 100.0 and 1100.0, rnd_double() * 2000 + 0.1 amount -- Random price between 0.1 and 2000.1 FROM long_sequence(10000) x;\n\nINSERT INTO trades SELECT timestamp_sequence('2024-01-01T00:00:00', 60000L * x) timestamp, -- Generate a timestamp every"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Inserting data ​", "text": "0.1 amount -- Random price between 0.1 and 2000.1 FROM long_sequence(10000) x;\n\nINSERT INTO trades SELECT timestamp_sequence('2024-01-01T00:00:00', 60000L * x) timestamp, -- Generate a timestamp every minute starting from Jan 1, 2024 rnd_str('ETH-USD', 'BTC-USD', 'SOL-USD', 'LTC-USD', 'UNI-USD') symbol, -- Random ticker symbols rnd_str('buy', 'sell') side, -- Random side (BUY or SELL) rnd_double() * 1000 + 100 price, -- Random price between 100.0 and 1100.0, rnd_double() * 2000 + 0.1 amount -- Random price between 0.1 and 2000.1 FROM long_sequence(10000) x;\n\nINSERT INTO trades SELECT timestamp_sequence('2024-01-01T00:00:00', 60000L * x) timestamp, -- Generate a timestamp every minute starting from Jan 1, 2024 rnd_str('ETH-USD', 'BTC-USD', 'SOL-USD', 'LTC-USD', 'UNI-USD') symbol, -- Random ticker symbols rnd_str('buy', 'sell') side, -- Random side (BUY or SELL) rnd_double() * 1000 + 100 price, -- Random price between 100.0 and 1100.0, rnd_double() * 2000 + 0.1 amount -- Random price between 0.1 and 2000.1 FROM long_sequence(10000) x;\n\nOur trades table now contains 10,000 randomly-generated trades. The comments indicate how we've structured our random data. We picked a few companies,"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Inserting data ​", "text": "ween 0.1 and 2000.1 FROM long_sequence(10000) x;\n\nOur trades table now contains 10,000 randomly-generated trades. The comments indicate how we've structured our random data. We picked a few companies, BUY vs. SELL, and created a timestamp every minute. We've dictated the overall number of rows generated via long_sequence(10000) . We can bump that up, if we want.\n\ntrades\n\nlong_sequence(10000)\n\nWe've also conservatively generated a timestamp per minute, even though in reality trades against these companies are likely much more frequent. This helps keep our basic examples basic.\n\nNow let's look at the table and its data:\n\n'trades';\n\n'trades';\n\n'trades';\n\n'trades';\n\nIt will look similar to this, albeit with alternative randomized values.\n\nThat's some fake market data. What about, say, sensor data?"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Sensors and readings ​", "text": "This next example will create and populate two more tables. One table will contain the metadata of our sensors, and the other will contain the actual readings (payload data) from these sensors. In both cases, we will create the table and generate the data at the same time.\n\nThis combines the CREATE & SELECT operations to perform a create-and-insert:\n\nCreate table as, readings CREATE TABLE readings AS( SELECT x ID, timestamp_sequence(to_timestamp('2019-10-17T00:00:00', 'yyyy-MM-ddTHH:mm:ss'), rnd_long(1,10,0) * 100000L) ts, rnd_double(0)*8 + 15 temp, rnd_long(0, 10000, 0) sensorId FROM long_sequence(10000000) x) TIMESTAMP(ts) PARTITION BY MONTH DEDUP UPSERT KEYS(ts);\n\nCreate table as, readings\n\nCREATE TABLE readings AS( SELECT x ID, timestamp_sequence(to_timestamp('2019-10-17T00:00:00', 'yyyy-MM-ddTHH:mm:ss'), rnd_long(1,10,0) * 100000L) ts, rnd_double(0)*8 + 15 temp, rnd_long(0, 10000, 0) sensorId FROM long_sequence(10000000) x) TIMESTAMP(ts) PARTITION BY MONTH DEDUP UPSERT KEYS(ts);\n\nCREATE TABLE readings AS( SELECT x ID, timestamp_sequence(to_timestamp('2019-10-17T00:00:00', 'yyyy-MM-ddTHH:mm:ss'), rnd_long(1,10,0) * 100000L) ts, rnd_double(0)*8 + 15 temp, rnd_long(0, 10000, 0) s"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Sensors and readings ​", "text": "CREATE TABLE readings AS( SELECT x ID, timestamp_sequence(to_timestamp('2019-10-17T00:00:00', 'yyyy-MM-ddTHH:mm:ss'), rnd_long(1,10,0) * 100000L) ts, rnd_double(0)*8 + 15 temp, rnd_long(0, 10000, 0) sensorId FROM long_sequence(10000000) x) TIMESTAMP(ts) PARTITION BY MONTH DEDUP UPSERT KEYS(ts);\n\nCREATE TABLE readings AS( SELECT x ID, timestamp_sequence(to_timestamp('2019-10-17T00:00:00', 'yyyy-MM-ddTHH:mm:ss'), rnd_long(1,10,0) * 100000L) ts, rnd_double(0)*8 + 15 temp, rnd_long(0, 10000, 0) sensorId FROM long_sequence(10000000) x) TIMESTAMP(ts) PARTITION BY MONTH DEDUP UPSERT KEYS(ts);\n\nFor our table, we've again hit the following key notes:\n\nTIMESTAMP(ts) elects the ts column as a designated timestamp for partitioning over time.\n\nTIMESTAMP(ts)\n\nts\n\nPARTITION BY MONTH creates a monthly partition, where the stored data is effectively sharded by month.\n\nPARTITION BY MONTH\n\nDEDUP UPSERT KEYS(ts) deduplicates the timestamp column\n\nDEDUP UPSERT KEYS(ts)\n\nThe generated data will look like the following:\n\nNice - and our next table, which includes the sensors themselves and their detail:\n\nCreate table as, sensors CREATE TABLE sensors AS( SELECT x ID, -- Increasing integer rnd_str('Eberle',"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Sensors and readings ​", "text": "e following:\n\nNice - and our next table, which includes the sensors themselves and their detail:\n\nCreate table as, sensors CREATE TABLE sensors AS( SELECT x ID, -- Increasing integer rnd_str('Eberle', 'Honeywell', 'Omron', 'United Automation', 'RS Pro') make, -- Random manufacturer rnd_str('New York', 'Miami', 'Boston', 'Chicago', 'San Francisco') city -- Random city FROM long_sequence(10000) x)\n\nCreate table as, sensors\n\nCREATE TABLE sensors AS( SELECT x ID, -- Increasing integer rnd_str('Eberle', 'Honeywell', 'Omron', 'United Automation', 'RS Pro') make, -- Random manufacturer rnd_str('New York', 'Miami', 'Boston', 'Chicago', 'San Francisco') city -- Random city FROM long_sequence(10000) x)\n\nCREATE TABLE sensors AS( SELECT x ID, -- Increasing integer rnd_str('Eberle', 'Honeywell', 'Omron', 'United Automation', 'RS Pro') make, -- Random manufacturer rnd_str('New York', 'Miami', 'Boston', 'Chicago', 'San Francisco') city -- Random city FROM long_sequence(10000) x)\n\nCREATE TABLE sensors AS( SELECT x ID, -- Increasing integer rnd_str('Eberle', 'Honeywell', 'Omron', 'United Automation', 'RS Pro') make, -- Random manufacturer rnd_str('New York', 'Miami', 'Boston', 'Chicago', 'San Franc"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Sensors and readings ​", "text": "AS( SELECT x ID, -- Increasing integer rnd_str('Eberle', 'Honeywell', 'Omron', 'United Automation', 'RS Pro') make, -- Random manufacturer rnd_str('New York', 'Miami', 'Boston', 'Chicago', 'San Francisco') city -- Random city FROM long_sequence(10000) x)\n\nNote that we've not included a timestamp in this sensors column. This is one of the rare, demonstrative examples where we're not including it, and thus not taking advantage of the bulk of the benefits received via time-series optimization. As we have a timestamp in the paired readings table, it's helpful to demonstrate them as a pair.\n\nreadings\n\nWith these two new tables, and our prior financial market data table, we've got a lot of useful queries we can test."}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Running queries ​", "text": "Our financial market data table is a great place to test various aggregate functions , to compute price over time intervals, and similar anaylsis.\n\nHowever, we'll expand on the readings * sensors tables.\n\nreadings\n\nsensors\n\nFirst, let's look at readings , running our shorthand for SELECT * FROM readings; :\n\nreadings\n\nSELECT * FROM readings;\n\nreadings;\n\nreadings;\n\nreadings;\n\nreadings;\n\nLet's then select the count of records from readings :\n\ncount\n\nreadings\n\nSELECT count() FROM readings;\n\nSELECT count() FROM readings;\n\nSELECT count() FROM readings;\n\nSELECT count() FROM readings;\n\nAnd then the average reading:\n\nSELECT avg(temp) FROM readings;\n\nSELECT avg(temp) FROM readings;\n\nSELECT avg(temp) FROM readings;\n\nSELECT avg(temp) FROM readings;\n\nWe can now use the sensors table alongside the readings table to get more interesting results using a JOIN :\n\nsensors\n\nreadings\n\nJOIN\n\nSELECT * FROM readings JOIN( SELECT ID sensId, make, city FROM sensors) ON readings.sensorId = sensId;\n\nSELECT * FROM readings JOIN( SELECT ID sensId, make, city FROM sensors) ON readings.sensorId = sensId;\n\nSELECT * FROM readings JOIN( SELECT ID sensId, make, city FROM sensors) ON readings.sensorId = sensId;\n\nSELEC"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Running queries ​", "text": "readings JOIN( SELECT ID sensId, make, city FROM sensors) ON readings.sensorId = sensId;\n\nSELECT * FROM readings JOIN( SELECT ID sensId, make, city FROM sensors) ON readings.sensorId = sensId;\n\nSELECT * FROM readings JOIN( SELECT ID sensId, make, city FROM sensors) ON readings.sensorId = sensId;\n\nThe results should look like the table below:\n\nNote the timestamps returned as we've JOIN'd the tables together.\n\nLet's try another type of aggregation:\n\nAggregation keyed by city SELECT city, max(temp) FROM readings JOIN( SELECT ID sensId, city FROM sensors) a ON readings.sensorId = a.sensId;\n\nAggregation keyed by city\n\nSELECT city, max(temp) FROM readings JOIN( SELECT ID sensId, city FROM sensors) a ON readings.sensorId = a.sensId;\n\nSELECT city, max(temp) FROM readings JOIN( SELECT ID sensId, city FROM sensors) a ON readings.sensorId = a.sensId;\n\nSELECT city, max(temp) FROM readings JOIN( SELECT ID sensId, city FROM sensors) a ON readings.sensorId = a.sensId;\n\nThe results should look like the table below:\n\nBack to time, given we have one table ( readings ) partitioned by time, let's see what we can do when we JOIN the tables together to perform an aggregation based on an hour of time:"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Running queries ​", "text": "he table below:\n\nBack to time, given we have one table ( readings ) partitioned by time, let's see what we can do when we JOIN the tables together to perform an aggregation based on an hour of time:\n\nreadings\n\nAggregation by hourly time buckets SELECT ts, city, make, avg(temp) FROM readings timestamp(ts) JOIN (SELECT ID sensId, city, make FROM sensors WHERE city='Miami' AND make='Omron') a ON readings.sensorId = a.sensId WHERE ts IN '2019-10-21;1d' -- this is an interval between 2019/10/21 and the next day SAMPLE BY 1h -- aggregation by hourly time buckets ALIGN TO CALENDAR; -- align the ts with the start of the hour (hh:00:00)\n\nAggregation by hourly time buckets\n\nSELECT ts, city, make, avg(temp) FROM readings timestamp(ts) JOIN (SELECT ID sensId, city, make FROM sensors WHERE city='Miami' AND make='Omron') a ON readings.sensorId = a.sensId WHERE ts IN '2019-10-21;1d' -- this is an interval between 2019/10/21 and the next day SAMPLE BY 1h -- aggregation by hourly time buckets ALIGN TO CALENDAR; -- align the ts with the start of the hour (hh:00:00)\n\nSELECT ts, city, make, avg(temp) FROM readings timestamp(ts) JOIN (SELECT ID sensId, city, make FROM sensors WHERE city='Miami' AND mak"}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Running queries ​", "text": "CALENDAR; -- align the ts with the start of the hour (hh:00:00)\n\nSELECT ts, city, make, avg(temp) FROM readings timestamp(ts) JOIN (SELECT ID sensId, city, make FROM sensors WHERE city='Miami' AND make='Omron') a ON readings.sensorId = a.sensId WHERE ts IN '2019-10-21;1d' -- this is an interval between 2019/10/21 and the next day SAMPLE BY 1h -- aggregation by hourly time buckets ALIGN TO CALENDAR; -- align the ts with the start of the hour (hh:00:00)\n\nSELECT ts, city, make, avg(temp) FROM readings timestamp(ts) JOIN (SELECT ID sensId, city, make FROM sensors WHERE city='Miami' AND make='Omron') a ON readings.sensorId = a.sensId WHERE ts IN '2019-10-21;1d' -- this is an interval between 2019/10/21 and the next day SAMPLE BY 1h -- aggregation by hourly time buckets ALIGN TO CALENDAR; -- align the ts with the start of the hour (hh:00:00)\n\nThe results should look like the table below:\n\nFor more information about these statements, please refer to the SELECT , JOIN and SAMPLE BY pages."}
{"source_url": "https://questdb.com/docs/guides/create-database", "title": "Create a sample database", "section": "Deleting tables ​", "text": "We can now clean up the demo data by using DROP TABLE SQL. Be careful using this statement as QuestDB cannot recover data that is deleted in this way:\n\nDROP TABLE\n\nDROP TABLE readings; DROP TABLE sensors; DROP TABLE trades;\n\nDROP TABLE readings; DROP TABLE sensors; DROP TABLE trades;\n\nDROP TABLE readings; DROP TABLE sensors; DROP TABLE trades;\n\nDROP TABLE readings; DROP TABLE sensors; DROP TABLE trades;\n\nEdit this page\n\nEdit this page\n\nPrevious\n\nConfiguration\n\nNext\n\nImport CSV\n\nBefore we begin... Creating a table Inserting data Sensors and readings Running queries Deleting tables\n\nBefore we begin... Creating a table Inserting data Sensors and readings Running queries Deleting tables\n\nBefore we begin...\n\nCreating a table\n\nInserting data\n\nSensors and readings\n\nRunning queries\n\nDeleting tables"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "Concepts Data deduplication On this page Data Deduplication Starting from QuestDB 7.3, there is an option to enable storage-level data deduplication on a table. Data deduplication works on all the data inserted into the table and replaces matching rows with the new versions. Only new rows that do no match existing data will be inserted. note Deduplication can only be enabled for Write-Ahead Log (WAL) tables. Practical considerations ​ Deduplication in QuestDB makes table inserts idempotent . The primary use case is to allow for re-sending data within a given time range without creating duplicates. This can be particularly useful in situations where there is an error in sending data, such as when using InfluxDB Line Protocol , and there is no clear indication of how much of the data has already been written. With deduplication enabled, it is safe to re-send data from a fixed period in the past to resume the writing process. Enabling deduplication on a table has an impact on writing performance, especially when multiple UPSERT KEYS are configured. Generally, if the data have mostly unique timestamps across all the rows, the performance impact of deduplication is low. Conversely, the"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "ormance, especially when multiple UPSERT KEYS are configured. Generally, if the data have mostly unique timestamps across all the rows, the performance impact of deduplication is low. Conversely, the most demanding data pattern occurs when there are many rows with the same timestamp that need to be deduplicated on additional columns. For example, in use cases where 10 million devices send CPU metrics every second precisely, deduplicating the data based on the device ID can be expensive. However, in cases where CPU metrics are sent at random and typically have unique timestamps, the cost of deduplication is negligible. note The on-disk ordering of rows with duplicate timestamps differs when deduplication is enabled. Without deduplication: the insertion order of each row will be preserved for rows with the same timestamp With deduplication: the rows will be stored in order sorted by the DEDUP UPSERT keys, with the same timestamp For example: DEDUP UPSERT keys(timestamp, symbol, price) -- will be stored on-disk in an order like: ORDER BY timestamp, symbol, price This is the natural order of data returned in plain queries, without any grouping, filtering or ordering. The SQL standard d"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "will be stored on-disk in an order like: ORDER BY timestamp, symbol, price This is the natural order of data returned in plain queries, without any grouping, filtering or ordering. The SQL standard does not guarantee the ordering of result sets without explicit ORDER BY clauses. Configuration ​ Create a WAL-enabled table with deduplication using CREATE TABLE syntax. Enable or disable deduplication at any time for individual tables using the following statements: ALTER TABLE DEDUP ENABLE ALTER TABLE DEDUP DISABLE Remember: correct UPSERT KEYS ensure that deduplication functions as expected. Deduplication UPSERT Keys ​ UPSERT is an abbreviation for UPDATE or INSERT , which is a common database concept. It means that the new row UPDATEs the existing row (or multiple rows in the general case) when the matching criteria are met. Otherwise, the new row is INSERTed into the table. In QuestDB deduplication, the UPSERT matching criteria are set by defining a column list in the UPSERT KEYS clause in the CREATE or ALTER table statement. UPSERT KEYS can be changed at any time. It can contain one or more columns. Please be aware that the designated Timestamp column must always be included in t"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "se in the CREATE or ALTER table statement. UPSERT KEYS can be changed at any time. It can contain one or more columns. Please be aware that the designated Timestamp column must always be included in the UPSERT KEYS list. Example ​ The easiest way to explain the usage of UPSERT KEYS is through an example: CREATE TABLE TICKER_PRICE ( ts TIMESTAMP, ticker SYMBOL, price DOUBLE ) TIMESTAMP(ts) PARTITION BY DAY WAL DEDUP UPSERT KEYS(ts, ticker); In this example, the deduplication keys are set to the ts column, which is the designated timestamp, and the ticker column. The intention is to have no more than one price point per ticker at any given time. Therefore, if the same price/day combination is sent twice, only the last price is saved. The following inserts demonstrate the deduplication behavior: INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 78.56); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 78.34); -- row 2 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 104.40); -- row 3 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 105.18); -- row 4 In this case, deduplication overwrites row 1 with row 2 because both deduplication keys have the same values"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "104.40); -- row 3 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 105.18); -- row 4 In this case, deduplication overwrites row 1 with row 2 because both deduplication keys have the same values: ts='2023-07-14' and ticker='QQQ' . The same behavior applies to the second pair of rows, where row 4 overwrites row 3. As a result, the table contains only two rows: SELECT * FROM TICKER_PRICE; ts ticker price 2023-07-14 QQQ 78.34 2023-07-14 AAPL 105.18 Regardless of whether the inserts are executed in a single transaction/batch or as individual inserts, the outcome remains unchanged as long as the order of the inserts is maintained. Deduplication can be disabled using the DEDUP DISABLE SQL statement: ALTER TABLE TICKER_PRICE DEDUP DISABLE This reverts the table to behave as usual, allowing the following inserts: INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 84.59); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 105.21); -- row 2 These inserts add two more rows to the TICKER_PRICE table: SELECT * FROM TICKER_PRICE; ts ticker price 2023-07-14 QQQ 78.34 2023-07-14 QQQ 84.59 2023-07-14 AAPL 105.18 2023-07-14 AAPL 105.21 Deduplication can be enabled again at any"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "o the TICKER_PRICE table: SELECT * FROM TICKER_PRICE; ts ticker price 2023-07-14 QQQ 78.34 2023-07-14 QQQ 84.59 2023-07-14 AAPL 105.18 2023-07-14 AAPL 105.21 Deduplication can be enabled again at any time: ALTER TABLE TICKER_PRICE DEDUP ENABLE UPSERT KEYS(ts, ticker) note Enabling deduplication does not have any effect on the existing data and only applies to newly inserted data. This means that a table with deduplication enabled can still contain duplicate data. Enabling deduplication does not change the number of rows in the table. After enabling deduplication, the following inserts demonstrate the deduplication behavior: INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 98.02); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 91.16); -- row 2 After these inserts, all rows with ts='2023-07-14' and ticker='QQQ' are replaced, first by row 1 and then by row 2, and the price is set to 91.16 : SELECT * FROM TICKER_PRICE; ts ticker price 2023-07-14 QQQ 91.16 2023-07-14 QQQ 91.16 2023-07-14 AAPL 105.18 2023-07-14 AAPL 105.21 Checking Deduplication Configuration ​ It is possible to utilize metadata tables query to verify whether deduplication is enabled for a specific t"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "Q 91.16 2023-07-14 AAPL 105.18 2023-07-14 AAPL 105.21 Checking Deduplication Configuration ​ It is possible to utilize metadata tables query to verify whether deduplication is enabled for a specific table: SELECT dedup FROM tables() WHERE table_name = '<the table name>' The function table_columns can be used to identify which columns are configured as deduplication UPSERT KEYS: SELECT `column`, upsertKey from table_columns('<the table name>') Edit this page Previous Create Table Next Designated timestamp Practical considerations Configuration Deduplication UPSERT Keys Example Checking Deduplication Configuration\n\nConcepts Data deduplication On this page Data Deduplication Starting from QuestDB 7.3, there is an option to enable storage-level data deduplication on a table. Data deduplication works on all the data inserted into the table and replaces matching rows with the new versions. Only new rows that do no match existing data will be inserted. note Deduplication can only be enabled for Write-Ahead Log (WAL) tables. Practical considerations ​ Deduplication in QuestDB makes table inserts idempotent . The primary use case is to allow for re-sending data within a given time range wit"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "or Write-Ahead Log (WAL) tables. Practical considerations ​ Deduplication in QuestDB makes table inserts idempotent . The primary use case is to allow for re-sending data within a given time range without creating duplicates. This can be particularly useful in situations where there is an error in sending data, such as when using InfluxDB Line Protocol , and there is no clear indication of how much of the data has already been written. With deduplication enabled, it is safe to re-send data from a fixed period in the past to resume the writing process. Enabling deduplication on a table has an impact on writing performance, especially when multiple UPSERT KEYS are configured. Generally, if the data have mostly unique timestamps across all the rows, the performance impact of deduplication is low. Conversely, the most demanding data pattern occurs when there are many rows with the same timestamp that need to be deduplicated on additional columns. For example, in use cases where 10 million devices send CPU metrics every second precisely, deduplicating the data based on the device ID can be expensive. However, in cases where CPU metrics are sent at random and typically have unique timest"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "devices send CPU metrics every second precisely, deduplicating the data based on the device ID can be expensive. However, in cases where CPU metrics are sent at random and typically have unique timestamps, the cost of deduplication is negligible. note The on-disk ordering of rows with duplicate timestamps differs when deduplication is enabled. Without deduplication: the insertion order of each row will be preserved for rows with the same timestamp With deduplication: the rows will be stored in order sorted by the DEDUP UPSERT keys, with the same timestamp For example: DEDUP UPSERT keys(timestamp, symbol, price) -- will be stored on-disk in an order like: ORDER BY timestamp, symbol, price This is the natural order of data returned in plain queries, without any grouping, filtering or ordering. The SQL standard does not guarantee the ordering of result sets without explicit ORDER BY clauses. Configuration ​ Create a WAL-enabled table with deduplication using CREATE TABLE syntax. Enable or disable deduplication at any time for individual tables using the following statements: ALTER TABLE DEDUP ENABLE ALTER TABLE DEDUP DISABLE Remember: correct UPSERT KEYS ensure that deduplication func"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "disable deduplication at any time for individual tables using the following statements: ALTER TABLE DEDUP ENABLE ALTER TABLE DEDUP DISABLE Remember: correct UPSERT KEYS ensure that deduplication functions as expected. Deduplication UPSERT Keys ​ UPSERT is an abbreviation for UPDATE or INSERT , which is a common database concept. It means that the new row UPDATEs the existing row (or multiple rows in the general case) when the matching criteria are met. Otherwise, the new row is INSERTed into the table. In QuestDB deduplication, the UPSERT matching criteria are set by defining a column list in the UPSERT KEYS clause in the CREATE or ALTER table statement. UPSERT KEYS can be changed at any time. It can contain one or more columns. Please be aware that the designated Timestamp column must always be included in the UPSERT KEYS list. Example ​ The easiest way to explain the usage of UPSERT KEYS is through an example: CREATE TABLE TICKER_PRICE ( ts TIMESTAMP, ticker SYMBOL, price DOUBLE ) TIMESTAMP(ts) PARTITION BY DAY WAL DEDUP UPSERT KEYS(ts, ticker); In this example, the deduplication keys are set to the ts column, which is the designated timestamp, and the ticker column. The intenti"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "IMESTAMP(ts) PARTITION BY DAY WAL DEDUP UPSERT KEYS(ts, ticker); In this example, the deduplication keys are set to the ts column, which is the designated timestamp, and the ticker column. The intention is to have no more than one price point per ticker at any given time. Therefore, if the same price/day combination is sent twice, only the last price is saved. The following inserts demonstrate the deduplication behavior: INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 78.56); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 78.34); -- row 2 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 104.40); -- row 3 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 105.18); -- row 4 In this case, deduplication overwrites row 1 with row 2 because both deduplication keys have the same values: ts='2023-07-14' and ticker='QQQ' . The same behavior applies to the second pair of rows, where row 4 overwrites row 3. As a result, the table contains only two rows: SELECT * FROM TICKER_PRICE; ts ticker price 2023-07-14 QQQ 78.34 2023-07-14 AAPL 105.18 Regardless of whether the inserts are executed in a single transaction/batch or as individual inserts, the outcome remains"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "OM TICKER_PRICE; ts ticker price 2023-07-14 QQQ 78.34 2023-07-14 AAPL 105.18 Regardless of whether the inserts are executed in a single transaction/batch or as individual inserts, the outcome remains unchanged as long as the order of the inserts is maintained. Deduplication can be disabled using the DEDUP DISABLE SQL statement: ALTER TABLE TICKER_PRICE DEDUP DISABLE This reverts the table to behave as usual, allowing the following inserts: INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 84.59); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 105.21); -- row 2 These inserts add two more rows to the TICKER_PRICE table: SELECT * FROM TICKER_PRICE; ts ticker price 2023-07-14 QQQ 78.34 2023-07-14 QQQ 84.59 2023-07-14 AAPL 105.18 2023-07-14 AAPL 105.21 Deduplication can be enabled again at any time: ALTER TABLE TICKER_PRICE DEDUP ENABLE UPSERT KEYS(ts, ticker) note Enabling deduplication does not have any effect on the existing data and only applies to newly inserted data. This means that a table with deduplication enabled can still contain duplicate data. Enabling deduplication does not change the number of rows in the table. After enabling deduplication, the follo"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "ta. This means that a table with deduplication enabled can still contain duplicate data. Enabling deduplication does not change the number of rows in the table. After enabling deduplication, the following inserts demonstrate the deduplication behavior: INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 98.02); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 91.16); -- row 2 After these inserts, all rows with ts='2023-07-14' and ticker='QQQ' are replaced, first by row 1 and then by row 2, and the price is set to 91.16 : SELECT * FROM TICKER_PRICE; ts ticker price 2023-07-14 QQQ 91.16 2023-07-14 QQQ 91.16 2023-07-14 AAPL 105.18 2023-07-14 AAPL 105.21 Checking Deduplication Configuration ​ It is possible to utilize metadata tables query to verify whether deduplication is enabled for a specific table: SELECT dedup FROM tables() WHERE table_name = '<the table name>' The function table_columns can be used to identify which columns are configured as deduplication UPSERT KEYS: SELECT `column`, upsertKey from table_columns('<the table name>') Edit this page Previous Create Table Next Designated timestamp Practical considerations Configuration Deduplication UPSERT Keys Exam"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": ": SELECT `column`, upsertKey from table_columns('<the table name>') Edit this page Previous Create Table Next Designated timestamp Practical considerations Configuration Deduplication UPSERT Keys Example Checking Deduplication Configuration\n\nConcepts Data deduplication On this page Data Deduplication Starting from QuestDB 7.3, there is an option to enable storage-level data deduplication on a table. Data deduplication works on all the data inserted into the table and replaces matching rows with the new versions. Only new rows that do no match existing data will be inserted. note Deduplication can only be enabled for Write-Ahead Log (WAL) tables. Practical considerations ​ Deduplication in QuestDB makes table inserts idempotent . The primary use case is to allow for re-sending data within a given time range without creating duplicates. This can be particularly useful in situations where there is an error in sending data, such as when using InfluxDB Line Protocol , and there is no clear indication of how much of the data has already been written. With deduplication enabled, it is safe to re-send data from a fixed period in the past to resume the writing process. Enabling deduplicatio"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "indication of how much of the data has already been written. With deduplication enabled, it is safe to re-send data from a fixed period in the past to resume the writing process. Enabling deduplication on a table has an impact on writing performance, especially when multiple UPSERT KEYS are configured. Generally, if the data have mostly unique timestamps across all the rows, the performance impact of deduplication is low. Conversely, the most demanding data pattern occurs when there are many rows with the same timestamp that need to be deduplicated on additional columns. For example, in use cases where 10 million devices send CPU metrics every second precisely, deduplicating the data based on the device ID can be expensive. However, in cases where CPU metrics are sent at random and typically have unique timestamps, the cost of deduplication is negligible. note The on-disk ordering of rows with duplicate timestamps differs when deduplication is enabled. Without deduplication: the insertion order of each row will be preserved for rows with the same timestamp With deduplication: the rows will be stored in order sorted by the DEDUP UPSERT keys, with the same timestamp For example: DEDU"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "ion order of each row will be preserved for rows with the same timestamp With deduplication: the rows will be stored in order sorted by the DEDUP UPSERT keys, with the same timestamp For example: DEDUP UPSERT keys(timestamp, symbol, price) -- will be stored on-disk in an order like: ORDER BY timestamp, symbol, price This is the natural order of data returned in plain queries, without any grouping, filtering or ordering. The SQL standard does not guarantee the ordering of result sets without explicit ORDER BY clauses. Configuration ​ Create a WAL-enabled table with deduplication using CREATE TABLE syntax. Enable or disable deduplication at any time for individual tables using the following statements: ALTER TABLE DEDUP ENABLE ALTER TABLE DEDUP DISABLE Remember: correct UPSERT KEYS ensure that deduplication functions as expected. Deduplication UPSERT Keys ​ UPSERT is an abbreviation for UPDATE or INSERT , which is a common database concept. It means that the new row UPDATEs the existing row (or multiple rows in the general case) when the matching criteria are met. Otherwise, the new row is INSERTed into the table. In QuestDB deduplication, the UPSERT matching criteria are set by defi"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "row (or multiple rows in the general case) when the matching criteria are met. Otherwise, the new row is INSERTed into the table. In QuestDB deduplication, the UPSERT matching criteria are set by defining a column list in the UPSERT KEYS clause in the CREATE or ALTER table statement. UPSERT KEYS can be changed at any time. It can contain one or more columns. Please be aware that the designated Timestamp column must always be included in the UPSERT KEYS list. Example ​ The easiest way to explain the usage of UPSERT KEYS is through an example: CREATE TABLE TICKER_PRICE ( ts TIMESTAMP, ticker SYMBOL, price DOUBLE ) TIMESTAMP(ts) PARTITION BY DAY WAL DEDUP UPSERT KEYS(ts, ticker); In this example, the deduplication keys are set to the ts column, which is the designated timestamp, and the ticker column. The intention is to have no more than one price point per ticker at any given time. Therefore, if the same price/day combination is sent twice, only the last price is saved. The following inserts demonstrate the deduplication behavior: INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 78.56); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 78.34); -- row 2 INSERT INTO"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "serts demonstrate the deduplication behavior: INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 78.56); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 78.34); -- row 2 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 104.40); -- row 3 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 105.18); -- row 4 In this case, deduplication overwrites row 1 with row 2 because both deduplication keys have the same values: ts='2023-07-14' and ticker='QQQ' . The same behavior applies to the second pair of rows, where row 4 overwrites row 3. As a result, the table contains only two rows: SELECT * FROM TICKER_PRICE; ts ticker price 2023-07-14 QQQ 78.34 2023-07-14 AAPL 105.18 Regardless of whether the inserts are executed in a single transaction/batch or as individual inserts, the outcome remains unchanged as long as the order of the inserts is maintained. Deduplication can be disabled using the DEDUP DISABLE SQL statement: ALTER TABLE TICKER_PRICE DEDUP DISABLE This reverts the table to behave as usual, allowing the following inserts: INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 84.59); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 105.21);"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "ts the table to behave as usual, allowing the following inserts: INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 84.59); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 105.21); -- row 2 These inserts add two more rows to the TICKER_PRICE table: SELECT * FROM TICKER_PRICE; ts ticker price 2023-07-14 QQQ 78.34 2023-07-14 QQQ 84.59 2023-07-14 AAPL 105.18 2023-07-14 AAPL 105.21 Deduplication can be enabled again at any time: ALTER TABLE TICKER_PRICE DEDUP ENABLE UPSERT KEYS(ts, ticker) note Enabling deduplication does not have any effect on the existing data and only applies to newly inserted data. This means that a table with deduplication enabled can still contain duplicate data. Enabling deduplication does not change the number of rows in the table. After enabling deduplication, the following inserts demonstrate the deduplication behavior: INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 98.02); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 91.16); -- row 2 After these inserts, all rows with ts='2023-07-14' and ticker='QQQ' are replaced, first by row 1 and then by row 2, and the price is set to 91.16 : SELECT * FROM TICKER_PRICE; ts ticke"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "1.16); -- row 2 After these inserts, all rows with ts='2023-07-14' and ticker='QQQ' are replaced, first by row 1 and then by row 2, and the price is set to 91.16 : SELECT * FROM TICKER_PRICE; ts ticker price 2023-07-14 QQQ 91.16 2023-07-14 QQQ 91.16 2023-07-14 AAPL 105.18 2023-07-14 AAPL 105.21 Checking Deduplication Configuration ​ It is possible to utilize metadata tables query to verify whether deduplication is enabled for a specific table: SELECT dedup FROM tables() WHERE table_name = '<the table name>' The function table_columns can be used to identify which columns are configured as deduplication UPSERT KEYS: SELECT `column`, upsertKey from table_columns('<the table name>') Edit this page Previous Create Table Next Designated timestamp\n\nConcepts Data deduplication On this page Data Deduplication Starting from QuestDB 7.3, there is an option to enable storage-level data deduplication on a table. Data deduplication works on all the data inserted into the table and replaces matching rows with the new versions. Only new rows that do no match existing data will be inserted. note Deduplication can only be enabled for Write-Ahead Log (WAL) tables. Practical considerations ​ Deduplic"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "rows with the new versions. Only new rows that do no match existing data will be inserted. note Deduplication can only be enabled for Write-Ahead Log (WAL) tables. Practical considerations ​ Deduplication in QuestDB makes table inserts idempotent . The primary use case is to allow for re-sending data within a given time range without creating duplicates. This can be particularly useful in situations where there is an error in sending data, such as when using InfluxDB Line Protocol , and there is no clear indication of how much of the data has already been written. With deduplication enabled, it is safe to re-send data from a fixed period in the past to resume the writing process. Enabling deduplication on a table has an impact on writing performance, especially when multiple UPSERT KEYS are configured. Generally, if the data have mostly unique timestamps across all the rows, the performance impact of deduplication is low. Conversely, the most demanding data pattern occurs when there are many rows with the same timestamp that need to be deduplicated on additional columns. For example, in use cases where 10 million devices send CPU metrics every second precisely, deduplicating the d"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "re many rows with the same timestamp that need to be deduplicated on additional columns. For example, in use cases where 10 million devices send CPU metrics every second precisely, deduplicating the data based on the device ID can be expensive. However, in cases where CPU metrics are sent at random and typically have unique timestamps, the cost of deduplication is negligible. note The on-disk ordering of rows with duplicate timestamps differs when deduplication is enabled. Without deduplication: the insertion order of each row will be preserved for rows with the same timestamp With deduplication: the rows will be stored in order sorted by the DEDUP UPSERT keys, with the same timestamp For example: DEDUP UPSERT keys(timestamp, symbol, price) -- will be stored on-disk in an order like: ORDER BY timestamp, symbol, price This is the natural order of data returned in plain queries, without any grouping, filtering or ordering. The SQL standard does not guarantee the ordering of result sets without explicit ORDER BY clauses. Configuration ​ Create a WAL-enabled table with deduplication using CREATE TABLE syntax. Enable or disable deduplication at any time for individual tables using the f"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "ithout explicit ORDER BY clauses. Configuration ​ Create a WAL-enabled table with deduplication using CREATE TABLE syntax. Enable or disable deduplication at any time for individual tables using the following statements: ALTER TABLE DEDUP ENABLE ALTER TABLE DEDUP DISABLE Remember: correct UPSERT KEYS ensure that deduplication functions as expected. Deduplication UPSERT Keys ​ UPSERT is an abbreviation for UPDATE or INSERT , which is a common database concept. It means that the new row UPDATEs the existing row (or multiple rows in the general case) when the matching criteria are met. Otherwise, the new row is INSERTed into the table. In QuestDB deduplication, the UPSERT matching criteria are set by defining a column list in the UPSERT KEYS clause in the CREATE or ALTER table statement. UPSERT KEYS can be changed at any time. It can contain one or more columns. Please be aware that the designated Timestamp column must always be included in the UPSERT KEYS list. Example ​ The easiest way to explain the usage of UPSERT KEYS is through an example: CREATE TABLE TICKER_PRICE ( ts TIMESTAMP, ticker SYMBOL, price DOUBLE ) TIMESTAMP(ts) PARTITION BY DAY WAL DEDUP UPSERT KEYS(ts, ticker); In"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "to explain the usage of UPSERT KEYS is through an example: CREATE TABLE TICKER_PRICE ( ts TIMESTAMP, ticker SYMBOL, price DOUBLE ) TIMESTAMP(ts) PARTITION BY DAY WAL DEDUP UPSERT KEYS(ts, ticker); In this example, the deduplication keys are set to the ts column, which is the designated timestamp, and the ticker column. The intention is to have no more than one price point per ticker at any given time. Therefore, if the same price/day combination is sent twice, only the last price is saved. The following inserts demonstrate the deduplication behavior: INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 78.56); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 78.34); -- row 2 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 104.40); -- row 3 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 105.18); -- row 4 In this case, deduplication overwrites row 1 with row 2 because both deduplication keys have the same values: ts='2023-07-14' and ticker='QQQ' . The same behavior applies to the second pair of rows, where row 4 overwrites row 3. As a result, the table contains only two rows: SELECT * FROM TICKER_PRICE; ts ticker price 2023-07-14 QQQ 78.34 2023-07-14 AAP"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "ehavior applies to the second pair of rows, where row 4 overwrites row 3. As a result, the table contains only two rows: SELECT * FROM TICKER_PRICE; ts ticker price 2023-07-14 QQQ 78.34 2023-07-14 AAPL 105.18 Regardless of whether the inserts are executed in a single transaction/batch or as individual inserts, the outcome remains unchanged as long as the order of the inserts is maintained. Deduplication can be disabled using the DEDUP DISABLE SQL statement: ALTER TABLE TICKER_PRICE DEDUP DISABLE This reverts the table to behave as usual, allowing the following inserts: INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 84.59); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 105.21); -- row 2 These inserts add two more rows to the TICKER_PRICE table: SELECT * FROM TICKER_PRICE; ts ticker price 2023-07-14 QQQ 78.34 2023-07-14 QQQ 84.59 2023-07-14 AAPL 105.18 2023-07-14 AAPL 105.21 Deduplication can be enabled again at any time: ALTER TABLE TICKER_PRICE DEDUP ENABLE UPSERT KEYS(ts, ticker) note Enabling deduplication does not have any effect on the existing data and only applies to newly inserted data. This means that a table with deduplication enabled can still con"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "ERT KEYS(ts, ticker) note Enabling deduplication does not have any effect on the existing data and only applies to newly inserted data. This means that a table with deduplication enabled can still contain duplicate data. Enabling deduplication does not change the number of rows in the table. After enabling deduplication, the following inserts demonstrate the deduplication behavior: INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 98.02); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 91.16); -- row 2 After these inserts, all rows with ts='2023-07-14' and ticker='QQQ' are replaced, first by row 1 and then by row 2, and the price is set to 91.16 : SELECT * FROM TICKER_PRICE; ts ticker price 2023-07-14 QQQ 91.16 2023-07-14 QQQ 91.16 2023-07-14 AAPL 105.18 2023-07-14 AAPL 105.21 Checking Deduplication Configuration ​ It is possible to utilize metadata tables query to verify whether deduplication is enabled for a specific table: SELECT dedup FROM tables() WHERE table_name = '<the table name>' The function table_columns can be used to identify which columns are configured as deduplication UPSERT KEYS: SELECT `column`, upsertKey from table_columns('<the table name>')"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "me = '<the table name>' The function table_columns can be used to identify which columns are configured as deduplication UPSERT KEYS: SELECT `column`, upsertKey from table_columns('<the table name>') Edit this page Previous Create Table Next Designated timestamp\n\nConcepts\n\nData deduplication\n\nOn this page\n\nData Deduplication Starting from QuestDB 7.3, there is an option to enable storage-level data deduplication on a table. Data deduplication works on all the data inserted into the table and replaces matching rows with the new versions. Only new rows that do no match existing data will be inserted. note Deduplication can only be enabled for Write-Ahead Log (WAL) tables. Practical considerations ​ Deduplication in QuestDB makes table inserts idempotent . The primary use case is to allow for re-sending data within a given time range without creating duplicates. This can be particularly useful in situations where there is an error in sending data, such as when using InfluxDB Line Protocol , and there is no clear indication of how much of the data has already been written. With deduplication enabled, it is safe to re-send data from a fixed period in the past to resume the writing proce"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": ", and there is no clear indication of how much of the data has already been written. With deduplication enabled, it is safe to re-send data from a fixed period in the past to resume the writing process. Enabling deduplication on a table has an impact on writing performance, especially when multiple UPSERT KEYS are configured. Generally, if the data have mostly unique timestamps across all the rows, the performance impact of deduplication is low. Conversely, the most demanding data pattern occurs when there are many rows with the same timestamp that need to be deduplicated on additional columns. For example, in use cases where 10 million devices send CPU metrics every second precisely, deduplicating the data based on the device ID can be expensive. However, in cases where CPU metrics are sent at random and typically have unique timestamps, the cost of deduplication is negligible. note The on-disk ordering of rows with duplicate timestamps differs when deduplication is enabled. Without deduplication: the insertion order of each row will be preserved for rows with the same timestamp With deduplication: the rows will be stored in order sorted by the DEDUP UPSERT keys, with the same ti"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "deduplication: the insertion order of each row will be preserved for rows with the same timestamp With deduplication: the rows will be stored in order sorted by the DEDUP UPSERT keys, with the same timestamp For example: DEDUP UPSERT keys(timestamp, symbol, price) -- will be stored on-disk in an order like: ORDER BY timestamp, symbol, price This is the natural order of data returned in plain queries, without any grouping, filtering or ordering. The SQL standard does not guarantee the ordering of result sets without explicit ORDER BY clauses. Configuration ​ Create a WAL-enabled table with deduplication using CREATE TABLE syntax. Enable or disable deduplication at any time for individual tables using the following statements: ALTER TABLE DEDUP ENABLE ALTER TABLE DEDUP DISABLE Remember: correct UPSERT KEYS ensure that deduplication functions as expected. Deduplication UPSERT Keys ​ UPSERT is an abbreviation for UPDATE or INSERT , which is a common database concept. It means that the new row UPDATEs the existing row (or multiple rows in the general case) when the matching criteria are met. Otherwise, the new row is INSERTed into the table. In QuestDB deduplication, the UPSERT matching"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "row UPDATEs the existing row (or multiple rows in the general case) when the matching criteria are met. Otherwise, the new row is INSERTed into the table. In QuestDB deduplication, the UPSERT matching criteria are set by defining a column list in the UPSERT KEYS clause in the CREATE or ALTER table statement. UPSERT KEYS can be changed at any time. It can contain one or more columns. Please be aware that the designated Timestamp column must always be included in the UPSERT KEYS list. Example ​ The easiest way to explain the usage of UPSERT KEYS is through an example: CREATE TABLE TICKER_PRICE ( ts TIMESTAMP, ticker SYMBOL, price DOUBLE ) TIMESTAMP(ts) PARTITION BY DAY WAL DEDUP UPSERT KEYS(ts, ticker); In this example, the deduplication keys are set to the ts column, which is the designated timestamp, and the ticker column. The intention is to have no more than one price point per ticker at any given time. Therefore, if the same price/day combination is sent twice, only the last price is saved. The following inserts demonstrate the deduplication behavior: INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 78.56); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 78.3"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "s saved. The following inserts demonstrate the deduplication behavior: INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 78.56); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 78.34); -- row 2 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 104.40); -- row 3 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 105.18); -- row 4 In this case, deduplication overwrites row 1 with row 2 because both deduplication keys have the same values: ts='2023-07-14' and ticker='QQQ' . The same behavior applies to the second pair of rows, where row 4 overwrites row 3. As a result, the table contains only two rows: SELECT * FROM TICKER_PRICE; ts ticker price 2023-07-14 QQQ 78.34 2023-07-14 AAPL 105.18 Regardless of whether the inserts are executed in a single transaction/batch or as individual inserts, the outcome remains unchanged as long as the order of the inserts is maintained. Deduplication can be disabled using the DEDUP DISABLE SQL statement: ALTER TABLE TICKER_PRICE DEDUP DISABLE This reverts the table to behave as usual, allowing the following inserts: INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 84.59); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "DEDUP DISABLE This reverts the table to behave as usual, allowing the following inserts: INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 84.59); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 105.21); -- row 2 These inserts add two more rows to the TICKER_PRICE table: SELECT * FROM TICKER_PRICE; ts ticker price 2023-07-14 QQQ 78.34 2023-07-14 QQQ 84.59 2023-07-14 AAPL 105.18 2023-07-14 AAPL 105.21 Deduplication can be enabled again at any time: ALTER TABLE TICKER_PRICE DEDUP ENABLE UPSERT KEYS(ts, ticker) note Enabling deduplication does not have any effect on the existing data and only applies to newly inserted data. This means that a table with deduplication enabled can still contain duplicate data. Enabling deduplication does not change the number of rows in the table. After enabling deduplication, the following inserts demonstrate the deduplication behavior: INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 98.02); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 91.16); -- row 2 After these inserts, all rows with ts='2023-07-14' and ticker='QQQ' are replaced, first by row 1 and then by row 2, and the price is set to 91.16 : SELECT * FR"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Introduction", "text": "S ('2023-07-14', 'QQQ', 91.16); -- row 2 After these inserts, all rows with ts='2023-07-14' and ticker='QQQ' are replaced, first by row 1 and then by row 2, and the price is set to 91.16 : SELECT * FROM TICKER_PRICE; ts ticker price 2023-07-14 QQQ 91.16 2023-07-14 QQQ 91.16 2023-07-14 AAPL 105.18 2023-07-14 AAPL 105.21 Checking Deduplication Configuration ​ It is possible to utilize metadata tables query to verify whether deduplication is enabled for a specific table: SELECT dedup FROM tables() WHERE table_name = '<the table name>' The function table_columns can be used to identify which columns are configured as deduplication UPSERT KEYS: SELECT `column`, upsertKey from table_columns('<the table name>')"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Data Deduplication", "text": "Starting from QuestDB 7.3, there is an option to enable storage-level data deduplication on a table. Data deduplication works on all the data inserted into the table and replaces matching rows with the new versions. Only new rows that do no match existing data will be inserted.\n\nnote Deduplication can only be enabled for Write-Ahead Log (WAL) tables.\n\nnote\n\nDeduplication can only be enabled for Write-Ahead Log (WAL) tables.\n\nDeduplication can only be enabled for Write-Ahead Log (WAL) tables."}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Practical considerations ​", "text": "Deduplication in QuestDB makes table inserts idempotent . The primary use case is to allow for re-sending data within a given time range without creating duplicates.\n\nThis can be particularly useful in situations where there is an error in sending data, such as when using InfluxDB Line Protocol , and there is no clear indication of how much of the data has already been written. With deduplication enabled, it is safe to re-send data from a fixed period in the past to resume the writing process.\n\nEnabling deduplication on a table has an impact on writing performance, especially when multiple UPSERT KEYS are configured. Generally, if the data have mostly unique timestamps across all the rows, the performance impact of deduplication is low. Conversely, the most demanding data pattern occurs when there are many rows with the same timestamp that need to be deduplicated on additional columns.\n\nUPSERT KEYS\n\nFor example, in use cases where 10 million devices send CPU metrics every second precisely, deduplicating the data based on the device ID can be expensive. However, in cases where CPU metrics are sent at random and typically have unique timestamps, the cost of deduplication is negligibl"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Practical considerations ​", "text": "ely, deduplicating the data based on the device ID can be expensive. However, in cases where CPU metrics are sent at random and typically have unique timestamps, the cost of deduplication is negligible.\n\nnote The on-disk ordering of rows with duplicate timestamps differs when deduplication is enabled. Without deduplication: the insertion order of each row will be preserved for rows with the same timestamp With deduplication: the rows will be stored in order sorted by the DEDUP UPSERT keys, with the same timestamp For example: DEDUP UPSERT keys(timestamp, symbol, price) -- will be stored on-disk in an order like: ORDER BY timestamp, symbol, price This is the natural order of data returned in plain queries, without any grouping, filtering or ordering. The SQL standard does not guarantee the ordering of result sets without explicit ORDER BY clauses.\n\nnote\n\nThe on-disk ordering of rows with duplicate timestamps differs when deduplication is enabled. Without deduplication: the insertion order of each row will be preserved for rows with the same timestamp With deduplication: the rows will be stored in order sorted by the DEDUP UPSERT keys, with the same timestamp For example: DEDUP UPSER"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Practical considerations ​", "text": "er of each row will be preserved for rows with the same timestamp With deduplication: the rows will be stored in order sorted by the DEDUP UPSERT keys, with the same timestamp For example: DEDUP UPSERT keys(timestamp, symbol, price) -- will be stored on-disk in an order like: ORDER BY timestamp, symbol, price This is the natural order of data returned in plain queries, without any grouping, filtering or ordering. The SQL standard does not guarantee the ordering of result sets without explicit ORDER BY clauses.\n\nThe on-disk ordering of rows with duplicate timestamps differs when deduplication is enabled.\n\nWithout deduplication: the insertion order of each row will be preserved for rows with the same timestamp\n\nthe insertion order of each row will be preserved for rows with the same timestamp\n\nWith deduplication: the rows will be stored in order sorted by the DEDUP UPSERT keys, with the same timestamp\n\nthe rows will be stored in order sorted by the DEDUP UPSERT keys, with the same timestamp\n\nDEDUP UPSERT\n\nFor example:\n\nDEDUP UPSERT keys(timestamp, symbol, price) -- will be stored on-disk in an order like: ORDER BY timestamp, symbol, price\n\nDEDUP UPSERT keys(timestamp, symbol, price)"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Practical considerations ​", "text": "amp\n\nDEDUP UPSERT\n\nFor example:\n\nDEDUP UPSERT keys(timestamp, symbol, price) -- will be stored on-disk in an order like: ORDER BY timestamp, symbol, price\n\nDEDUP UPSERT keys(timestamp, symbol, price) -- will be stored on-disk in an order like: ORDER BY timestamp, symbol, price\n\nDEDUP UPSERT keys(timestamp, symbol, price) -- will be stored on-disk in an order like: ORDER BY timestamp, symbol, price\n\nDEDUP UPSERT keys(timestamp, symbol, price) -- will be stored on-disk in an order like: ORDER BY timestamp, symbol, price\n\nThis is the natural order of data returned in plain queries, without any grouping, filtering or ordering. The SQL standard does not guarantee the ordering of result sets without explicit ORDER BY clauses.\n\nORDER BY"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Configuration ​", "text": "Create a WAL-enabled table with deduplication using CREATE TABLE syntax.\n\nCREATE TABLE\n\nEnable or disable deduplication at any time for individual tables using the following statements:\n\nALTER TABLE DEDUP ENABLE\n\nALTER TABLE DEDUP DISABLE\n\nRemember: correct UPSERT KEYS ensure that deduplication functions as expected.\n\nUPSERT KEYS"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Deduplication UPSERT Keys ​", "text": "UPSERT is an abbreviation for UPDATE or INSERT , which is a common database concept. It means that the new row UPDATEs the existing row (or multiple rows in the general case) when the matching criteria are met. Otherwise, the new row is INSERTed into the table. In QuestDB deduplication, the UPSERT matching criteria are set by defining a column list in the UPSERT KEYS clause in the CREATE or ALTER table statement.\n\nUPSERT KEYS\n\nCREATE\n\nALTER\n\nUPSERT KEYS can be changed at any time. It can contain one or more columns.\n\nUPSERT KEYS\n\nPlease be aware that the designated Timestamp column must always be included in the UPSERT KEYS list.\n\nUPSERT KEYS"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Example ​", "text": "The easiest way to explain the usage of UPSERT KEYS is through an example:\n\nUPSERT KEYS\n\nCREATE TABLE TICKER_PRICE ( ts TIMESTAMP, ticker SYMBOL, price DOUBLE ) TIMESTAMP(ts) PARTITION BY DAY WAL DEDUP UPSERT KEYS(ts, ticker);\n\nCREATE TABLE TICKER_PRICE ( ts TIMESTAMP, ticker SYMBOL, price DOUBLE ) TIMESTAMP(ts) PARTITION BY DAY WAL DEDUP UPSERT KEYS(ts, ticker);\n\nCREATE TABLE TICKER_PRICE ( ts TIMESTAMP, ticker SYMBOL, price DOUBLE ) TIMESTAMP(ts) PARTITION BY DAY WAL DEDUP UPSERT KEYS(ts, ticker);\n\nCREATE TABLE TICKER_PRICE ( ts TIMESTAMP, ticker SYMBOL, price DOUBLE ) TIMESTAMP(ts) PARTITION BY DAY WAL DEDUP UPSERT KEYS(ts, ticker);\n\nIn this example, the deduplication keys are set to the ts column, which is the designated timestamp, and the ticker column. The intention is to have no more than one price point per ticker at any given time. Therefore, if the same price/day combination is sent twice, only the last price is saved.\n\nts\n\nticker\n\nThe following inserts demonstrate the deduplication behavior:\n\nINSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 78.56); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 78.34); -- row 2 INSERT INTO TICKER_PRICE VALUES ('2023-"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Example ​", "text": "ication behavior:\n\nINSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 78.56); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 78.34); -- row 2 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 104.40); -- row 3 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 105.18); -- row 4\n\nINSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 78.56); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 78.34); -- row 2 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 104.40); -- row 3 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 105.18); -- row 4\n\nINSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 78.56); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 78.34); -- row 2 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 104.40); -- row 3 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 105.18); -- row 4\n\nINSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 78.56); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 78.34); -- row 2 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 104.40); -- row 3 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 105.18); -- row 4\n\nIn this case, deduplicati"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Example ​", "text": "4', 'QQQ', 78.34); -- row 2 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 104.40); -- row 3 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 105.18); -- row 4\n\nIn this case, deduplication overwrites row 1 with row 2 because both deduplication keys have the same values: ts='2023-07-14' and ticker='QQQ' . The same behavior applies to the second pair of rows, where row 4 overwrites row 3.\n\nts='2023-07-14'\n\nticker='QQQ'\n\nAs a result, the table contains only two rows:\n\nSELECT * FROM TICKER_PRICE;\n\nSELECT * FROM TICKER_PRICE;\n\nSELECT * FROM TICKER_PRICE;\n\nSELECT * FROM TICKER_PRICE;\n\nRegardless of whether the inserts are executed in a single transaction/batch or as individual inserts, the outcome remains unchanged as long as the order of the inserts is maintained.\n\nDeduplication can be disabled using the DEDUP DISABLE SQL statement:\n\nALTER TABLE TICKER_PRICE DEDUP DISABLE\n\nALTER TABLE TICKER_PRICE DEDUP DISABLE\n\nALTER TABLE TICKER_PRICE DEDUP DISABLE\n\nALTER TABLE TICKER_PRICE DEDUP DISABLE\n\nThis reverts the table to behave as usual, allowing the following inserts:\n\nINSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 84.59); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Example ​", "text": "DEDUP DISABLE\n\nThis reverts the table to behave as usual, allowing the following inserts:\n\nINSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 84.59); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 105.21); -- row 2\n\nINSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 84.59); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 105.21); -- row 2\n\nINSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 84.59); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 105.21); -- row 2\n\nINSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 84.59); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'AAPL', 105.21); -- row 2\n\nThese inserts add two more rows to the TICKER_PRICE table:\n\nSELECT * FROM TICKER_PRICE;\n\nSELECT * FROM TICKER_PRICE;\n\nSELECT * FROM TICKER_PRICE;\n\nSELECT * FROM TICKER_PRICE;\n\nDeduplication can be enabled again at any time:\n\nALTER TABLE TICKER_PRICE DEDUP ENABLE UPSERT KEYS(ts, ticker)\n\nALTER TABLE TICKER_PRICE DEDUP ENABLE UPSERT KEYS(ts, ticker)\n\nALTER TABLE TICKER_PRICE DEDUP ENABLE UPSERT KEYS(ts, ticker)\n\nALTER TABLE TICKER_PRICE DEDUP ENABLE UPSERT KEYS(ts, ticker)\n\nnote Enabling deduplication does not have any effect"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Example ​", "text": "ERT KEYS(ts, ticker)\n\nALTER TABLE TICKER_PRICE DEDUP ENABLE UPSERT KEYS(ts, ticker)\n\nALTER TABLE TICKER_PRICE DEDUP ENABLE UPSERT KEYS(ts, ticker)\n\nnote Enabling deduplication does not have any effect on the existing data and only applies to newly inserted data. This means that a table with deduplication enabled can still contain duplicate data.\n\nnote\n\nEnabling deduplication does not have any effect on the existing data and only applies to newly inserted data. This means that a table with deduplication enabled can still contain duplicate data.\n\nEnabling deduplication does not have any effect on the existing data and only applies to newly inserted data. This means that a table with deduplication enabled can still contain duplicate data.\n\nEnabling deduplication does not change the number of rows in the table. After enabling deduplication, the following inserts demonstrate the deduplication behavior:\n\nINSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 98.02); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 91.16); -- row 2\n\nINSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 98.02); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 91.16); -- row 2\n\nIN"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Example ​", "text": "R_PRICE VALUES ('2023-07-14', 'QQQ', 91.16); -- row 2\n\nINSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 98.02); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 91.16); -- row 2\n\nINSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 98.02); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 91.16); -- row 2\n\nINSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 98.02); -- row 1 INSERT INTO TICKER_PRICE VALUES ('2023-07-14', 'QQQ', 91.16); -- row 2\n\nAfter these inserts, all rows with ts='2023-07-14' and ticker='QQQ' are replaced, first by row 1 and then by row 2, and the price is set to 91.16 :\n\nts='2023-07-14'\n\nticker='QQQ'\n\nSELECT * FROM TICKER_PRICE;\n\nSELECT * FROM TICKER_PRICE;\n\nSELECT * FROM TICKER_PRICE;\n\nSELECT * FROM TICKER_PRICE;"}
{"source_url": "https://questdb.com/docs/concept/deduplication", "title": "Data Deduplication", "section": "Checking Deduplication Configuration ​", "text": "It is possible to utilize metadata tables query to verify whether deduplication is enabled for a specific table:\n\nSELECT dedup FROM tables() WHERE table_name = '<the table name>'\n\nSELECT dedup FROM tables() WHERE table_name = '<the table name>'\n\nSELECT dedup FROM tables() WHERE table_name = '<the table name>'\n\nSELECT dedup FROM tables() WHERE table_name = '<the table name>'\n\nThe function table_columns can be used to identify which columns are configured as deduplication UPSERT KEYS:\n\nSELECT `column`, upsertKey from table_columns('<the table name>')\n\nSELECT `column`, upsertKey from table_columns('<the table name>')\n\nSELECT `column`, upsertKey from table_columns('<the table name>')\n\nSELECT `column`, upsertKey from table_columns('<the table name>')\n\nEdit this page\n\nEdit this page\n\nPrevious\n\nCreate Table\n\nNext\n\nDesignated timestamp\n\nPractical considerations Configuration Deduplication UPSERT Keys Example Checking Deduplication Configuration\n\nPractical considerations Configuration Deduplication UPSERT Keys Example Checking Deduplication Configuration\n\nPractical considerations\n\nConfiguration\n\nDeduplication UPSERT Keys\n\nExample\n\nChecking Deduplication Configuration"}
